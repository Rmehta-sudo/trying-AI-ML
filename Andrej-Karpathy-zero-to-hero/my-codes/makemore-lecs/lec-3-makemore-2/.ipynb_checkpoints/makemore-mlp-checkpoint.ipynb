{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2307e1ab",
   "metadata": {},
   "source": [
    "Gemini generated In depth summary \n",
    "Based on the video chapters and your code, here is a step-by-step guide to implementing the multi-layer perceptron (MLP) language model.\n",
    "\n",
    "***\n",
    "\n",
    "### Part 1: Dataset and Model Architecture\n",
    "\n",
    "1.  **Create the Dataset**:\n",
    "    * Load the `names.txt` file and define your character vocabulary (`stoi`, `itos`).\n",
    "    * Choose a `block_size` (context length), which is the number of previous characters used to predict the next one.\n",
    "    * Iterate through each word and create a list of contexts (`X`) and their corresponding next characters (`Y`). The `.` token is used to pad the context at the beginning and signal the end of a word.\n",
    "    * Shuffle the words and split the dataset into **training (80%)**, **validation (10%)**, and **test (10%)** sets. Use `Xtr, Ytr`, `Xdev, Ydev`, and `Xte, Yte` to store these.\n",
    "2.  **Initialize the Neural Network**:\n",
    "    * **Embedding Layer**: Create an embedding lookup table `C` as a `27x10` tensor. Each row represents a character, and the 10 values are its **embedding**. This is a trainable parameter.\n",
    "    * **Hidden Layer**: Define the weights `W1` (a `30x200` tensor, `30` because `block_size * embedding_size = 3 * 10`) and biases `b1` (`200` elements).\n",
    "    * **Output Layer**: Define the weights `W2` (`200x27`) and biases `b2` (`27` elements). The output size matches the number of characters.\n",
    "    * Put all these tensors (`C, W1, b1, W2, b2`) into a list called `parameters` and set `requires_grad=True` for all of them. \n",
    "\n",
    "***\n",
    "\n",
    "### Part 2: Training and Evaluation\n",
    "\n",
    "1.  **Set Up the Training Loop**:\n",
    "    * Loop for a specified number of iterations (e.g., 200,000).\n",
    "    * For each iteration, construct a **minibatch** by randomly selecting a small number of indices (`ix`) from your training data `Xtr` and `Ytr`.\n",
    "2.  **Forward Pass**:\n",
    "    * Perform an **embedding lookup**: Use `Xtr[ix]` to get the embeddings from `C`, resulting in a tensor of shape `(batch_size, block_size, embedding_size)`.\n",
    "    * Reshape the embeddings into a single vector per example using `.view(-1, block_size * embedding_size)`.\n",
    "    * Pass this through the hidden layer: compute `emb.view(...) @ W1 + b1` and apply the **tanh activation function**.\n",
    "    * Pass the hidden layer output through the output layer: compute `h @ W2 + b2`. This gives you the `logits`.\n",
    "    * Calculate the **loss** using PyTorch's `F.cross_entropy`, passing in the `logits` and the labels `Ytr[ix]`. This function efficiently combines `softmax`, `log`, and `mean`.\n",
    "3.  **Backward Pass and Update**:\n",
    "    * Zero out the gradients for all parameters by setting `p.grad = None` for each parameter `p`.\n",
    "    * Call `loss.backward()` to compute the gradients.\n",
    "    * Update the parameters using a learning rate: `p.data += -lr * p.grad`. Use a decaying learning rate, such as starting with `0.1` and dropping to `0.01` after a certain number of steps.\n",
    "4.  **Evaluate and Visualize**:\n",
    "    * After training, evaluate the loss on the validation set (`Xdev, Ydev`) to check for **overfitting**.\n",
    "    * Visualize the embedding space by plotting the first two dimensions of the `C` matrix. Each point represents a character.\n",
    "\n",
    "***\n",
    "\n",
    "### Part 3: Sampling and Conclusion\n",
    "\n",
    "1.  **Sample from the Model**:\n",
    "    * Start with an initial `context` of all `.` tokens.\n",
    "    * Enter a loop that continues until the model predicts a `.` token.\n",
    "    * Inside the loop, get the embeddings for the current `context` from the trained `C` matrix.\n",
    "    * Perform a forward pass through the hidden and output layers to get the `logits`.\n",
    "    * Apply `F.softmax` to the logits to get probabilities.\n",
    "    * Use `torch.multinomial` to sample the index of the next character.\n",
    "    * Append the new index to your output list and update the `context` by sliding the window.\n",
    "    * Finally, join the characters from the output list to form a new name.\n",
    "\n",
    "    ---\n",
    "    ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b893817",
   "metadata": {},
   "source": [
    "GPT summary walkthrough , with lesser help , covering all ideas in the code \n",
    "---\n",
    "# ðŸ§  Character-Level MLP Language Model â€” Complete From-Scratch Walkthrough\n",
    "\n",
    "This document summarizes the **entire lecture** so you can reimplement the model without looking at the original code.  \n",
    "It covers **every step**: data prep, architecture, training, and sampling.\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£ Problem Setup\n",
    "\n",
    "We want to train a **character-level language model** that generates new names.  \n",
    "The model will be an **MLP** (multi-layer perceptron) trained from scratch on a dataset of names.\n",
    "\n",
    "The modelâ€™s job:  \n",
    "Given a **context** (a fixed number of previous characters), predict the **next character**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2ï¸âƒ£ Data Preparation\n",
    "\n",
    "1. **Load Dataset**  \n",
    "   - Read the `names.txt` file into a list of strings, one name per line.  \n",
    "   - Inspect dataset: size, min/max length.\n",
    "\n",
    "2. **Define Vocabulary**  \n",
    "   - Collect all unique characters in the dataset.  \n",
    "   - Add a special `.` token for start/end of a word.  \n",
    "   - Create two dictionaries:\n",
    "     - `stoi`: char â†’ index\n",
    "     - `itos`: index â†’ char\n",
    "\n",
    "3. **Context Windows**  \n",
    "   - Choose a fixed context size `block_size` (e.g., 3).  \n",
    "   - For each name:\n",
    "     - Pad with `.` tokens at the start.\n",
    "     - Slide a window of length `block_size` across the name.\n",
    "     - The window characters are the **input**.\n",
    "     - The next character is the **target**.\n",
    "\n",
    "4. **Numerical Encoding**  \n",
    "   - Map characters in the context and the target to integers using `stoi`.  \n",
    "   - Store all contexts in an integer tensor `X`.  \n",
    "   - Store all targets in integer tensor `Y`.\n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£ Model Architecture\n",
    "\n",
    "The MLP has three main parts:\n",
    "\n",
    "1. **Embedding Layer**  \n",
    "   - A learnable matrix `C` of size `(vocab_size, embedding_dim)`.  \n",
    "   - Converts each character index into a dense vector.\n",
    "\n",
    "2. **Hidden Layer**  \n",
    "   - Flatten all embeddings for the context into a single vector.  \n",
    "   - Apply a linear transformation: `h = tanh(X @ W1 + b1)`  \n",
    "     - `W1`: weight matrix of shape `(context_size * embedding_dim, hidden_size)`\n",
    "     - `b1`: bias vector of length `hidden_size`.\n",
    "\n",
    "3. **Output Layer**  \n",
    "   - Map hidden activations to vocabulary logits: `logits = h @ W2 + b2`  \n",
    "     - `W2`: weight matrix `(hidden_size, vocab_size)`\n",
    "     - `b2`: bias vector `(vocab_size,)`\n",
    "\n",
    "---\n",
    "\n",
    "## 4ï¸âƒ£ Loss Function\n",
    "\n",
    "We use **cross-entropy loss** between predicted logits and target indices.\n",
    "\n",
    "Two ways to compute:\n",
    "1. **Manual**: softmax â†’ log â†’ negative log likelihood â†’ mean over batch.\n",
    "2. **Built-in**: `torch.nn.functional.cross_entropy(logits, targets)`.\n",
    "\n",
    "---\n",
    "\n",
    "## 5ï¸âƒ£ Training Loop\n",
    "\n",
    "1. **Initialization**  \n",
    "   - Randomly initialize all weights with small values (e.g., normal distribution).  \n",
    "   - Zero biases.\n",
    "\n",
    "2. **Forward Pass**  \n",
    "   - Embed context characters â†’ concatenate â†’ hidden layer â†’ output layer.  \n",
    "   - Compute loss vs targets.\n",
    "\n",
    "3. **Backward Pass**  \n",
    "   - Call `.backward()` on loss to compute gradients.\n",
    "\n",
    "4. **Parameter Update**  \n",
    "   - Update all parameters with gradient descent:  \n",
    "     `param -= learning_rate * param.grad`  \n",
    "   - Zero gradients after each update.\n",
    "\n",
    "5. **Minibatch Training**  \n",
    "   - Shuffle dataset each epoch.  \n",
    "   - Train in batches for efficiency.\n",
    "\n",
    "6. **Learning Rate Tuning**  \n",
    "   - Try a small range of learning rates.  \n",
    "   - Pick one that leads to fastest stable loss decrease.\n",
    "\n",
    "---\n",
    "\n",
    "## 6ï¸âƒ£ Train/Validation/Test Split\n",
    "\n",
    "- Split dataset: 80% train, 10% val, 10% test.  \n",
    "- Train only on training set, tune hyperparameters on val set, report final test loss.\n",
    "\n",
    "---\n",
    "\n",
    "## 7ï¸âƒ£ Experiments & Insights\n",
    "\n",
    "- **Bigger Hidden Layer**: more capacity, better fit.  \n",
    "- **Bigger Embedding Dim**: richer character representations.  \n",
    "- **Regularization**: optional L2 penalty to reduce overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## 8ï¸âƒ£ Sampling from the Model\n",
    "\n",
    "To generate a name:\n",
    "1. Start with `.` tokens as context.\n",
    "2. Predict probability distribution over next char.\n",
    "3. Sample a char from distribution.\n",
    "4. Shift context, append new char.\n",
    "5. Repeat until `.` is generated (end of name).\n",
    "\n",
    "---\n",
    "\n",
    "## 9ï¸âƒ£ Visualizing Embeddings\n",
    "\n",
    "- After training, the embedding matrix `C` contains a vector for each character.  \n",
    "- You can plot them in 2D (e.g., PCA or t-SNE) to see relationships between characters.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”Ÿ Full Process Recap\n",
    "\n",
    "1. Load data & build vocab.  \n",
    "2. Create contextâ€“target pairs.  \n",
    "3. Encode to integers.  \n",
    "4. Build embedding + MLP layers.  \n",
    "5. Train with cross-entropy loss.  \n",
    "6. Tune hyperparameters.  \n",
    "7. Generate samples.  \n",
    "8. Visualize learned embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "**End Goal**: A fully trained MLP that can generate realistic-looking new names purely from character-level probabilities learned on the training set.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "369cf9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "# let's code \n",
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee597aef",
   "metadata": {},
   "source": [
    "Video Transcript - summary \n",
    "# Multi-Layer Perceptron Language Model Implementation Summary\n",
    "\n",
    "## Introduction and Problem Statement\n",
    "\n",
    "This lecture continues implementing \"makemore\" by transitioning from bigram language models to multi-layer perceptrons (MLPs). The previous bigram model used single character context to predict the next character through count-based probability tables, where each row summed to one.\n",
    "\n",
    "**Core Problem with Bigram Models:**\n",
    "- Limited to single character context produces poor, non-name-like predictions\n",
    "- Scaling to more context creates exponential growth in table size:\n",
    "  - 1 character context: 27 possibilities\n",
    "  - 2 character context: 27 Ã— 27 = 729 possibilities  \n",
    "  - 3 character context: ~20,000 possibilities\n",
    "- Results in sparse counts and system breakdown\n",
    "\n",
    "## Theoretical Foundation: Bengio et al. 2003\n",
    "\n",
    "The implementation follows the influential Bengio et al. 2003 paper on neural language models.\n",
    "\n",
    "**Paper's Approach:**\n",
    "- 17,000 word vocabulary embedded in 30-dimensional feature vectors\n",
    "- Words initially positioned randomly in embedding space\n",
    "- Through backpropagation, semantically similar words cluster together\n",
    "- Identical modeling approach: maximize log likelihood of training data\n",
    "\n",
    "**Key Insight - Generalization Through Embeddings:**\n",
    "Example: \"A dog was running in a ___\"\n",
    "- Even if exact phrase never seen in training, model can generalize\n",
    "- If seen \"The dog was running in a ___\", embeddings for \"a\" and \"the\" learn similarity\n",
    "- Knowledge transfers through embedding space to novel scenarios\n",
    "- Similar concept applies to \"cats\" and \"dogs\" as animals\n",
    "\n",
    "**Neural Network Architecture:**\n",
    "- Input: 3 previous words (indices 0-16999)\n",
    "- Embedding lookup table C: 17,000 Ã— 30 matrix\n",
    "- Each word index retrieves corresponding 30-dimensional embedding\n",
    "- Input layer: 90 neurons (3 words Ã— 30 dimensions)\n",
    "- Hidden layer: Hyperparameter size (e.g., 100 neurons), fully connected\n",
    "- Tanh nonlinearity\n",
    "- Output layer: 17,000 neurons (one per possible next word), fully connected\n",
    "- Softmax normalization for probability distribution\n",
    "\n",
    "**Training Process:**\n",
    "- Parameters include embedding table C, hidden layer weights/biases, output layer weights/biases\n",
    "- All optimized via backpropagation\n",
    "- Most computation in expensive output layer due to vocabulary size\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "### Dataset Preparation\n",
    "```python\n",
    "block_size = 3  # Context length (3 characters predict 4th)\n",
    "```\n",
    "\n",
    "**Dataset Creation Process:**\n",
    "- Build examples from character sequences with padding dots\n",
    "- For word \"emma\": context [...] â†’ e, [..e] â†’ m, [.em] â†’ m, [emm] â†’ a, [mma] â†’ .\n",
    "- Generate X (contexts) and Y (target characters) arrays\n",
    "- 32,000 names total, initially testing on first 5 words (32 examples)\n",
    "\n",
    "### Embedding Implementation\n",
    "\n",
    "**Embedding Lookup Table:**\n",
    "- 27 possible characters embedded in lower-dimensional space\n",
    "- Start with 2D embeddings for visualization: 27 Ã— 2 matrix C\n",
    "- Random initialization\n",
    "\n",
    "**Indexing Methods:**\n",
    "1. Direct indexing: `C[5]` retrieves 5th row\n",
    "2. One-hot equivalent: `F.one_hot(torch.tensor(5), 27).float() @ C`\n",
    "   - Demonstrates embedding as first neural network layer\n",
    "   - Direct indexing preferred for efficiency\n",
    "\n",
    "**Batch Processing:**\n",
    "- PyTorch supports flexible indexing with lists, tensors, multi-dimensional arrays\n",
    "- `C[X]` embeds entire batch simultaneously\n",
    "- Output shape: 32 Ã— 3 Ã— 2 (batch_size Ã— context_length Ã— embedding_dim)\n",
    "\n",
    "### Neural Network Layers\n",
    "\n",
    "**Hidden Layer Construction:**\n",
    "- Input: Concatenated embeddings (3 Ã— 2 = 6 dimensions)\n",
    "- Two concatenation approaches:\n",
    "  1. `torch.cat([emb[:, 0], emb[:, 1], emb[:, 2]], dim=1)` - creates new tensor\n",
    "  2. `emb.view(32, 6)` - efficient view manipulation (preferred)\n",
    "\n",
    "**View Operation Efficiency:**\n",
    "- PyTorch tensors have underlying 1D storage\n",
    "- `view()` manipulates tensor metadata (strides, shapes) without copying data\n",
    "- Extremely efficient compared to concatenation which creates new memory\n",
    "\n",
    "**Hidden Layer Forward Pass:**\n",
    "```python\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "```\n",
    "- W1: 6 Ã— 100 weight matrix\n",
    "- b1: 100-dimensional bias vector\n",
    "- Broadcasting ensures bias added to each example\n",
    "\n",
    "**Output Layer:**\n",
    "```python\n",
    "logits = h @ W2 + b2\n",
    "```\n",
    "- W2: 100 Ã— 27 weight matrix  \n",
    "- b2: 27-dimensional bias vector\n",
    "- Output: 32 Ã— 27 logits\n",
    "\n",
    "### Loss Calculation\n",
    "\n",
    "**Manual Implementation:**\n",
    "```python\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdim=True)\n",
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "```\n",
    "\n",
    "**PyTorch Built-in (Preferred):**\n",
    "```python\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "```\n",
    "\n",
    "**Advantages of F.cross_entropy:**\n",
    "1. **Efficiency**: Fused kernels, no intermediate tensors\n",
    "2. **Numerical stability**: Handles extreme logit values via offset subtraction\n",
    "3. **Simpler backward pass**: Analytically derived derivatives\n",
    "\n",
    "**Numerical Stability Example:**\n",
    "- Large positive logits (e.g., 100) cause overflow in exp()\n",
    "- F.cross_entropy subtracts maximum logit value internally\n",
    "- Exploits property: softmax(x) = softmax(x + c) for any constant c\n",
    "\n",
    "### Training Implementation\n",
    "\n",
    "**Basic Training Loop:**\n",
    "```python\n",
    "for _ in range(1000):\n",
    "    # Zero gradients\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    \n",
    "    # Forward pass\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Parameter update\n",
    "    for p in parameters:\n",
    "        p.data += -learning_rate * p.grad\n",
    "```\n",
    "\n",
    "**Overfitting Demonstration:**\n",
    "- 3,400 parameters vs 32 examples â†’ easy overfitting\n",
    "- Achieves very low loss but not exactly zero\n",
    "- Limitation: same input contexts can have different valid outputs\n",
    "\n",
    "### Mini-batch Training\n",
    "\n",
    "**Problem**: Full dataset (228,000 examples) too slow per iteration\n",
    "\n",
    "**Solution**: Mini-batch gradient descent\n",
    "```python\n",
    "ix = torch.randint(0, X.shape[0], (32,))  # Random batch indices\n",
    "loss = F.cross_entropy(logits[ix], Y[ix])\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Much faster iterations\n",
    "- Approximate gradients sufficient for progress\n",
    "- Better to take many approximate steps than few exact steps\n",
    "\n",
    "### Learning Rate Selection\n",
    "\n",
    "**Learning Rate Range Finding:**\n",
    "1. Test very low rates (e.g., 0.001) â†’ minimal progress\n",
    "2. Test very high rates (e.g., 0.1, 1.0) â†’ instability/explosion\n",
    "3. Use exponential spacing: `torch.linspace(-3, 0, 1000)` â†’ `10**lre`\n",
    "4. Plot learning rate vs loss to find optimal range\n",
    "5. Choose rate from \"valley\" region of plot\n",
    "\n",
    "**Typical Process:**\n",
    "- Start with found learning rate\n",
    "- Train until plateau\n",
    "- Apply learning rate decay (10x reduction)\n",
    "- Continue training\n",
    "\n",
    "### Train/Validation/Test Splits\n",
    "\n",
    "**Problem**: Training loss alone insufficient for model evaluation\n",
    "- Models can memorize training data (overfitting)\n",
    "- Need generalization assessment\n",
    "\n",
    "**Standard Split:**\n",
    "- **Training (80%)**: Parameter optimization via gradient descent  \n",
    "- **Validation/Dev (10%)**: Hyperparameter tuning\n",
    "- **Test (10%)**: Final performance evaluation (use sparingly)\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "n1 = int(0.8 * len(words))  # 80% train\n",
    "n2 = int(0.9 * len(words))  # 90% train+dev\n",
    "X_train, Y_train = build_dataset(words[:n1])\n",
    "X_dev, Y_dev = build_dataset(words[n1:n2])  \n",
    "X_test, Y_test = build_dataset(words[n2:])\n",
    "```\n",
    "\n",
    "### Model Scaling and Optimization\n",
    "\n",
    "**Underfitting Diagnosis:**\n",
    "- Training loss â‰ˆ Validation loss indicates underfitting\n",
    "- Solution: Increase model capacity\n",
    "\n",
    "**Scaling Experiments:**\n",
    "1. **Hidden layer size**: 100 â†’ 300 neurons\n",
    "2. **Embedding dimension**: 2 â†’ 10 dimensions\n",
    "3. **Context length**: 3 â†’ larger block_size\n",
    "\n",
    "**Embedding Visualization (2D case):**\n",
    "- Plot character embeddings after training\n",
    "- Reveals learned structure: vowels cluster together\n",
    "- Special characters (q, .) positioned as outliers\n",
    "- Demonstrates meaningful learned representations\n",
    "\n",
    "**Final Architecture:**\n",
    "- 10-dimensional character embeddings\n",
    "- 200 hidden neurons  \n",
    "- Input: 30 dimensions (3 characters Ã— 10D embeddings)\n",
    "- ~11,000 total parameters\n",
    "\n",
    "**Training Schedule:**\n",
    "- 100k steps at learning_rate=0.1\n",
    "- 100k steps at learning_rate=0.01 (decay)\n",
    "- Achieved ~2.17 validation loss (surpassing 2.45 bigram baseline)\n",
    "\n",
    "### Sampling from Trained Model\n",
    "\n",
    "**Generation Process:**\n",
    "```python\n",
    "context = [0, 0, 0]  # Start with dots\n",
    "for _ in range(20):\n",
    "    emb = C[context]\n",
    "    h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    ix = torch.multinomial(probs, 1).item()\n",
    "    context = context[1:] + [ix]  # Shift context window\n",
    "    if ix == 0: break  # Stop at end token\n",
    "```\n",
    "\n",
    "**Results:**\n",
    "- Generated names significantly more name-like than bigram model\n",
    "- Examples: \"ham\", \"joes\" - showing improved quality\n",
    "- Still room for improvement with further optimization\n",
    "\n",
    "### Optimization Challenges and Improvements\n",
    "\n",
    "**Available Tuning Parameters:**\n",
    "1. Hidden layer neuron count\n",
    "2. Embedding dimensionality  \n",
    "3. Context length (block_size)\n",
    "4. Learning rate schedule\n",
    "5. Batch size\n",
    "6. Training duration\n",
    "7. Regularization techniques\n",
    "\n",
    "**Best Practices:**\n",
    "- Systematic hyperparameter search rather than random tuning\n",
    "- Monitor both training and validation performance\n",
    "- Use learning rate scheduling\n",
    "- Implement proper gradient tracking and visualization\n",
    "\n",
    "**Final Performance:**\n",
    "- Validation loss: 2.17 \n",
    "- Significant improvement over bigram baseline (2.45)\n",
    "- Demonstrates effectiveness of neural approach with learned embeddings\n",
    "\n",
    "This implementation successfully demonstrates the transition from simple statistical models to neural networks, showing how embeddings enable better generalization and the importance of proper training methodology including data splitting and hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2be27bec-1a0b-475e-98c1-a5458be7383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plan of action\n",
    "# words ,  split into block-sized-context and outputs \n",
    "# split into train , dev , testing data\n",
    "# emdedding vector -> vocab     X embed_dimensions \n",
    "# context_size*embed_dimensions X num_neurons_lay_1\n",
    "# 2nd layer: num_neurons_lay_1  X num_poss_outputs\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "de0e5461-deb7-45b8-81d4-bb738be4418d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stoi :  {'.': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "itos :  {0: '.', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n",
      "\n",
      "input contexts:  ['...', '..e', '.em', 'emm', 'mma', '...', '..o', '.ol', 'oli', 'liv', 'ivi', 'via', '...', '..a', '.av', 'ava', '...', '..i', '.is', 'isa', 'sab', 'abe', 'bel', 'ell', 'lla', '...', '..s', '.so', 'sop', 'oph', 'phi', 'hia', '...', '..c', '.ch', 'cha', 'har', 'arl', 'rlo', 'lot', 'ott', 'tte', '...', '..m', '.mi', 'mia', '...', '..a', '.am', 'ame', 'mel', 'eli', 'lia', '...', '..h', '.ha', 'har', 'arp', 'rpe', 'per', '...', '..e', '.ev', 'eve', 'vel', 'ely', 'lyn']\n",
      "\n",
      "outputs:  ['e', 'm', 'm', 'a', '.', 'o', 'l', 'i', 'v', 'i', 'a', '.', 'a', 'v', 'a', '.', 'i', 's', 'a', 'b', 'e', 'l', 'l', 'a', '.', 's', 'o', 'p', 'h', 'i', 'a', '.', 'c', 'h', 'a', 'r', 'l', 'o', 't', 't', 'e', '.', 'm', 'i', 'a', '.', 'a', 'm', 'e', 'l', 'i', 'a', '.', 'h', 'a', 'r', 'p', 'e', 'r', '.', 'e', 'v', 'e', 'l', 'y', 'n', '.']\n"
     ]
    }
   ],
   "source": [
    "# start\n",
    "words = open(\"names.txt\" , \"r\").read().splitlines()\n",
    "\n",
    "chars = set(''.join(words))\n",
    "chars.add('.')\n",
    "\n",
    "# itos , stoi\n",
    "\n",
    "stoi , itos = {} , {}\n",
    "sorted_chars = sorted(chars)\n",
    "for index , char in enumerate(sorted_chars):\n",
    "    stoi[char] =  index\n",
    "    itos[index] =  char\n",
    "\n",
    "print(\"stoi : \" , stoi)\n",
    "print(\"itos : \" , itos)\n",
    "print()\n",
    "\n",
    "block_size = 3\n",
    "\n",
    "words = words[:10]\n",
    "\n",
    "xs , ys = [] , []\n",
    "for word in words:\n",
    "    word = '.' + word + '.'\n",
    "    context = '.'*block_size\n",
    "    for i in range(len(word)-1):\n",
    "        context = context[1:] + word[i]\n",
    "        xs.append(context)\n",
    "        ys.append(word[i+1])\n",
    "\n",
    "# for i in range(len(xs)):\n",
    "#     print(xs[i] , ys[i])\n",
    "print(\"input contexts: \",xs)\n",
    "print()\n",
    "print(\"outputs: \" , ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a7fe2ab-202c-465f-81a1-2874a13a748f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 60\n",
      "['...', '..e', '.em', 'emm', 'mma', '...', '..o', '.ol', 'oli', 'liv', 'ivi', 'via', '...', '..a', '.av', 'ava', '...', '..i', '.is', 'isa', 'sab', 'abe', 'bel', 'ell', 'lla', '...', '..s', '.so', 'sop', 'oph', 'phi', 'hia', '...', '..c', '.ch', 'cha', 'har', 'arl', 'rlo', 'lot', 'ott', 'tte', '...', '..m', '.mi', 'mia', '...', '..a', '.am', 'ame', 'mel', 'eli', 'lia'] ['e', 'm', 'm', 'a', '.', 'o', 'l', 'i', 'v', 'i', 'a', '.', 'a', 'v', 'a', '.', 'i', 's', 'a', 'b', 'e', 'l', 'l', 'a', '.', 's', 'o', 'p', 'h', 'i', 'a', '.', 'c', 'h', 'a', 'r', 'l', 'o', 't', 't', 'e', '.', 'm', 'i', 'a', '.', 'a', 'm', 'e', 'l', 'i', 'a', '.'] ['..h', '.ha', 'har', 'arp', 'rpe', 'per'] ['a', 'r', 'p', 'e', 'r', '.'] ['..e', '.ev', 'eve', 'vel', 'ely', 'lyn'] ['v', 'e', 'l', 'y', 'n', '.']\n"
     ]
    }
   ],
   "source": [
    "# spitting the data\n",
    "\n",
    "n1 = int(0.8*len(xs))\n",
    "n2 = int(0.9*len(xs))\n",
    "\n",
    "print(n1,n2)\n",
    "\n",
    "Xtr = xs[:n1]\n",
    "Ytr = ys[:n1]\n",
    "\n",
    "Xdev = xs[n1+1:n2]\n",
    "Ydev = ys[n1+1:n2]\n",
    "\n",
    "Xtst = xs[n2+1:]\n",
    "Ytst = ys[n2+1:]\n",
    "\n",
    "print(Xtr,Ytr,Xdev,Ydev,Xtst,Ytst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a33403a0-6491-4b2b-be43-d3ddc20da9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7959, 0.8220],\n",
      "        [0.7867, 0.3392],\n",
      "        [0.2167, 0.6377],\n",
      "        [0.3915, 0.7522],\n",
      "        [0.0396, 0.3702],\n",
      "        [0.9246, 0.8007],\n",
      "        [0.4372, 0.8419],\n",
      "        [0.9572, 0.0074],\n",
      "        [0.3668, 0.8365],\n",
      "        [0.4535, 0.2865],\n",
      "        [0.7329, 0.1500],\n",
      "        [0.6216, 0.1936],\n",
      "        [0.7426, 0.8234],\n",
      "        [0.4672, 0.9077],\n",
      "        [0.9147, 0.2395],\n",
      "        [0.0253, 0.6093],\n",
      "        [0.6715, 0.0908],\n",
      "        [0.4667, 0.2945],\n",
      "        [0.5290, 0.5700],\n",
      "        [0.7545, 0.3256],\n",
      "        [0.5646, 0.5873],\n",
      "        [0.8192, 0.5292],\n",
      "        [0.5906, 0.3194],\n",
      "        [0.9318, 0.3357],\n",
      "        [0.7719, 0.6991],\n",
      "        [0.3833, 0.3892],\n",
      "        [0.3543, 0.5844]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# embedding vector\n",
    "# hyper paramameters\n",
    "\n",
    "block_size = 3\n",
    "emb_size = 2\n",
    "vocab_size = len(chars)\n",
    "hidden_layers_neurons = 100\n",
    "\n",
    "# parameters\n",
    "\n",
    "C = torch.rand((vocab_size , emb_size) , dtype = torch.float32 , requires_grad = True)\n",
    "print(C)\n",
    "\n",
    "W1 = torch.rand((block_size * emb_size , vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd72d46b-6c3b-4c4c-95fd-8b1120cbf1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

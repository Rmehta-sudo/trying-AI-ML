{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2307e1ab",
   "metadata": {},
   "source": [
    "Gemini generated In depth summary \n",
    "Based on the video chapters and your code, here is a step-by-step guide to implementing the multi-layer perceptron (MLP) language model.\n",
    "\n",
    "***\n",
    "\n",
    "### Part 1: Dataset and Model Architecture\n",
    "\n",
    "1.  **Create the Dataset**:\n",
    "    * Load the `names.txt` file and define your character vocabulary (`stoi`, `itos`).\n",
    "    * Choose a `block_size` (context length), which is the number of previous characters used to predict the next one.\n",
    "    * Iterate through each word and create a list of contexts (`X`) and their corresponding next characters (`Y`). The `.` token is used to pad the context at the beginning and signal the end of a word.\n",
    "    * Shuffle the words and split the dataset into **training (80%)**, **validation (10%)**, and **test (10%)** sets. Use `Xtr, Ytr`, `Xdev, Ydev`, and `Xte, Yte` to store these.\n",
    "2.  **Initialize the Neural Network**:\n",
    "    * **Embedding Layer**: Create an embedding lookup table `C` as a `27x10` tensor. Each row represents a character, and the 10 values are its **embedding**. This is a trainable parameter.\n",
    "    * **Hidden Layer**: Define the weights `W1` (a `30x200` tensor, `30` because `block_size * embedding_size = 3 * 10`) and biases `b1` (`200` elements).\n",
    "    * **Output Layer**: Define the weights `W2` (`200x27`) and biases `b2` (`27` elements). The output size matches the number of characters.\n",
    "    * Put all these tensors (`C, W1, b1, W2, b2`) into a list called `parameters` and set `requires_grad=True` for all of them. \n",
    "\n",
    "***\n",
    "\n",
    "### Part 2: Training and Evaluation\n",
    "\n",
    "1.  **Set Up the Training Loop**:\n",
    "    * Loop for a specified number of iterations (e.g., 200,000).\n",
    "    * For each iteration, construct a **minibatch** by randomly selecting a small number of indices (`ix`) from your training data `Xtr` and `Ytr`.\n",
    "2.  **Forward Pass**:\n",
    "    * Perform an **embedding lookup**: Use `Xtr[ix]` to get the embeddings from `C`, resulting in a tensor of shape `(batch_size, block_size, embedding_size)`.\n",
    "    * Reshape the embeddings into a single vector per example using `.view(-1, block_size * embedding_size)`.\n",
    "    * Pass this through the hidden layer: compute `emb.view(...) @ W1 + b1` and apply the **tanh activation function**.\n",
    "    * Pass the hidden layer output through the output layer: compute `h @ W2 + b2`. This gives you the `logits`.\n",
    "    * Calculate the **loss** using PyTorch's `F.cross_entropy`, passing in the `logits` and the labels `Ytr[ix]`. This function efficiently combines `softmax`, `log`, and `mean`.\n",
    "3.  **Backward Pass and Update**:\n",
    "    * Zero out the gradients for all parameters by setting `p.grad = None` for each parameter `p`.\n",
    "    * Call `loss.backward()` to compute the gradients.\n",
    "    * Update the parameters using a learning rate: `p.data += -lr * p.grad`. Use a decaying learning rate, such as starting with `0.1` and dropping to `0.01` after a certain number of steps.\n",
    "4.  **Evaluate and Visualize**:\n",
    "    * After training, evaluate the loss on the validation set (`Xdev, Ydev`) to check for **overfitting**.\n",
    "    * Visualize the embedding space by plotting the first two dimensions of the `C` matrix. Each point represents a character.\n",
    "\n",
    "***\n",
    "\n",
    "### Part 3: Sampling and Conclusion\n",
    "\n",
    "1.  **Sample from the Model**:\n",
    "    * Start with an initial `context` of all `.` tokens.\n",
    "    * Enter a loop that continues until the model predicts a `.` token.\n",
    "    * Inside the loop, get the embeddings for the current `context` from the trained `C` matrix.\n",
    "    * Perform a forward pass through the hidden and output layers to get the `logits`.\n",
    "    * Apply `F.softmax` to the logits to get probabilities.\n",
    "    * Use `torch.multinomial` to sample the index of the next character.\n",
    "    * Append the new index to your output list and update the `context` by sliding the window.\n",
    "    * Finally, join the characters from the output list to form a new name.\n",
    "\n",
    "    ---\n",
    "    ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b893817",
   "metadata": {},
   "source": [
    "GPT summary walkthrough , with lesser help , covering all ideas in the code \n",
    "---\n",
    "# ðŸ§  Character-Level MLP Language Model â€” Complete From-Scratch Walkthrough\n",
    "\n",
    "This document summarizes the **entire lecture** so you can reimplement the model without looking at the original code.  \n",
    "It covers **every step**: data prep, architecture, training, and sampling.\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£ Problem Setup\n",
    "\n",
    "We want to train a **character-level language model** that generates new names.  \n",
    "The model will be an **MLP** (multi-layer perceptron) trained from scratch on a dataset of names.\n",
    "\n",
    "The modelâ€™s job:  \n",
    "Given a **context** (a fixed number of previous characters), predict the **next character**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2ï¸âƒ£ Data Preparation\n",
    "\n",
    "1. **Load Dataset**  \n",
    "   - Read the `names.txt` file into a list of strings, one name per line.  \n",
    "   - Inspect dataset: size, min/max length.\n",
    "\n",
    "2. **Define Vocabulary**  \n",
    "   - Collect all unique characters in the dataset.  \n",
    "   - Add a special `.` token for start/end of a word.  \n",
    "   - Create two dictionaries:\n",
    "     - `stoi`: char â†’ index\n",
    "     - `itos`: index â†’ char\n",
    "\n",
    "3. **Context Windows**  \n",
    "   - Choose a fixed context size `block_size` (e.g., 3).  \n",
    "   - For each name:\n",
    "     - Pad with `.` tokens at the start.\n",
    "     - Slide a window of length `block_size` across the name.\n",
    "     - The window characters are the **input**.\n",
    "     - The next character is the **target**.\n",
    "\n",
    "4. **Numerical Encoding**  \n",
    "   - Map characters in the context and the target to integers using `stoi`.  \n",
    "   - Store all contexts in an integer tensor `X`.  \n",
    "   - Store all targets in integer tensor `Y`.\n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£ Model Architecture\n",
    "\n",
    "The MLP has three main parts:\n",
    "\n",
    "1. **Embedding Layer**  \n",
    "   - A learnable matrix `C` of size `(vocab_size, embedding_dim)`.  \n",
    "   - Converts each character index into a dense vector.\n",
    "\n",
    "2. **Hidden Layer**  \n",
    "   - Flatten all embeddings for the context into a single vector.  \n",
    "   - Apply a linear transformation: `h = tanh(X @ W1 + b1)`  \n",
    "     - `W1`: weight matrix of shape `(context_size * embedding_dim, hidden_size)`\n",
    "     - `b1`: bias vector of length `hidden_size`.\n",
    "\n",
    "3. **Output Layer**  \n",
    "   - Map hidden activations to vocabulary logits: `logits = h @ W2 + b2`  \n",
    "     - `W2`: weight matrix `(hidden_size, vocab_size)`\n",
    "     - `b2`: bias vector `(vocab_size,)`\n",
    "\n",
    "---\n",
    "\n",
    "## 4ï¸âƒ£ Loss Function\n",
    "\n",
    "We use **cross-entropy loss** between predicted logits and target indices.\n",
    "\n",
    "Two ways to compute:\n",
    "1. **Manual**: softmax â†’ log â†’ negative log likelihood â†’ mean over batch.\n",
    "2. **Built-in**: `torch.nn.functional.cross_entropy(logits, targets)`.\n",
    "\n",
    "---\n",
    "\n",
    "## 5ï¸âƒ£ Training Loop\n",
    "\n",
    "1. **Initialization**  \n",
    "   - Randomly initialize all weights with small values (e.g., normal distribution).  \n",
    "   - Zero biases.\n",
    "\n",
    "2. **Forward Pass**  \n",
    "   - Embed context characters â†’ concatenate â†’ hidden layer â†’ output layer.  \n",
    "   - Compute loss vs targets.\n",
    "\n",
    "3. **Backward Pass**  \n",
    "   - Call `.backward()` on loss to compute gradients.\n",
    "\n",
    "4. **Parameter Update**  \n",
    "   - Update all parameters with gradient descent:  \n",
    "     `param -= learning_rate * param.grad`  \n",
    "   - Zero gradients after each update.\n",
    "\n",
    "5. **Minibatch Training**  \n",
    "   - Shuffle dataset each epoch.  \n",
    "   - Train in batches for efficiency.\n",
    "\n",
    "6. **Learning Rate Tuning**  \n",
    "   - Try a small range of learning rates.  \n",
    "   - Pick one that leads to fastest stable loss decrease.\n",
    "\n",
    "---\n",
    "\n",
    "## 6ï¸âƒ£ Train/Validation/Test Split\n",
    "\n",
    "- Split dataset: 80% train, 10% val, 10% test.  \n",
    "- Train only on training set, tune hyperparameters on val set, report final test loss.\n",
    "\n",
    "---\n",
    "\n",
    "## 7ï¸âƒ£ Experiments & Insights\n",
    "\n",
    "- **Bigger Hidden Layer**: more capacity, better fit.  \n",
    "- **Bigger Embedding Dim**: richer character representations.  \n",
    "- **Regularization**: optional L2 penalty to reduce overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## 8ï¸âƒ£ Sampling from the Model\n",
    "\n",
    "To generate a name:\n",
    "1. Start with `.` tokens as context.\n",
    "2. Predict probability distribution over next char.\n",
    "3. Sample a char from distribution.\n",
    "4. Shift context, append new char.\n",
    "5. Repeat until `.` is generated (end of name).\n",
    "\n",
    "---\n",
    "\n",
    "## 9ï¸âƒ£ Visualizing Embeddings\n",
    "\n",
    "- After training, the embedding matrix `C` contains a vector for each character.  \n",
    "- You can plot them in 2D (e.g., PCA or t-SNE) to see relationships between characters.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”Ÿ Full Process Recap\n",
    "\n",
    "1. Load data & build vocab.  \n",
    "2. Create contextâ€“target pairs.  \n",
    "3. Encode to integers.  \n",
    "4. Build embedding + MLP layers.  \n",
    "5. Train with cross-entropy loss.  \n",
    "6. Tune hyperparameters.  \n",
    "7. Generate samples.  \n",
    "8. Visualize learned embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "**End Goal**: A fully trained MLP that can generate realistic-looking new names purely from character-level probabilities learned on the training set.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "369cf9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "# let's code \n",
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee597aef",
   "metadata": {},
   "source": [
    "Video Transcript - summary \n",
    "# Multi-Layer Perceptron Language Model Implementation Summary\n",
    "\n",
    "## Introduction and Problem Statement\n",
    "\n",
    "This lecture continues implementing \"makemore\" by transitioning from bigram language models to multi-layer perceptrons (MLPs). The previous bigram model used single character context to predict the next character through count-based probability tables, where each row summed to one.\n",
    "\n",
    "**Core Problem with Bigram Models:**\n",
    "- Limited to single character context produces poor, non-name-like predictions\n",
    "- Scaling to more context creates exponential growth in table size:\n",
    "  - 1 character context: 27 possibilities\n",
    "  - 2 character context: 27 Ã— 27 = 729 possibilities  \n",
    "  - 3 character context: ~20,000 possibilities\n",
    "- Results in sparse counts and system breakdown\n",
    "\n",
    "## Theoretical Foundation: Bengio et al. 2003\n",
    "\n",
    "The implementation follows the influential Bengio et al. 2003 paper on neural language models.\n",
    "\n",
    "**Paper's Approach:**\n",
    "- 17,000 word vocabulary embedded in 30-dimensional feature vectors\n",
    "- Words initially positioned randomly in embedding space\n",
    "- Through backpropagation, semantically similar words cluster together\n",
    "- Identical modeling approach: maximize log likelihood of training data\n",
    "\n",
    "**Key Insight - Generalization Through Embeddings:**\n",
    "Example: \"A dog was running in a ___\"\n",
    "- Even if exact phrase never seen in training, model can generalize\n",
    "- If seen \"The dog was running in a ___\", embeddings for \"a\" and \"the\" learn similarity\n",
    "- Knowledge transfers through embedding space to novel scenarios\n",
    "- Similar concept applies to \"cats\" and \"dogs\" as animals\n",
    "\n",
    "**Neural Network Architecture:**\n",
    "- Input: 3 previous words (indices 0-16999)\n",
    "- Embedding lookup table C: 17,000 Ã— 30 matrix\n",
    "- Each word index retrieves corresponding 30-dimensional embedding\n",
    "- Input layer: 90 neurons (3 words Ã— 30 dimensions)\n",
    "- Hidden layer: Hyperparameter size (e.g., 100 neurons), fully connected\n",
    "- Tanh nonlinearity\n",
    "- Output layer: 17,000 neurons (one per possible next word), fully connected\n",
    "- Softmax normalization for probability distribution\n",
    "\n",
    "**Training Process:**\n",
    "- Parameters include embedding table C, hidden layer weights/biases, output layer weights/biases\n",
    "- All optimized via backpropagation\n",
    "- Most computation in expensive output layer due to vocabulary size\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "### Dataset Preparation\n",
    "```python\n",
    "block_size = 3  # Context length (3 characters predict 4th)\n",
    "```\n",
    "\n",
    "**Dataset Creation Process:**\n",
    "- Build examples from character sequences with padding dots\n",
    "- For word \"emma\": context [...] â†’ e, [..e] â†’ m, [.em] â†’ m, [emm] â†’ a, [mma] â†’ .\n",
    "- Generate X (contexts) and Y (target characters) arrays\n",
    "- 32,000 names total, initially testing on first 5 words (32 examples)\n",
    "\n",
    "### Embedding Implementation\n",
    "\n",
    "**Embedding Lookup Table:**\n",
    "- 27 possible characters embedded in lower-dimensional space\n",
    "- Start with 2D embeddings for visualization: 27 Ã— 2 matrix C\n",
    "- Random initialization\n",
    "\n",
    "**Indexing Methods:**\n",
    "1. Direct indexing: `C[5]` retrieves 5th row\n",
    "2. One-hot equivalent: `F.one_hot(torch.tensor(5), 27).float() @ C`\n",
    "   - Demonstrates embedding as first neural network layer\n",
    "   - Direct indexing preferred for efficiency\n",
    "\n",
    "**Batch Processing:**\n",
    "- PyTorch supports flexible indexing with lists, tensors, multi-dimensional arrays\n",
    "- `C[X]` embeds entire batch simultaneously\n",
    "- Output shape: 32 Ã— 3 Ã— 2 (batch_size Ã— context_length Ã— embedding_dim)\n",
    "\n",
    "### Neural Network Layers\n",
    "\n",
    "**Hidden Layer Construction:**\n",
    "- Input: Concatenated embeddings (3 Ã— 2 = 6 dimensions)\n",
    "- Two concatenation approaches:\n",
    "  1. `torch.cat([emb[:, 0], emb[:, 1], emb[:, 2]], dim=1)` - creates new tensor\n",
    "  2. `emb.view(32, 6)` - efficient view manipulation (preferred)\n",
    "\n",
    "**View Operation Efficiency:**\n",
    "- PyTorch tensors have underlying 1D storage\n",
    "- `view()` manipulates tensor metadata (strides, shapes) without copying data\n",
    "- Extremely efficient compared to concatenation which creates new memory\n",
    "\n",
    "**Hidden Layer Forward Pass:**\n",
    "```python\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "```\n",
    "- W1: 6 Ã— 100 weight matrix\n",
    "- b1: 100-dimensional bias vector\n",
    "- Broadcasting ensures bias added to each example\n",
    "\n",
    "**Output Layer:**\n",
    "```python\n",
    "logits = h @ W2 + b2\n",
    "```\n",
    "- W2: 100 Ã— 27 weight matrix  \n",
    "- b2: 27-dimensional bias vector\n",
    "- Output: 32 Ã— 27 logits\n",
    "\n",
    "### Loss Calculation\n",
    "\n",
    "**Manual Implementation:**\n",
    "```python\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdim=True)\n",
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "```\n",
    "\n",
    "**PyTorch Built-in (Preferred):**\n",
    "```python\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "```\n",
    "\n",
    "**Advantages of F.cross_entropy:**\n",
    "1. **Efficiency**: Fused kernels, no intermediate tensors\n",
    "2. **Numerical stability**: Handles extreme logit values via offset subtraction\n",
    "3. **Simpler backward pass**: Analytically derived derivatives\n",
    "\n",
    "**Numerical Stability Example:**\n",
    "- Large positive logits (e.g., 100) cause overflow in exp()\n",
    "- F.cross_entropy subtracts maximum logit value internally\n",
    "- Exploits property: softmax(x) = softmax(x + c) for any constant c\n",
    "\n",
    "### Training Implementation\n",
    "\n",
    "**Basic Training Loop:**\n",
    "```python\n",
    "for _ in range(1000):\n",
    "    # Zero gradients\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    \n",
    "    # Forward pass\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Parameter update\n",
    "    for p in parameters:\n",
    "        p.data += -learning_rate * p.grad\n",
    "```\n",
    "\n",
    "**Overfitting Demonstration:**\n",
    "- 3,400 parameters vs 32 examples â†’ easy overfitting\n",
    "- Achieves very low loss but not exactly zero\n",
    "- Limitation: same input contexts can have different valid outputs\n",
    "\n",
    "### Mini-batch Training\n",
    "\n",
    "**Problem**: Full dataset (228,000 examples) too slow per iteration\n",
    "\n",
    "**Solution**: Mini-batch gradient descent\n",
    "```python\n",
    "ix = torch.randint(0, X.shape[0], (32,))  # Random batch indices\n",
    "loss = F.cross_entropy(logits[ix], Y[ix])\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Much faster iterations\n",
    "- Approximate gradients sufficient for progress\n",
    "- Better to take many approximate steps than few exact steps\n",
    "\n",
    "### Learning Rate Selection\n",
    "\n",
    "**Learning Rate Range Finding:**\n",
    "1. Test very low rates (e.g., 0.001) â†’ minimal progress\n",
    "2. Test very high rates (e.g., 0.1, 1.0) â†’ instability/explosion\n",
    "3. Use exponential spacing: `torch.linspace(-3, 0, 1000)` â†’ `10**lre`\n",
    "4. Plot learning rate vs loss to find optimal range\n",
    "5. Choose rate from \"valley\" region of plot\n",
    "\n",
    "**Typical Process:**\n",
    "- Start with found learning rate\n",
    "- Train until plateau\n",
    "- Apply learning rate decay (10x reduction)\n",
    "- Continue training\n",
    "\n",
    "### Train/Validation/Test Splits\n",
    "\n",
    "**Problem**: Training loss alone insufficient for model evaluation\n",
    "- Models can memorize training data (overfitting)\n",
    "- Need generalization assessment\n",
    "\n",
    "**Standard Split:**\n",
    "- **Training (80%)**: Parameter optimization via gradient descent  \n",
    "- **Validation/Dev (10%)**: Hyperparameter tuning\n",
    "- **Test (10%)**: Final performance evaluation (use sparingly)\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "n1 = int(0.8 * len(words))  # 80% train\n",
    "n2 = int(0.9 * len(words))  # 90% train+dev\n",
    "X_train, Y_train = build_dataset(words[:n1])\n",
    "X_dev, Y_dev = build_dataset(words[n1:n2])  \n",
    "X_test, Y_test = build_dataset(words[n2:])\n",
    "```\n",
    "\n",
    "### Model Scaling and Optimization\n",
    "\n",
    "**Underfitting Diagnosis:**\n",
    "- Training loss â‰ˆ Validation loss indicates underfitting\n",
    "- Solution: Increase model capacity\n",
    "\n",
    "**Scaling Experiments:**\n",
    "1. **Hidden layer size**: 100 â†’ 300 neurons\n",
    "2. **Embedding dimension**: 2 â†’ 10 dimensions\n",
    "3. **Context length**: 3 â†’ larger block_size\n",
    "\n",
    "**Embedding Visualization (2D case):**\n",
    "- Plot character embeddings after training\n",
    "- Reveals learned structure: vowels cluster together\n",
    "- Special characters (q, .) positioned as outliers\n",
    "- Demonstrates meaningful learned representations\n",
    "\n",
    "**Final Architecture:**\n",
    "- 10-dimensional character embeddings\n",
    "- 200 hidden neurons  \n",
    "- Input: 30 dimensions (3 characters Ã— 10D embeddings)\n",
    "- ~11,000 total parameters\n",
    "\n",
    "**Training Schedule:**\n",
    "- 100k steps at learning_rate=0.1\n",
    "- 100k steps at learning_rate=0.01 (decay)\n",
    "- Achieved ~2.17 validation loss (surpassing 2.45 bigram baseline)\n",
    "\n",
    "### Sampling from Trained Model\n",
    "\n",
    "**Generation Process:**\n",
    "```python\n",
    "context = [0, 0, 0]  # Start with dots\n",
    "for _ in range(20):\n",
    "    emb = C[context]\n",
    "    h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    ix = torch.multinomial(probs, 1).item()\n",
    "    context = context[1:] + [ix]  # Shift context window\n",
    "    if ix == 0: break  # Stop at end token\n",
    "```\n",
    "\n",
    "**Results:**\n",
    "- Generated names significantly more name-like than bigram model\n",
    "- Examples: \"ham\", \"joes\" - showing improved quality\n",
    "- Still room for improvement with further optimization\n",
    "\n",
    "### Optimization Challenges and Improvements\n",
    "\n",
    "**Available Tuning Parameters:**\n",
    "1. Hidden layer neuron count\n",
    "2. Embedding dimensionality  \n",
    "3. Context length (block_size)\n",
    "4. Learning rate schedule\n",
    "5. Batch size\n",
    "6. Training duration\n",
    "7. Regularization techniques\n",
    "\n",
    "**Best Practices:**\n",
    "- Systematic hyperparameter search rather than random tuning\n",
    "- Monitor both training and validation performance\n",
    "- Use learning rate scheduling\n",
    "- Implement proper gradient tracking and visualization\n",
    "\n",
    "**Final Performance:**\n",
    "- Validation loss: 2.17 \n",
    "- Significant improvement over bigram baseline (2.45)\n",
    "- Demonstrates effectiveness of neural approach with learned embeddings\n",
    "\n",
    "This implementation successfully demonstrates the transition from simple statistical models to neural networks, showing how embeddings enable better generalization and the importance of proper training methodology including data splitting and hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2be27bec-1a0b-475e-98c1-a5458be7383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plan of action\n",
    "# words ,  split into block-sized-context and outputs \n",
    "# split into train , dev , testing data\n",
    "# emdedding vector -> vocab     X embed_dimensions \n",
    "# context_size*embed_dimensions X num_neurons_lay_1\n",
    "# 2nd layer: num_neurons_lay_1  X num_poss_outputs\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de0e5461-deb7-45b8-81d4-bb738be4418d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stoi :  {'.': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "itos :  {0: '.', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# start\n",
    "words = open(\"names.txt\" , \"r\").read().splitlines()\n",
    "\n",
    "chars = set(''.join(words))\n",
    "chars.add('.')\n",
    "\n",
    "# itos , stoi\n",
    "\n",
    "stoi , itos = {} , {}\n",
    "sorted_chars = sorted(chars)\n",
    "for index , char in enumerate(sorted_chars):\n",
    "    stoi[char] =  index\n",
    "    itos[index] =  char\n",
    "\n",
    "print(\"stoi : \" , stoi)\n",
    "print(\"itos : \" , itos)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff798427-0935-420c-b76c-724912a83736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... y\n",
      "..y u\n",
      ".yu h\n",
      "yuh e\n",
      "uhe n\n",
      "hen g\n",
      "eng .\n",
      "... d\n",
      "..d i\n",
      ".di o\n",
      "dio n\n",
      "ion d\n",
      "ond r\n",
      "ndr e\n",
      "dre .\n",
      "... x\n",
      "..x a\n",
      ".xa v\n",
      "xav i\n",
      "avi e\n",
      "vie n\n",
      "ien .\n",
      "... j\n",
      "..j o\n",
      ".jo r\n",
      "jor i\n",
      "ori .\n",
      "... j\n",
      "..j u\n",
      ".ju a\n",
      "jua n\n",
      "uan l\n",
      "anl u\n",
      "nlu i\n",
      "lui s\n",
      "uis .\n",
      "... e\n",
      "..e r\n",
      ".er a\n",
      "era n\n",
      "ran d\n",
      "and i\n",
      "ndi .\n",
      "... p\n",
      "..p h\n",
      ".ph i\n",
      "phi a\n",
      "hia .\n",
      "... s\n",
      "..s a\n",
      ".sa m\n",
      "sam a\n",
      "ama t\n",
      "mat h\n",
      "ath a\n",
      "tha .\n",
      "... p\n",
      "..p h\n",
      ".ph o\n",
      "pho e\n",
      "hoe n\n",
      "oen i\n",
      "eni x\n",
      "nix .\n",
      "... e\n",
      "..e m\n",
      ".em m\n",
      "emm e\n",
      "mme l\n",
      "mel y\n",
      "ely n\n",
      "lyn n\n",
      "ynn .\n",
      "... h\n",
      "..h o\n",
      ".ho l\n",
      "hol l\n",
      "oll a\n",
      "lla n\n",
      "lan .\n",
      "... h\n",
      "..h o\n",
      ".ho l\n",
      "hol l\n",
      "oll i\n",
      "lli s\n",
      "lis .\n",
      "... c\n",
      "..c a\n",
      ".ca l\n",
      "cal l\n",
      "all a\n",
      "lla l\n",
      "lal i\n",
      "ali l\n",
      "lil y\n",
      "ily .\n",
      "... a\n",
      "..a d\n",
      ".ad e\n",
      "ade l\n",
      "del a\n",
      "ela y\n",
      "lay d\n",
      "ayd e\n",
      "yde .\n",
      "... j\n",
      "..j o\n",
      ".jo s\n",
      "jos e\n",
      "ose p\n",
      "sep h\n",
      "eph y\n",
      "phy n\n",
      "hyn e\n",
      "yne .\n",
      "... w\n",
      "..w e\n",
      ".we l\n",
      "wel d\n",
      "eld o\n",
      "ldo n\n",
      "don .\n",
      "... k\n",
      "..k a\n",
      ".ka y\n",
      "kay l\n",
      "ayl e\n",
      "yle .\n",
      "... r\n",
      "..r a\n",
      ".ra g\n",
      "rag n\n",
      "agn a\n",
      "gna r\n",
      "nar .\n",
      "... c\n",
      "..c o\n",
      ".co l\n",
      "col b\n",
      "olb i\n",
      "lbi e\n",
      "bie .\n",
      "... t\n",
      "..t a\n",
      ".ta v\n",
      "tav e\n",
      "ave o\n",
      "veo n\n",
      "eon .\n",
      "... a\n",
      "..a k\n",
      ".ak i\n",
      "aki .\n",
      "... p\n",
      "..p e\n",
      ".pe y\n",
      "pey t\n",
      "eyt e\n",
      "yte n\n",
      "ten .\n",
      "... k\n",
      "..k e\n",
      ".ke v\n",
      "kev a\n",
      "eva r\n",
      "var i\n",
      "ari .\n",
      "... j\n",
      "..j o\n",
      ".jo e\n",
      "joe l\n",
      "oel l\n",
      "ell a\n",
      "lla .\n",
      "... m\n",
      "..m e\n",
      ".me c\n",
      "mec c\n",
      "ecc a\n",
      "cca .\n",
      "... e\n",
      "..e g\n",
      ".eg a\n",
      "ega n\n",
      "gan .\n",
      "... j\n",
      "..j u\n",
      ".ju s\n",
      "jus t\n",
      "ust y\n",
      "sty c\n",
      "tyc e\n",
      "yce .\n",
      "... t\n",
      "..t a\n",
      ".ta l\n",
      "tal i\n",
      "ali y\n",
      "liy a\n",
      "iya h\n",
      "yah .\n",
      "... h\n",
      "..h a\n",
      ".ha y\n",
      "hay l\n",
      "ayl e\n",
      "yle y\n",
      "ley .\n",
      "... a\n",
      "..a l\n",
      ".al l\n",
      "all e\n",
      "lle a\n",
      "lea h\n",
      "eah .\n",
      "... k\n",
      "..k y\n",
      ".ky m\n",
      "kym b\n",
      "ymb e\n",
      "mbe r\n",
      "ber l\n",
      "erl y\n",
      "rly n\n",
      "lyn n\n",
      "ynn .\n",
      "... p\n",
      "..p a\n",
      ".pa r\n",
      "par r\n",
      "arr i\n",
      "rri s\n",
      "ris h\n",
      "ish .\n",
      "... h\n",
      "..h o\n",
      ".ho u\n",
      "hou s\n",
      "ous t\n",
      "ust y\n",
      "sty n\n",
      "tyn .\n",
      "... j\n",
      "..j a\n",
      ".ja m\n",
      "jam a\n",
      "ama y\n",
      "may a\n",
      "aya .\n",
      "... a\n",
      "..a h\n",
      ".ah m\n",
      "ahm o\n",
      "hmo d\n",
      "mod .\n",
      "... n\n",
      "..n i\n",
      ".ni v\n",
      "niv i\n",
      "ivi n\n",
      "vin .\n",
      "... m\n",
      "..m i\n",
      ".mi l\n",
      "mil l\n",
      "ill i\n",
      "lli .\n",
      "... c\n",
      "..c r\n",
      ".cr i\n",
      "cri s\n",
      "ris t\n",
      "ist i\n",
      "sti a\n",
      "tia n\n",
      "ian a\n",
      "ana .\n",
      "... j\n",
      "..j a\n",
      ".ja i\n",
      "jai m\n",
      "aim e\n",
      "ime e\n",
      "mee .\n",
      "... m\n",
      "..m i\n",
      ".mi t\n",
      "mit c\n",
      "itc h\n",
      "tch e\n",
      "che l\n",
      "hel l\n",
      "ell .\n",
      "... n\n",
      "..n a\n",
      ".na i\n",
      "nai r\n",
      "air a\n",
      "ira h\n",
      "rah .\n",
      "... l\n",
      "..l o\n",
      ".lo r\n",
      "lor e\n",
      "ore n\n",
      "ren a\n",
      "ena .\n",
      "... g\n",
      "..g e\n",
      ".ge n\n",
      "gen t\n",
      "ent r\n",
      "ntr i\n",
      "tri e\n",
      "rie .\n",
      "... t\n",
      "..t o\n",
      ".to r\n",
      "tor r\n",
      "orr i\n",
      "rri o\n",
      "rio n\n",
      "ion .\n",
      "... s\n",
      "..s a\n",
      ".sa v\n",
      "sav i\n",
      "avi a\n",
      "via n\n",
      "ian .\n",
      "... b\n",
      "..b e\n",
      ".be n\n",
      "ben j\n",
      "enj a\n",
      "nja m\n",
      "jam i\n",
      "ami n\n",
      "min e\n",
      "ine .\n",
      "... a\n",
      "..a i\n",
      ".ai r\n",
      "air e\n",
      "ire s\n",
      "res s\n",
      "ess .\n",
      "... k\n",
      "..k n\n",
      ".kn u\n",
      "knu t\n",
      "nut e\n",
      "ute .\n",
      "... s\n",
      "..s u\n",
      ".su l\n",
      "sul t\n",
      "ult a\n",
      "lta n\n",
      "tan a\n",
      "ana .\n",
      "... d\n",
      "..d a\n",
      ".da n\n",
      "dan a\n",
      "ana i\n",
      "nai .\n",
      "... a\n",
      "..a z\n",
      ".az z\n",
      "azz a\n",
      "zza n\n",
      "zan .\n",
      "... i\n",
      "..i s\n",
      ".is s\n",
      "iss a\n",
      "ssa b\n",
      "sab e\n",
      "abe l\n",
      "bel l\n",
      "ell e\n",
      "lle .\n",
      "... a\n",
      "..a b\n",
      ".ab r\n",
      "abr a\n",
      "bra h\n",
      "rah i\n",
      "ahi m\n",
      "him .\n",
      "... a\n",
      "..a i\n",
      ".ai s\n",
      "ais l\n",
      "isl y\n",
      "sly n\n",
      "lyn .\n",
      "... a\n",
      "..a e\n",
      ".ae r\n",
      "aer y\n",
      "ery s\n",
      "rys .\n",
      "... m\n",
      "..m a\n",
      ".ma l\n",
      "mal a\n",
      "ala i\n",
      "lai y\n",
      "aiy a\n",
      "iya .\n",
      "... k\n",
      "..k i\n",
      ".ki e\n",
      "kie o\n",
      "ieo n\n",
      "eon .\n",
      "... a\n",
      "..a y\n",
      ".ay a\n",
      "aya n\n",
      "yan s\n",
      "ans h\n",
      "nsh .\n",
      "... b\n",
      "..b e\n",
      ".be r\n",
      "ber k\n",
      "erk l\n",
      "rkl e\n",
      "kle e\n",
      "lee .\n",
      "... e\n",
      "..e m\n",
      ".em m\n",
      "emm a\n",
      "mma k\n",
      "mak a\n",
      "aka t\n",
      "kat e\n",
      "ate .\n",
      "... a\n",
      "..a v\n",
      ".av a\n",
      "ava n\n",
      "van e\n",
      "ane e\n",
      "nee s\n",
      "ees h\n",
      "esh .\n",
      "... a\n",
      "..a m\n",
      ".am e\n",
      "ame n\n",
      "men a\n",
      "ena d\n",
      "nad i\n",
      "adi e\n",
      "die l\n",
      "iel .\n",
      "... r\n",
      "..r e\n",
      ".re n\n",
      "ren a\n",
      "ena r\n",
      "nar d\n",
      "ard o\n",
      "rdo .\n",
      "... b\n",
      "..b r\n",
      ".br e\n",
      "bre k\n",
      "rek k\n",
      "ekk e\n",
      "kke n\n",
      "ken .\n",
      "... k\n",
      "..k a\n",
      ".ka m\n",
      "kam r\n",
      "amr y\n",
      "mry .\n",
      "... w\n",
      "..w y\n",
      ".wy l\n",
      "wyl i\n",
      "yli n\n",
      "lin .\n",
      "... t\n",
      "..t r\n",
      ".tr i\n",
      "tri x\n",
      "rix i\n",
      "ixi e\n",
      "xie .\n",
      "... b\n",
      "..b r\n",
      ".br e\n",
      "bre s\n",
      "res l\n",
      "esl y\n",
      "sly n\n",
      "lyn n\n",
      "ynn .\n",
      "... b\n",
      "..b i\n",
      ".bi a\n",
      "bia n\n",
      "ian k\n",
      "ank a\n",
      "nka .\n",
      "... m\n",
      "..m o\n",
      ".mo r\n",
      "mor d\n",
      "ord c\n",
      "rdc h\n",
      "dch a\n",
      "cha i\n",
      "hai .\n",
      "... h\n",
      "..h a\n",
      ".ha o\n",
      "hao y\n",
      "aoy u\n",
      "oyu .\n",
      "... f\n",
      "..f r\n",
      ".fr a\n",
      "fra n\n",
      "ran k\n",
      "ank i\n",
      "nki e\n",
      "kie .\n",
      "... j\n",
      "..j o\n",
      ".jo s\n",
      "jos s\n",
      "oss e\n",
      "sse l\n",
      "sel i\n",
      "eli n\n",
      "lin .\n",
      "... s\n",
      "..s a\n",
      ".sa i\n",
      "sai l\n",
      "ail a\n",
      "ila .\n",
      "... k\n",
      "..k i\n",
      ".ki o\n",
      "kio n\n",
      "ion n\n",
      "onn a\n",
      "nna .\n",
      "... j\n",
      "..j e\n",
      ".je n\n",
      "jen n\n",
      "enn i\n",
      "nni n\n",
      "nin g\n",
      "ing s\n",
      "ngs .\n",
      "... a\n",
      "..a r\n",
      ".ar l\n",
      "arl i\n",
      "rli n\n",
      "lin g\n",
      "ing t\n",
      "ngt o\n",
      "gto n\n",
      "ton .\n",
      "... j\n",
      "..j u\n",
      ".ju p\n",
      "jup i\n",
      "upi t\n",
      "pit e\n",
      "ite r\n",
      "ter .\n",
      "... a\n",
      "..a l\n",
      ".al e\n",
      "ale i\n",
      "lei s\n",
      "eis h\n",
      "ish a\n",
      "sha .\n",
      "... k\n",
      "..k e\n",
      ".ke m\n",
      "kem o\n",
      "emo r\n",
      "mor a\n",
      "ora h\n",
      "rah .\n",
      "... d\n",
      "..d e\n",
      ".de n\n",
      "den i\n",
      "eni s\n",
      "nis s\n",
      "iss e\n",
      "sse .\n",
      "... z\n",
      "..z a\n",
      ".za d\n",
      "zad a\n",
      "ada .\n",
      "... k\n",
      "..k a\n",
      ".ka y\n",
      "kay d\n",
      "ayd y\n",
      "ydy n\n",
      "dyn c\n",
      "ync e\n",
      "nce .\n",
      "... b\n",
      "..b a\n",
      ".ba r\n",
      "bar i\n",
      "ari .\n",
      "... d\n",
      "..d a\n",
      ".da r\n",
      "dar y\n",
      "ary a\n",
      "rya .\n",
      "... e\n",
      "..e l\n",
      ".el l\n",
      "ell i\n",
      "lli e\n",
      "lie a\n",
      "iea n\n",
      "ean n\n",
      "ann e\n",
      "nne .\n",
      "... g\n",
      "..g r\n",
      ".gr e\n",
      "gre t\n",
      "ret e\n",
      "ete l\n",
      "tel .\n",
      "... m\n",
      "..m a\n",
      ".ma h\n",
      "mah n\n",
      "ahn o\n",
      "hno o\n",
      "noo r\n",
      "oor .\n",
      "... a\n",
      "..a r\n",
      ".ar y\n",
      "ary o\n",
      "ryo .\n",
      "... m\n",
      "..m i\n",
      ".mi l\n",
      "mil e\n",
      "ile s\n",
      "les .\n",
      "... e\n",
      "..e v\n",
      ".ev a\n",
      "eva l\n",
      "val i\n",
      "ali s\n",
      "lis e\n",
      "ise .\n",
      "... i\n",
      "..i g\n",
      ".ig o\n",
      "igo r\n",
      "gor .\n",
      "... t\n",
      "..t h\n",
      ".th o\n",
      "tho r\n",
      "hor e\n",
      "ore n\n",
      "ren .\n",
      "... h\n",
      "..h e\n",
      ".he c\n",
      "hec t\n",
      "ect o\n",
      "cto r\n",
      "tor .\n",
      "... a\n",
      "..a n\n",
      ".an u\n",
      "anu e\n",
      "nue l\n",
      "uel .\n",
      "... e\n",
      "..e m\n",
      ".em m\n",
      "emm e\n",
      "mme r\n",
      "mer i\n",
      "eri e\n",
      "rie .\n",
      "... e\n",
      "..e r\n",
      ".er i\n",
      "eri a\n",
      "ria n\n",
      "ian .\n",
      "... b\n",
      "..b l\n",
      ".bl a\n",
      "bla k\n",
      "lak e\n",
      "ake l\n",
      "kel e\n",
      "ele i\n",
      "lei g\n",
      "eig h\n",
      "igh .\n",
      "... m\n",
      "..m a\n",
      ".ma d\n",
      "mad i\n",
      "adi n\n",
      "din a\n",
      "ina .\n",
      "... f\n",
      "..f a\n",
      ".fa t\n",
      "fat u\n",
      "atu m\n",
      "tum a\n",
      "uma t\n",
      "mat a\n",
      "ata .\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "\n",
    "block_size = 3\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "xs , ys = [] , []\n",
    "for word in words[:100]:\n",
    "    # print(word)\n",
    "    word = '.' + word + '.'\n",
    "    context = ['.']*block_size\n",
    "    for i in range(len(word)-1):\n",
    "        context = context[1:] + [word[i]]\n",
    "        # print(context)\n",
    "        xs.append(\"\".join(context))\n",
    "        ys.append(word[i+1])\n",
    "\n",
    "# for i in range(len(xs)):\n",
    "#     print(xs[i] , ys[i])\n",
    "# xs = torch.tensor(xs)\n",
    "# ys = torch.tensor(ys)\n",
    "for i in range(len(xs)):\n",
    "    print(xs[i] , ys[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a7fe2ab-202c-465f-81a1-2874a13a748f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595 669\n",
      "['...', '..y', '.yu', 'yuh', 'uhe', 'hen', 'eng', '...', '..d', '.di', 'dio', 'ion', 'ond', 'ndr', 'dre', '...', '..x', '.xa', 'xav', 'avi', 'vie', 'ien', '...', '..j', '.jo', 'jor', 'ori', '...', '..j', '.ju', 'jua', 'uan', 'anl', 'nlu', 'lui', 'uis', '...', '..e', '.er', 'era', 'ran', 'and', 'ndi', '...', '..p', '.ph', 'phi', 'hia', '...', '..s', '.sa', 'sam', 'ama', 'mat', 'ath', 'tha', '...', '..p', '.ph', 'pho', 'hoe', 'oen', 'eni', 'nix', '...', '..e', '.em', 'emm', 'mme', 'mel', 'ely', 'lyn', 'ynn', '...', '..h', '.ho', 'hol', 'oll', 'lla', 'lan', '...', '..h', '.ho', 'hol', 'oll', 'lli', 'lis', '...', '..c', '.ca', 'cal', 'all', 'lla', 'lal', 'ali', 'lil', 'ily', '...', '..a', '.ad', 'ade', 'del', 'ela', 'lay', 'ayd', 'yde', '...', '..j', '.jo', 'jos', 'ose', 'sep', 'eph', 'phy', 'hyn', 'yne', '...', '..w', '.we', 'wel', 'eld', 'ldo', 'don', '...', '..k', '.ka', 'kay', 'ayl', 'yle', '...', '..r', '.ra', 'rag', 'agn', 'gna', 'nar', '...', '..c', '.co', 'col', 'olb', 'lbi', 'bie', '...', '..t', '.ta', 'tav', 'ave', 'veo', 'eon', '...', '..a', '.ak', 'aki', '...', '..p', '.pe', 'pey', 'eyt', 'yte', 'ten', '...', '..k', '.ke', 'kev', 'eva', 'var', 'ari', '...', '..j', '.jo', 'joe', 'oel', 'ell', 'lla', '...', '..m', '.me', 'mec', 'ecc', 'cca', '...', '..e', '.eg', 'ega', 'gan', '...', '..j', '.ju', 'jus', 'ust', 'sty', 'tyc', 'yce', '...', '..t', '.ta', 'tal', 'ali', 'liy', 'iya', 'yah', '...', '..h', '.ha', 'hay', 'ayl', 'yle', 'ley', '...', '..a', '.al', 'all', 'lle', 'lea', 'eah', '...', '..k', '.ky', 'kym', 'ymb', 'mbe', 'ber', 'erl', 'rly', 'lyn', 'ynn', '...', '..p', '.pa', 'par', 'arr', 'rri', 'ris', 'ish', '...', '..h', '.ho', 'hou', 'ous', 'ust', 'sty', 'tyn', '...', '..j', '.ja', 'jam', 'ama', 'may', 'aya', '...', '..a', '.ah', 'ahm', 'hmo', 'mod', '...', '..n', '.ni', 'niv', 'ivi', 'vin', '...', '..m', '.mi', 'mil', 'ill', 'lli', '...', '..c', '.cr', 'cri', 'ris', 'ist', 'sti', 'tia', 'ian', 'ana', '...', '..j', '.ja', 'jai', 'aim', 'ime', 'mee', '...', '..m', '.mi', 'mit', 'itc', 'tch', 'che', 'hel', 'ell', '...', '..n', '.na', 'nai', 'air', 'ira', 'rah', '...', '..l', '.lo', 'lor', 'ore', 'ren', 'ena', '...', '..g', '.ge', 'gen', 'ent', 'ntr', 'tri', 'rie', '...', '..t', '.to', 'tor', 'orr', 'rri', 'rio', 'ion', '...', '..s', '.sa', 'sav', 'avi', 'via', 'ian', '...', '..b', '.be', 'ben', 'enj', 'nja', 'jam', 'ami', 'min', 'ine', '...', '..a', '.ai', 'air', 'ire', 'res', 'ess', '...', '..k', '.kn', 'knu', 'nut', 'ute', '...', '..s', '.su', 'sul', 'ult', 'lta', 'tan', 'ana', '...', '..d', '.da', 'dan', 'ana', 'nai', '...', '..a', '.az', 'azz', 'zza', 'zan', '...', '..i', '.is', 'iss', 'ssa', 'sab', 'abe', 'bel', 'ell', 'lle', '...', '..a', '.ab', 'abr', 'bra', 'rah', 'ahi', 'him', '...', '..a', '.ai', 'ais', 'isl', 'sly', 'lyn', '...', '..a', '.ae', 'aer', 'ery', 'rys', '...', '..m', '.ma', 'mal', 'ala', 'lai', 'aiy', 'iya', '...', '..k', '.ki', 'kie', 'ieo', 'eon', '...', '..a', '.ay', 'aya', 'yan', 'ans', 'nsh', '...', '..b', '.be', 'ber', 'erk', 'rkl', 'kle', 'lee', '...', '..e', '.em', 'emm', 'mma', 'mak', 'aka', 'kat', 'ate', '...', '..a', '.av', 'ava', 'van', 'ane', 'nee', 'ees', 'esh', '...', '..a', '.am', 'ame', 'men', 'ena', 'nad', 'adi', 'die', 'iel', '...', '..r', '.re', 'ren', 'ena', 'nar', 'ard', 'rdo', '...', '..b', '.br', 'bre', 'rek', 'ekk', 'kke', 'ken', '...', '..k', '.ka', 'kam', 'amr', 'mry', '...', '..w', '.wy', 'wyl', 'yli', 'lin', '...', '..t', '.tr', 'tri', 'rix', 'ixi', 'xie', '...', '..b', '.br', 'bre', 'res', 'esl', 'sly', 'lyn', 'ynn', '...', '..b', '.bi', 'bia', 'ian', 'ank', 'nka', '...', '..m', '.mo', 'mor', 'ord', 'rdc', 'dch', 'cha', 'hai', '...', '..h', '.ha', 'hao', 'aoy', 'oyu', '...', '..f', '.fr', 'fra', 'ran', 'ank', 'nki', 'kie', '...', '..j', '.jo', 'jos', 'oss', 'sse', 'sel', 'eli', 'lin', '...', '..s', '.sa', 'sai', 'ail', 'ila', '...', '..k', '.ki', 'kio', 'ion', 'onn', 'nna', '...', '..j', '.je', 'jen', 'enn', 'nni', 'nin', 'ing', 'ngs', '...', '..a', '.ar', 'arl', 'rli', 'lin', 'ing', 'ngt', 'gto', 'ton', '...', '..j', '.ju', 'jup', 'upi', 'pit', 'ite', 'ter', '...', '..a', '.al', 'ale', 'lei', 'eis', 'ish', 'sha', '...', '..k'] ['y', 'u', 'h', 'e', 'n', 'g', '.', 'd', 'i', 'o', 'n', 'd', 'r', 'e', '.', 'x', 'a', 'v', 'i', 'e', 'n', '.', 'j', 'o', 'r', 'i', '.', 'j', 'u', 'a', 'n', 'l', 'u', 'i', 's', '.', 'e', 'r', 'a', 'n', 'd', 'i', '.', 'p', 'h', 'i', 'a', '.', 's', 'a', 'm', 'a', 't', 'h', 'a', '.', 'p', 'h', 'o', 'e', 'n', 'i', 'x', '.', 'e', 'm', 'm', 'e', 'l', 'y', 'n', 'n', '.', 'h', 'o', 'l', 'l', 'a', 'n', '.', 'h', 'o', 'l', 'l', 'i', 's', '.', 'c', 'a', 'l', 'l', 'a', 'l', 'i', 'l', 'y', '.', 'a', 'd', 'e', 'l', 'a', 'y', 'd', 'e', '.', 'j', 'o', 's', 'e', 'p', 'h', 'y', 'n', 'e', '.', 'w', 'e', 'l', 'd', 'o', 'n', '.', 'k', 'a', 'y', 'l', 'e', '.', 'r', 'a', 'g', 'n', 'a', 'r', '.', 'c', 'o', 'l', 'b', 'i', 'e', '.', 't', 'a', 'v', 'e', 'o', 'n', '.', 'a', 'k', 'i', '.', 'p', 'e', 'y', 't', 'e', 'n', '.', 'k', 'e', 'v', 'a', 'r', 'i', '.', 'j', 'o', 'e', 'l', 'l', 'a', '.', 'm', 'e', 'c', 'c', 'a', '.', 'e', 'g', 'a', 'n', '.', 'j', 'u', 's', 't', 'y', 'c', 'e', '.', 't', 'a', 'l', 'i', 'y', 'a', 'h', '.', 'h', 'a', 'y', 'l', 'e', 'y', '.', 'a', 'l', 'l', 'e', 'a', 'h', '.', 'k', 'y', 'm', 'b', 'e', 'r', 'l', 'y', 'n', 'n', '.', 'p', 'a', 'r', 'r', 'i', 's', 'h', '.', 'h', 'o', 'u', 's', 't', 'y', 'n', '.', 'j', 'a', 'm', 'a', 'y', 'a', '.', 'a', 'h', 'm', 'o', 'd', '.', 'n', 'i', 'v', 'i', 'n', '.', 'm', 'i', 'l', 'l', 'i', '.', 'c', 'r', 'i', 's', 't', 'i', 'a', 'n', 'a', '.', 'j', 'a', 'i', 'm', 'e', 'e', '.', 'm', 'i', 't', 'c', 'h', 'e', 'l', 'l', '.', 'n', 'a', 'i', 'r', 'a', 'h', '.', 'l', 'o', 'r', 'e', 'n', 'a', '.', 'g', 'e', 'n', 't', 'r', 'i', 'e', '.', 't', 'o', 'r', 'r', 'i', 'o', 'n', '.', 's', 'a', 'v', 'i', 'a', 'n', '.', 'b', 'e', 'n', 'j', 'a', 'm', 'i', 'n', 'e', '.', 'a', 'i', 'r', 'e', 's', 's', '.', 'k', 'n', 'u', 't', 'e', '.', 's', 'u', 'l', 't', 'a', 'n', 'a', '.', 'd', 'a', 'n', 'a', 'i', '.', 'a', 'z', 'z', 'a', 'n', '.', 'i', 's', 's', 'a', 'b', 'e', 'l', 'l', 'e', '.', 'a', 'b', 'r', 'a', 'h', 'i', 'm', '.', 'a', 'i', 's', 'l', 'y', 'n', '.', 'a', 'e', 'r', 'y', 's', '.', 'm', 'a', 'l', 'a', 'i', 'y', 'a', '.', 'k', 'i', 'e', 'o', 'n', '.', 'a', 'y', 'a', 'n', 's', 'h', '.', 'b', 'e', 'r', 'k', 'l', 'e', 'e', '.', 'e', 'm', 'm', 'a', 'k', 'a', 't', 'e', '.', 'a', 'v', 'a', 'n', 'e', 'e', 's', 'h', '.', 'a', 'm', 'e', 'n', 'a', 'd', 'i', 'e', 'l', '.', 'r', 'e', 'n', 'a', 'r', 'd', 'o', '.', 'b', 'r', 'e', 'k', 'k', 'e', 'n', '.', 'k', 'a', 'm', 'r', 'y', '.', 'w', 'y', 'l', 'i', 'n', '.', 't', 'r', 'i', 'x', 'i', 'e', '.', 'b', 'r', 'e', 's', 'l', 'y', 'n', 'n', '.', 'b', 'i', 'a', 'n', 'k', 'a', '.', 'm', 'o', 'r', 'd', 'c', 'h', 'a', 'i', '.', 'h', 'a', 'o', 'y', 'u', '.', 'f', 'r', 'a', 'n', 'k', 'i', 'e', '.', 'j', 'o', 's', 's', 'e', 'l', 'i', 'n', '.', 's', 'a', 'i', 'l', 'a', '.', 'k', 'i', 'o', 'n', 'n', 'a', '.', 'j', 'e', 'n', 'n', 'i', 'n', 'g', 's', '.', 'a', 'r', 'l', 'i', 'n', 'g', 't', 'o', 'n', '.', 'j', 'u', 'p', 'i', 't', 'e', 'r', '.', 'a', 'l', 'e', 'i', 's', 'h', 'a', '.', 'k', 'e'] ['kem', 'emo', 'mor', 'ora', 'rah', '...', '..d', '.de', 'den', 'eni', 'nis', 'iss', 'sse', '...', '..z', '.za', 'zad', 'ada', '...', '..k', '.ka', 'kay', 'ayd', 'ydy', 'dyn', 'ync', 'nce', '...', '..b', '.ba', 'bar', 'ari', '...', '..d', '.da', 'dar', 'ary', 'rya', '...', '..e', '.el', 'ell', 'lli', 'lie', 'iea', 'ean', 'ann', 'nne', '...', '..g', '.gr', 'gre', 'ret', 'ete', 'tel', '...', '..m', '.ma', 'mah', 'ahn', 'hno', 'noo', 'oor', '...', '..a', '.ar', 'ary', 'ryo', '...', '..m', '.mi', 'mil', 'ile'] ['o', 'r', 'a', 'h', '.', 'd', 'e', 'n', 'i', 's', 's', 'e', '.', 'z', 'a', 'd', 'a', '.', 'k', 'a', 'y', 'd', 'y', 'n', 'c', 'e', '.', 'b', 'a', 'r', 'i', '.', 'd', 'a', 'r', 'y', 'a', '.', 'e', 'l', 'l', 'i', 'e', 'a', 'n', 'n', 'e', '.', 'g', 'r', 'e', 't', 'e', 'l', '.', 'm', 'a', 'h', 'n', 'o', 'o', 'r', '.', 'a', 'r', 'y', 'o', '.', 'm', 'i', 'l', 'e', 's'] ['...', '..e', '.ev', 'eva', 'val', 'ali', 'lis', 'ise', '...', '..i', '.ig', 'igo', 'gor', '...', '..t', '.th', 'tho', 'hor', 'ore', 'ren', '...', '..h', '.he', 'hec', 'ect', 'cto', 'tor', '...', '..a', '.an', 'anu', 'nue', 'uel', '...', '..e', '.em', 'emm', 'mme', 'mer', 'eri', 'rie', '...', '..e', '.er', 'eri', 'ria', 'ian', '...', '..b', '.bl', 'bla', 'lak', 'ake', 'kel', 'ele', 'lei', 'eig', 'igh', '...', '..m', '.ma', 'mad', 'adi', 'din', 'ina', '...', '..f', '.fa', 'fat', 'atu', 'tum', 'uma', 'mat', 'ata'] ['e', 'v', 'a', 'l', 'i', 's', 'e', '.', 'i', 'g', 'o', 'r', '.', 't', 'h', 'o', 'r', 'e', 'n', '.', 'h', 'e', 'c', 't', 'o', 'r', '.', 'a', 'n', 'u', 'e', 'l', '.', 'e', 'm', 'm', 'e', 'r', 'i', 'e', '.', 'e', 'r', 'i', 'a', 'n', '.', 'b', 'l', 'a', 'k', 'e', 'l', 'e', 'i', 'g', 'h', '.', 'm', 'a', 'd', 'i', 'n', 'a', '.', 'f', 'a', 't', 'u', 'm', 'a', 't', 'a', '.']\n"
     ]
    }
   ],
   "source": [
    "# spitting the data\n",
    "\n",
    "n1 = int(0.8*len(xs))\n",
    "n2 = int(0.9*len(xs))\n",
    "\n",
    "print(n1,n2)\n",
    "\n",
    "Xtr = xs[:n1]\n",
    "Ytr = ys[:n1]\n",
    "\n",
    "Xdev = xs[n1+1:n2]\n",
    "Ydev = ys[n1+1:n2]\n",
    "\n",
    "Xtst = xs[n2+1:]\n",
    "Ytst = ys[n2+1:]\n",
    "\n",
    "print(Xtr,Ytr,Xdev,Ydev,Xtst,Ytst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a33403a0-6491-4b2b-be43-d3ddc20da9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini batch size :  0\n",
      "tensor([[0.6856, 0.0114],\n",
      "        [0.7402, 0.0363],\n",
      "        [0.8819, 0.0425],\n",
      "        [0.9433, 0.8630],\n",
      "        [0.7422, 0.1298],\n",
      "        [0.6088, 0.3692],\n",
      "        [0.5116, 0.2341],\n",
      "        [0.2454, 0.5123],\n",
      "        [0.9040, 0.3495],\n",
      "        [0.8431, 0.2337],\n",
      "        [0.1011, 0.3419],\n",
      "        [0.2789, 0.5695],\n",
      "        [0.2641, 0.8558],\n",
      "        [0.1298, 0.2401],\n",
      "        [0.9122, 0.7090],\n",
      "        [0.1806, 0.1759],\n",
      "        [0.2430, 0.8846],\n",
      "        [0.8845, 0.3351],\n",
      "        [0.5884, 0.8332],\n",
      "        [0.9479, 0.7864],\n",
      "        [0.5057, 0.1851],\n",
      "        [0.8593, 0.9850],\n",
      "        [0.0693, 0.7938],\n",
      "        [0.5298, 0.2598],\n",
      "        [0.5644, 0.6012],\n",
      "        [0.8435, 0.8888],\n",
      "        [0.6606, 0.3906]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# embedding vector\n",
    "# hyper paramameters\n",
    "\n",
    "block_size = 3\n",
    "emb_size = 2\n",
    "vocab_size = len(chars)\n",
    "hidden_layer_neurons = 100\n",
    "batch_size = int(0.001 * len(Xtr))\n",
    "print(\"Mini batch size : \",batch_size)\n",
    "\n",
    "hyper_params = [block_size , emb_size , vocab_size , hidden_layer_neurons ]\n",
    "\n",
    "# parameters\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "C = torch.rand((vocab_size , emb_size) , dtype = torch.float32 , requires_grad = True) # 27 x 2\n",
    "print(C)\n",
    "\n",
    "W1 = torch.rand((block_size * emb_size , hidden_layer_neurons  ) , dtype = torch.float32 , requires_grad = True) # 2*3 = 6 x 100\n",
    "b1 = torch.rand(hidden_layer_neurons , dtype = torch.float32 , requires_grad = True) # 100\n",
    "\n",
    "W2 = torch.rand(( hidden_layer_neurons , vocab_size ) , dtype = torch.float32 , requires_grad = True) # 100 x 27\n",
    "b2 = torch.rand(vocab_size , dtype = torch.float32 , requires_grad = True) # 27\n",
    "\n",
    "params = [C , W1, b1 , W2 , b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd72d46b-6c3b-4c4c-95fd-8b1120cbf1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0.6856, 0.0114],\n",
      "        [0.7402, 0.0363],\n",
      "        [0.8819, 0.0425],\n",
      "        [0.9433, 0.8630],\n",
      "        [0.7422, 0.1298],\n",
      "        [0.6088, 0.3692],\n",
      "        [0.5116, 0.2341],\n",
      "        [0.2454, 0.5123],\n",
      "        [0.9040, 0.3495],\n",
      "        [0.8431, 0.2337],\n",
      "        [0.1011, 0.3419],\n",
      "        [0.2789, 0.5695],\n",
      "        [0.2641, 0.8558],\n",
      "        [0.1298, 0.2401],\n",
      "        [0.9122, 0.7090],\n",
      "        [0.1806, 0.1759],\n",
      "        [0.2430, 0.8846],\n",
      "        [0.8845, 0.3351],\n",
      "        [0.5884, 0.8332],\n",
      "        [0.9479, 0.7864],\n",
      "        [0.5057, 0.1851],\n",
      "        [0.8593, 0.9850],\n",
      "        [0.0693, 0.7938],\n",
      "        [0.5298, 0.2598],\n",
      "        [0.5644, 0.6012],\n",
      "        [0.8435, 0.8888],\n",
      "        [0.6606, 0.3906]], requires_grad=True), tensor([[0.2877, 0.0909, 0.3207, 0.6723, 0.1006, 0.2635, 0.6446, 0.7315, 0.9054,\n",
      "         0.4232, 0.4236, 0.2706, 0.1901, 0.5009, 0.9115, 0.1473, 0.0486, 0.9921,\n",
      "         0.7246, 0.0433, 0.4788, 0.2761, 0.4413, 0.4827, 0.2420, 0.1625, 0.8155,\n",
      "         0.2294, 0.0134, 0.6089, 0.4624, 0.1544, 0.7393, 0.3041, 0.4301, 0.0092,\n",
      "         0.4899, 0.0632, 0.1182, 0.5728, 0.0244, 0.6867, 0.2357, 0.9508, 0.4505,\n",
      "         0.3973, 0.2923, 0.3709, 0.9546, 0.7111, 0.6588, 0.1586, 0.1926, 0.1461,\n",
      "         0.4513, 0.7753, 0.2502, 0.8003, 0.7198, 0.6046, 0.9174, 0.1118, 0.2306,\n",
      "         0.7891, 0.1647, 0.8011, 0.4619, 0.8979, 0.6510, 0.4227, 0.4644, 0.8406,\n",
      "         0.3154, 0.1981, 0.7617, 0.0301, 0.8711, 0.3918, 0.7052, 0.2632, 0.5093,\n",
      "         0.8352, 0.6375, 0.7283, 0.4070, 0.0988, 0.4243, 0.1476, 0.3079, 0.5624,\n",
      "         0.9895, 0.7969, 0.9790, 0.8185, 0.8720, 0.7897, 0.5926, 0.0333, 0.8213,\n",
      "         0.1189],\n",
      "        [0.9663, 0.6668, 0.9425, 0.0233, 0.0846, 0.6882, 0.8124, 0.7145, 0.0313,\n",
      "         0.2848, 0.2979, 0.0088, 0.6174, 0.6156, 0.4327, 0.5069, 0.6357, 0.3131,\n",
      "         0.8039, 0.4305, 0.5352, 0.0168, 0.8135, 0.6144, 0.5525, 0.6117, 0.2105,\n",
      "         0.6694, 0.7206, 0.6029, 0.1413, 0.8436, 0.6709, 0.6268, 0.0612, 0.0735,\n",
      "         0.9048, 0.9608, 0.2298, 0.1711, 0.8969, 0.9267, 0.6913, 0.0648, 0.7122,\n",
      "         0.0110, 0.9323, 0.2300, 0.1825, 0.1209, 0.8222, 0.0277, 0.7170, 0.2163,\n",
      "         0.8562, 0.2567, 0.1394, 0.3297, 0.5178, 0.2930, 0.5769, 0.3898, 0.5609,\n",
      "         0.4237, 0.0784, 0.9149, 0.4850, 0.6875, 0.2219, 0.0718, 0.7254, 0.7419,\n",
      "         0.1078, 0.3881, 0.6493, 0.9315, 0.6353, 0.5236, 0.5606, 0.1969, 0.7012,\n",
      "         0.0953, 0.1309, 0.9471, 0.2222, 0.5477, 0.6676, 0.1306, 0.4304, 0.2396,\n",
      "         0.0923, 0.4845, 0.7781, 0.8017, 0.5402, 0.8008, 0.3015, 0.1700, 0.8096,\n",
      "         0.0632],\n",
      "        [0.4738, 0.2995, 0.6906, 0.3549, 0.4592, 0.6536, 0.1172, 0.2912, 0.7008,\n",
      "         0.8300, 0.2429, 0.9452, 0.3733, 0.7717, 0.4749, 0.5524, 0.1606, 0.7294,\n",
      "         0.4149, 0.2421, 0.1939, 0.1068, 0.4156, 0.8645, 0.6367, 0.5323, 0.2174,\n",
      "         0.0673, 0.0240, 0.1315, 0.7259, 0.2380, 0.9802, 0.8452, 0.0101, 0.2215,\n",
      "         0.0719, 0.2726, 0.8394, 0.7627, 0.1843, 0.3526, 0.2601, 0.2775, 0.7245,\n",
      "         0.1091, 0.0200, 0.5908, 0.8829, 0.2333, 0.6884, 0.2194, 0.1832, 0.8833,\n",
      "         0.7062, 0.4558, 0.3998, 0.3791, 0.7910, 0.8117, 0.5266, 0.5838, 0.8654,\n",
      "         0.1176, 0.0755, 0.4187, 0.5303, 0.6853, 0.6111, 0.2024, 0.0341, 0.8473,\n",
      "         0.2681, 0.3995, 0.2302, 0.8416, 0.3924, 0.9724, 0.5122, 0.6875, 0.6111,\n",
      "         0.8939, 0.4692, 0.3373, 0.2740, 0.4027, 0.4093, 0.6898, 0.2739, 0.4332,\n",
      "         0.6795, 0.0433, 0.5465, 0.1348, 0.0417, 0.8313, 0.6204, 0.6787, 0.8386,\n",
      "         0.6303],\n",
      "        [0.9277, 0.6068, 0.6769, 0.7980, 0.0334, 0.3778, 0.9632, 0.1130, 0.7708,\n",
      "         0.3936, 0.2998, 0.8866, 0.6711, 0.9763, 0.6370, 0.0948, 0.1503, 0.9841,\n",
      "         0.2889, 0.4517, 0.6385, 0.6694, 0.8185, 0.0054, 0.8071, 0.0415, 0.9411,\n",
      "         0.5645, 0.6363, 0.6915, 0.5468, 0.3004, 0.6361, 0.9275, 0.3289, 0.9255,\n",
      "         0.9923, 0.9660, 0.8850, 0.7681, 0.4086, 0.6206, 0.1772, 0.7204, 0.0371,\n",
      "         0.4537, 0.2986, 0.7513, 0.4835, 0.6209, 0.5752, 0.1548, 0.1544, 0.2344,\n",
      "         0.9826, 0.3095, 0.9721, 0.2873, 0.6209, 0.4545, 0.3435, 0.0872, 0.1076,\n",
      "         0.3344, 0.0781, 0.6332, 0.3776, 0.1515, 0.9095, 0.0051, 0.4798, 0.2130,\n",
      "         0.2649, 0.9243, 0.5964, 0.2028, 0.3572, 0.8700, 0.6272, 0.2628, 0.9977,\n",
      "         0.3596, 0.5636, 0.2251, 0.4749, 0.1870, 0.0041, 0.1692, 0.3931, 0.5607,\n",
      "         0.9455, 0.9302, 0.8122, 0.6301, 0.6430, 0.1161, 0.8733, 0.2478, 0.2769,\n",
      "         0.5891],\n",
      "        [0.9792, 0.3094, 0.5221, 0.9645, 0.8221, 0.4800, 0.6864, 0.4575, 0.5632,\n",
      "         0.0483, 0.4772, 0.7134, 0.9794, 0.8237, 0.6644, 0.2114, 0.0684, 0.3757,\n",
      "         0.2353, 0.9181, 0.0036, 0.9439, 0.4285, 0.1904, 0.8406, 0.8444, 0.2428,\n",
      "         0.2293, 0.0307, 0.5673, 0.4266, 0.9337, 0.2402, 0.3733, 0.6607, 0.7019,\n",
      "         0.3707, 0.5555, 0.5534, 0.8886, 0.4389, 0.9045, 0.5414, 0.1656, 0.8944,\n",
      "         0.9069, 0.0168, 0.7322, 0.8898, 0.0123, 0.4361, 0.3066, 0.9205, 0.8696,\n",
      "         0.6386, 0.7759, 0.5153, 0.5542, 0.9141, 0.1165, 0.3413, 0.5470, 0.6771,\n",
      "         0.0291, 0.9389, 0.9294, 0.9393, 0.0323, 0.0208, 0.4242, 0.1633, 0.9256,\n",
      "         0.0737, 0.1890, 0.7078, 0.9932, 0.9991, 0.5505, 0.6964, 0.3608, 0.1188,\n",
      "         0.4671, 0.8990, 0.9942, 0.8655, 0.2082, 0.4937, 0.7439, 0.6319, 0.7072,\n",
      "         0.1767, 0.2823, 0.7468, 0.7354, 0.1272, 0.6253, 0.6004, 0.3907, 0.6874,\n",
      "         0.2375],\n",
      "        [0.0254, 0.0502, 0.3235, 0.2920, 0.4973, 0.9106, 0.4221, 0.5352, 0.4512,\n",
      "         0.4540, 0.2676, 0.4928, 0.2135, 0.2264, 0.2758, 0.7810, 0.0955, 0.0421,\n",
      "         0.6017, 0.4790, 0.9745, 0.9330, 0.2636, 0.3706, 0.7403, 0.7106, 0.0485,\n",
      "         0.2144, 0.5377, 0.3078, 0.7119, 0.0113, 0.0974, 0.5955, 0.8372, 0.0240,\n",
      "         0.8959, 0.1658, 0.5620, 0.1562, 0.2844, 0.7986, 0.1491, 0.3823, 0.1899,\n",
      "         0.8679, 0.6302, 0.0195, 0.1582, 0.5386, 0.1558, 0.5953, 0.1888, 0.4444,\n",
      "         0.4652, 0.5730, 0.8947, 0.6896, 0.2852, 0.8688, 0.2892, 0.5036, 0.9380,\n",
      "         0.7006, 0.7002, 0.2151, 0.4026, 0.3094, 0.7581, 0.2525, 0.8085, 0.6912,\n",
      "         0.0021, 0.7127, 0.7492, 0.5944, 0.4499, 0.7758, 0.8918, 0.6299, 0.9800,\n",
      "         0.2122, 0.7831, 0.8033, 0.8790, 0.6966, 0.4061, 0.8778, 0.3454, 0.5062,\n",
      "         0.4844, 0.1374, 0.1499, 0.1110, 0.5201, 0.1081, 0.4233, 0.3401, 0.6164,\n",
      "         0.2755]], requires_grad=True), tensor([0.8354, 0.8008, 0.7098, 0.9432, 0.7994, 0.0166, 0.5639, 0.9511, 0.3188,\n",
      "        0.4541, 0.3632, 0.0500, 0.2736, 0.8168, 0.8453, 0.4834, 0.2148, 0.5594,\n",
      "        0.0632, 0.2007, 0.5981, 0.6485, 0.7153, 0.0222, 0.0122, 0.8942, 0.9139,\n",
      "        0.3269, 0.6895, 0.7709, 0.9501, 0.4257, 0.6747, 0.0356, 0.8267, 0.3559,\n",
      "        0.6604, 0.2093, 0.0861, 0.3840, 0.0961, 0.3019, 0.1590, 0.2547, 0.3259,\n",
      "        0.7548, 0.7772, 0.5278, 0.3852, 0.7242, 0.9727, 0.1972, 0.7748, 0.7763,\n",
      "        0.8191, 0.1230, 0.6842, 0.5942, 0.2662, 0.0013, 0.1815, 0.4118, 0.3810,\n",
      "        0.1463, 0.2564, 0.0579, 0.4316, 0.2301, 0.6411, 0.1667, 0.6002, 0.0926,\n",
      "        0.9612, 0.8748, 0.0064, 0.5553, 0.2744, 0.7015, 0.3193, 0.5438, 0.3918,\n",
      "        0.2537, 0.8746, 0.2498, 0.4906, 0.9381, 0.4678, 0.0882, 0.7535, 0.3298,\n",
      "        0.8660, 0.7998, 0.2859, 0.5496, 0.7344, 0.3734, 0.8751, 0.9706, 0.9128,\n",
      "        0.9435], requires_grad=True), tensor([[0.6649, 0.5951, 0.6692,  ..., 0.8772, 0.1215, 0.8037],\n",
      "        [0.1454, 0.2707, 0.6595,  ..., 0.7988, 0.7186, 0.1304],\n",
      "        [0.1797, 0.2506, 0.1410,  ..., 0.9570, 0.0740, 0.4184],\n",
      "        ...,\n",
      "        [0.8232, 0.0655, 0.9066,  ..., 0.5505, 0.0170, 0.6114],\n",
      "        [0.6245, 0.2079, 0.3289,  ..., 0.5591, 0.4832, 0.0496],\n",
      "        [0.3036, 0.6940, 0.2689,  ..., 0.5022, 0.8108, 0.8273]],\n",
      "       requires_grad=True), tensor([0.3058, 0.9757, 0.6659, 0.6567, 0.6386, 0.7418, 0.3086, 0.2484, 0.7617,\n",
      "        0.0338, 0.8494, 0.9840, 0.9105, 0.2307, 0.0144, 0.2136, 0.1111, 0.0878,\n",
      "        0.3779, 0.5479, 0.8113, 0.0668, 0.9478, 0.4400, 0.5052, 0.8231, 0.2224],\n",
      "       requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2bc826b-bcbe-4632-a27b-f3abf65f15d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# create mini_batch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m ix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(Xtr), (batch_size,))\n\u001b[0;32m----> 3\u001b[0m X \u001b[38;5;241m=\u001b[39m Xtr[ix]\n\u001b[1;32m      4\u001b[0m Y \u001b[38;5;241m=\u001b[39m Ytr[ix]\n\u001b[1;32m      5\u001b[0m emb \u001b[38;5;241m=\u001b[39m C[X]\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "# create mini_batch\n",
    "ix = torch.randint(0, len(Xtr), (batch_size,))\n",
    "X = Xtr[ix]\n",
    "Y = Ytr[ix]\n",
    "emb = C[X]\n",
    "\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119aec8a-8139-4521-a366-55f8e7b174c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([182, 3, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae3ebd2-9055-4110-b50c-e8ba3ad60ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = torch.tanh(emb.view( batch_size , block_size * emb_size) @ W1 + b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30920550-1ba1-4dd0-960b-9aec1066b7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([182, 100])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eccbbd-df8a-4ebc-ac88-c659472c300d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[44.8095, 48.2617, 39.9420,  ..., 41.1934, 49.2096, 45.2271],\n",
       "        [46.7581, 50.2267, 41.7099,  ..., 43.2900, 51.2857, 47.1326],\n",
       "        [44.5744, 47.8927, 39.6194,  ..., 41.0400, 48.7794, 44.8521],\n",
       "        ...,\n",
       "        [43.8106, 47.1896, 39.0796,  ..., 40.4553, 48.2304, 44.3814],\n",
       "        [46.6806, 50.1619, 41.6366,  ..., 43.2414, 51.2043, 47.0809],\n",
       "        [44.9423, 48.3940, 40.0832,  ..., 41.4891, 49.3296, 45.4873]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = (hidden @ W2 + b2)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42d75f4-612f-4406-bbc3-58b40f7803fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([182, 27])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f8967c-6401-46f8-a33f-310c8505cad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.8876e+19, 9.1160e+20, 2.2212e+17,  ..., 7.7636e+17, 2.3520e+21,\n",
       "         4.3839e+19],\n",
       "        [2.0267e+20, 6.5042e+21, 1.3014e+18,  ..., 6.3183e+18, 1.8754e+22,\n",
       "         2.9473e+20],\n",
       "        [2.2825e+19, 6.3029e+20, 1.6087e+17,  ..., 6.6598e+17, 1.5297e+21,\n",
       "         3.0130e+19],\n",
       "        ...,\n",
       "        [1.0634e+19, 3.1201e+20, 9.3766e+16,  ..., 3.7113e+17, 8.8346e+20,\n",
       "         1.8820e+19],\n",
       "        [1.8756e+20, 6.0957e+21, 1.2094e+18,  ..., 6.0186e+18, 1.7288e+22,\n",
       "         2.7988e+20],\n",
       "        [3.2977e+19, 1.0405e+21, 2.5581e+17,  ..., 1.0435e+18, 2.6519e+21,\n",
       "         5.6870e+19]], grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = logits.exp()\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe5142e-c5e5-4c0e-90b6-ead5d80a5c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0641e-04, 1.2830e-02, 3.1262e-06,  ..., 1.0927e-05, 3.3103e-02,\n",
      "         6.1700e-04],\n",
      "        [3.6510e-04, 1.1717e-02, 2.3444e-06,  ..., 1.1382e-05, 3.3785e-02,\n",
      "         5.3095e-04],\n",
      "        [5.5504e-04, 1.5327e-02, 3.9120e-06,  ..., 1.6195e-05, 3.7198e-02,\n",
      "         7.3270e-04],\n",
      "        ...,\n",
      "        [4.0708e-04, 1.1944e-02, 3.5894e-06,  ..., 1.4207e-05, 3.3820e-02,\n",
      "         7.2043e-04],\n",
      "        [3.6734e-04, 1.1939e-02, 2.3686e-06,  ..., 1.1788e-05, 3.3859e-02,\n",
      "         5.4816e-04],\n",
      "        [3.9202e-04, 1.2369e-02, 3.0410e-06,  ..., 1.2404e-05, 3.1525e-02,\n",
      "         6.7605e-04]], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(5.7239, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = counts / counts.sum(1 , keepdim=True)\n",
    "print(probs)\n",
    "\n",
    "loss = - probs[torch.arange(batch_size) , Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b1a4eb-95ff-4e33-b107-d393163f7dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this manual way of calculating is annoying , makes for worse backprop and creates issues like storing inf and nan\n",
    "# instead , we use the inbuild softmax function that allows for better backprop using analytically derived derivatives , and \n",
    "# it first subtracts the largest value from each row before raises exp hence no inf and nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a01feb7-ab79-4915-a5de-928ad7cc47d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.7239, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.cross_entropy(logits , Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d50f35-da38-49f7-8a7c-d499b1b3cf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters are  3481\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of parameters are \" , sum (p.nelement() for p in params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0ccf0e-3f56-4323-8c00-4058eeee17ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini batch size :  73\n",
      "Number of parameters are  3481\n"
     ]
    }
   ],
   "source": [
    "# now we put it together and build the loop\n",
    "\n",
    "# embedding vector\n",
    "# hyper paramameters\n",
    "\n",
    "block_size = 3\n",
    "emb_size = 2\n",
    "vocab_size = len(chars)\n",
    "hidden_layer_neurons = 100\n",
    "batch_size = int(0.0004 * len(Xtr))\n",
    "print(\"Mini batch size : \",batch_size)\n",
    "\n",
    "hyper_params = [block_size , emb_size , vocab_size , hidden_layer_neurons ]\n",
    "\n",
    "# parameters\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "C = torch.rand((vocab_size , emb_size) , dtype = torch.float32 , requires_grad = True) # 27 x 2\n",
    "\n",
    "W1 = torch.rand((block_size * emb_size , hidden_layer_neurons  ) , dtype = torch.float32 , requires_grad = True) # 2*3 = 6 x 100\n",
    "b1 = torch.rand(hidden_layer_neurons , dtype = torch.float32 , requires_grad = True) # 100\n",
    "\n",
    "W2 = torch.rand(( hidden_layer_neurons , vocab_size ) , dtype = torch.float32 , requires_grad = True) # 100 x 27\n",
    "b2 = torch.rand(vocab_size , dtype = torch.float32 , requires_grad = True) # 27\n",
    "\n",
    "params = [C , W1, b1 , W2 , b2]\n",
    "print(\"Number of parameters are \" , sum (p.nelement() for p in params))\n",
    "\n",
    "# create mini_batch\n",
    "ix = torch.randint(0, len(Xtr), (batch_size,))\n",
    "X = Xtr[ix]\n",
    "Y = Ytr[ix]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7338679-972e-4fc8-8c42-59bdcc33631e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 : loss = 9.373917579650879\n",
      "Iteration 10 : loss = 2.924607038497925\n",
      "Iteration 20 : loss = 2.8582231998443604\n",
      "Iteration 30 : loss = 2.8392229080200195\n",
      "Iteration 40 : loss = 2.8302903175354004\n",
      "Iteration 50 : loss = 2.8256754875183105\n",
      "Iteration 60 : loss = 2.8222668170928955\n",
      "Iteration 70 : loss = 2.819258213043213\n",
      "Iteration 80 : loss = 2.8164031505584717\n",
      "Iteration 90 : loss = 2.8135671615600586\n",
      "Iteration 100 : loss = 2.8106565475463867\n",
      "Iteration 110 : loss = 2.8075947761535645\n",
      "Iteration 120 : loss = 2.8043134212493896\n",
      "Iteration 130 : loss = 2.800737142562866\n",
      "Iteration 140 : loss = 2.7967922687530518\n",
      "Iteration 150 : loss = 2.792391538619995\n",
      "Iteration 160 : loss = 2.7874338626861572\n",
      "Iteration 170 : loss = 2.781806468963623\n",
      "Iteration 180 : loss = 2.7753875255584717\n",
      "Iteration 190 : loss = 2.7680418491363525\n",
      "Iteration 200 : loss = 2.759645700454712\n",
      "Iteration 210 : loss = 2.750096082687378\n",
      "Iteration 220 : loss = 2.739339828491211\n",
      "Iteration 230 : loss = 2.7273952960968018\n",
      "Iteration 240 : loss = 2.7143309116363525\n",
      "Iteration 250 : loss = 2.700241804122925\n",
      "Iteration 260 : loss = 2.6852104663848877\n",
      "Iteration 270 : loss = 2.6692683696746826\n",
      "Iteration 280 : loss = 2.652397394180298\n",
      "Iteration 290 : loss = 2.634553909301758\n",
      "Iteration 300 : loss = 2.6156909465789795\n",
      "Iteration 310 : loss = 2.5957565307617188\n",
      "Iteration 320 : loss = 2.5747499465942383\n",
      "Iteration 330 : loss = 2.55268931388855\n",
      "Iteration 340 : loss = 2.529648780822754\n",
      "Iteration 350 : loss = 2.505740165710449\n",
      "Iteration 360 : loss = 2.481078863143921\n",
      "Iteration 370 : loss = 2.455754518508911\n",
      "Iteration 380 : loss = 2.4298126697540283\n",
      "Iteration 390 : loss = 2.403249502182007\n",
      "Iteration 400 : loss = 2.3760478496551514\n",
      "Iteration 410 : loss = 2.3482065200805664\n",
      "Iteration 420 : loss = 2.3197741508483887\n",
      "Iteration 430 : loss = 2.2908527851104736\n",
      "Iteration 440 : loss = 2.2616076469421387\n",
      "Iteration 450 : loss = 2.232215404510498\n",
      "Iteration 460 : loss = 2.2028489112854004\n",
      "Iteration 470 : loss = 2.173610210418701\n",
      "Iteration 480 : loss = 2.1445696353912354\n",
      "Iteration 490 : loss = 2.1157567501068115\n",
      "Iteration 500 : loss = 2.087200164794922\n",
      "Iteration 600 : loss = 1.82427978515625\n",
      "Iteration 700 : loss = 1.6007417440414429\n",
      "Iteration 800 : loss = 1.3822962045669556\n",
      "Iteration 900 : loss = 1.2023391723632812\n",
      "Iteration 1000 : loss = 1.0669547319412231\n",
      "Iteration 1100 : loss = 0.9694616198539734\n",
      "Iteration 1200 : loss = 0.9015824794769287\n",
      "Iteration 1300 : loss = 0.8520017862319946\n",
      "Iteration 1400 : loss = 0.8134307265281677\n",
      "Iteration 1500 : loss = 0.7818111181259155\n",
      "Iteration 1600 : loss = 0.7559409737586975\n",
      "Iteration 1700 : loss = 0.735205888748169\n",
      "Iteration 1800 : loss = 0.718647837638855\n",
      "Iteration 1900 : loss = 0.7054110765457153\n",
      "Iteration 2000 : loss = 0.6947786808013916\n",
      "Iteration 2100 : loss = 0.6861652135848999\n",
      "Iteration 2200 : loss = 0.6791095733642578\n",
      "Iteration 2300 : loss = 0.6732584238052368\n",
      "Iteration 2400 : loss = 0.6683475375175476\n",
      "Iteration 2500 : loss = 0.6641786694526672\n",
      "Iteration 2600 : loss = 0.6606040000915527\n",
      "Iteration 2700 : loss = 0.6575106978416443\n",
      "Iteration 2800 : loss = 0.6548126339912415\n",
      "Iteration 2900 : loss = 0.6524429321289062\n",
      "Iteration 3000 : loss = 0.6503480672836304\n",
      "Iteration 3100 : loss = 0.6484858393669128\n",
      "Iteration 3200 : loss = 0.6468215584754944\n",
      "Iteration 3300 : loss = 0.645327627658844\n",
      "Iteration 3400 : loss = 0.6439805626869202\n",
      "Iteration 3500 : loss = 0.6427611112594604\n",
      "Iteration 3600 : loss = 0.6416529417037964\n",
      "Iteration 3700 : loss = 0.640642523765564\n",
      "Iteration 3800 : loss = 0.6397184133529663\n",
      "Iteration 3900 : loss = 0.6388704776763916\n",
      "Iteration 4000 : loss = 0.638090193271637\n",
      "Iteration 4100 : loss = 0.6373704075813293\n",
      "Iteration 4200 : loss = 0.6367047429084778\n",
      "Iteration 4300 : loss = 0.6360874772071838\n",
      "Iteration 4400 : loss = 0.635513961315155\n",
      "Iteration 4500 : loss = 0.6349800825119019\n",
      "Iteration 4600 : loss = 0.6344818472862244\n",
      "Iteration 4700 : loss = 0.6340160965919495\n",
      "Iteration 4800 : loss = 0.6335799694061279\n",
      "Iteration 4900 : loss = 0.6331709623336792\n",
      "Iteration 5000 : loss = 0.6327865123748779\n",
      "Iteration 5100 : loss = 0.6324249505996704\n",
      "Iteration 5200 : loss = 0.6320841312408447\n",
      "Iteration 5300 : loss = 0.6317624449729919\n",
      "Iteration 5400 : loss = 0.6314585208892822\n",
      "Iteration 5500 : loss = 0.6311708092689514\n",
      "Iteration 5600 : loss = 0.6308984160423279\n",
      "Iteration 5700 : loss = 0.6306399703025818\n",
      "Iteration 5800 : loss = 0.6303946375846863\n",
      "Iteration 5900 : loss = 0.6301613450050354\n",
      "Iteration 6000 : loss = 0.6299393773078918\n",
      "Iteration 6100 : loss = 0.629727840423584\n",
      "Iteration 6200 : loss = 0.6295262575149536\n",
      "Iteration 6300 : loss = 0.6293337345123291\n",
      "Iteration 6400 : loss = 0.6291499733924866\n",
      "Iteration 6500 : loss = 0.6289742588996887\n",
      "Iteration 6600 : loss = 0.6288060545921326\n",
      "Iteration 6700 : loss = 0.6286450624465942\n",
      "Iteration 6800 : loss = 0.628490686416626\n",
      "Iteration 6900 : loss = 0.6283426284790039\n",
      "Iteration 7000 : loss = 0.6282007098197937\n",
      "Iteration 7100 : loss = 0.6280640363693237\n",
      "Iteration 7200 : loss = 0.6279328465461731\n",
      "Iteration 7300 : loss = 0.6278066635131836\n",
      "Iteration 7400 : loss = 0.6276851892471313\n",
      "Iteration 7500 : loss = 0.6275683045387268\n",
      "Iteration 7600 : loss = 0.6274555325508118\n",
      "Iteration 7700 : loss = 0.627346932888031\n",
      "Iteration 7800 : loss = 0.6272419691085815\n",
      "Iteration 7900 : loss = 0.6271407008171082\n",
      "Iteration 8000 : loss = 0.6270429491996765\n",
      "Iteration 8100 : loss = 0.6269485354423523\n",
      "Iteration 8200 : loss = 0.6268572807312012\n",
      "Iteration 8300 : loss = 0.6267687678337097\n",
      "Iteration 8400 : loss = 0.6266833543777466\n",
      "Iteration 8500 : loss = 0.6266005635261536\n",
      "Iteration 8600 : loss = 0.6265202760696411\n",
      "Iteration 8700 : loss = 0.626442551612854\n",
      "Iteration 8800 : loss = 0.6263672709465027\n",
      "Iteration 8900 : loss = 0.6262941360473633\n",
      "Iteration 9000 : loss = 0.6262232065200806\n",
      "Iteration 9100 : loss = 0.626154363155365\n",
      "Iteration 9200 : loss = 0.626087486743927\n",
      "Iteration 9300 : loss = 0.6260225772857666\n",
      "Iteration 9400 : loss = 0.6259593963623047\n",
      "Iteration 9500 : loss = 0.6258980631828308\n",
      "Iteration 9600 : loss = 0.6258385181427002\n",
      "Iteration 9700 : loss = 0.6257804036140442\n",
      "Iteration 9800 : loss = 0.6257239580154419\n",
      "Iteration 9900 : loss = 0.6256690621376038\n"
     ]
    }
   ],
   "source": [
    "# number of iterations\n",
    "num_itr = 10000\n",
    "\n",
    "# learning rate\n",
    "lr = 0.1\n",
    "\n",
    "for i in range(num_itr):\n",
    "    # forward pass\n",
    "    emb = C[X]\n",
    "    hidden = torch.tanh(emb.view(batch_size , block_size*emb_size)  @ W1 + b1)\n",
    "    logits = hidden @ W2 + b2\n",
    "    loss = F.cross_entropy(logits,Y)\n",
    "    if (i < 500 and i%10 ==0) or (i%100==0):\n",
    "        print(f'Iteration {i} : loss = {loss}')\n",
    "\n",
    "    # backward pass\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    for p in params:\n",
    "        p.data -= p.grad * lr\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b20fd7-a050-4926-bf13-5117a91b82d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we achieve a super low loss here coz we are overfitting this batch pf 182 samples - and 3480 params !!\n",
    "# and ya we cannot make loss zero , coz for most inputs , there are multple poss outputs so that wont ever happen\n",
    "# ... is supposed to predict a , o , e , etc (depeinid on all the starting letters of diferent names) \n",
    "# hence the prob for those will always be less than 1 , and so on\n",
    "\n",
    "# so for a more general loss , we use the whole data set , and take random minibatches of it in each iteration ,\n",
    "# making the learning faster , \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92981458-e21b-410d-a0e8-7c9337acbb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 : loss = 10.331025123596191\n",
      "Iteration 10 : loss = 8.360413551330566\n",
      "Iteration 20 : loss = 7.017969608306885\n",
      "Iteration 30 : loss = 5.076692581176758\n",
      "Iteration 40 : loss = 4.456148147583008\n",
      "Iteration 50 : loss = 4.460413455963135\n",
      "Iteration 60 : loss = 4.356618881225586\n",
      "Iteration 70 : loss = 3.916355609893799\n",
      "Iteration 80 : loss = 3.494235038757324\n",
      "Iteration 90 : loss = 3.4267807006835938\n",
      "Iteration 100 : loss = 3.661517858505249\n",
      "Iteration 110 : loss = 3.4106671810150146\n",
      "Iteration 120 : loss = 3.111236572265625\n",
      "Iteration 130 : loss = 2.9684159755706787\n",
      "Iteration 140 : loss = 2.8471484184265137\n",
      "Iteration 150 : loss = 3.0878546237945557\n",
      "Iteration 160 : loss = 2.8277485370635986\n",
      "Iteration 170 : loss = 3.2165262699127197\n",
      "Iteration 180 : loss = 3.3762621879577637\n",
      "Iteration 190 : loss = 2.9490301609039307\n",
      "Iteration 200 : loss = 2.9708616733551025\n",
      "Iteration 210 : loss = 3.016784191131592\n",
      "Iteration 220 : loss = 2.6250052452087402\n",
      "Iteration 230 : loss = 2.8556182384490967\n",
      "Iteration 240 : loss = 2.820850372314453\n",
      "Iteration 250 : loss = 3.206341028213501\n",
      "Iteration 260 : loss = 2.7178211212158203\n",
      "Iteration 270 : loss = 2.7392055988311768\n",
      "Iteration 280 : loss = 3.25683856010437\n",
      "Iteration 290 : loss = 2.8406012058258057\n",
      "Iteration 300 : loss = 2.9555532932281494\n",
      "Iteration 310 : loss = 2.7195119857788086\n",
      "Iteration 320 : loss = 2.6506848335266113\n",
      "Iteration 330 : loss = 3.008683443069458\n",
      "Iteration 340 : loss = 2.7266182899475098\n",
      "Iteration 350 : loss = 2.938539743423462\n",
      "Iteration 360 : loss = 2.995577573776245\n",
      "Iteration 370 : loss = 2.4817378520965576\n",
      "Iteration 380 : loss = 2.8495824337005615\n",
      "Iteration 390 : loss = 2.6826250553131104\n",
      "Iteration 400 : loss = 2.6837217807769775\n",
      "Iteration 410 : loss = 2.615692138671875\n",
      "Iteration 420 : loss = 2.637458562850952\n",
      "Iteration 430 : loss = 2.9208481311798096\n",
      "Iteration 440 : loss = 2.847386360168457\n",
      "Iteration 450 : loss = 2.9307589530944824\n",
      "Iteration 460 : loss = 2.6276042461395264\n",
      "Iteration 470 : loss = 2.8380796909332275\n",
      "Iteration 480 : loss = 2.493941068649292\n",
      "Iteration 490 : loss = 2.797102451324463\n",
      "Iteration 540 : loss = 2.7481064796447754\n",
      "Iteration 600 : loss = 2.777176856994629\n",
      "Iteration 660 : loss = 2.5666539669036865\n",
      "Iteration 720 : loss = 2.7411489486694336\n",
      "Iteration 780 : loss = 2.551382541656494\n",
      "Iteration 840 : loss = 2.6374173164367676\n",
      "Iteration 900 : loss = 2.6840059757232666\n",
      "Iteration 960 : loss = 2.680438756942749\n",
      "Iteration 1020 : loss = 2.7562057971954346\n",
      "Iteration 1080 : loss = 2.514160394668579\n",
      "Iteration 1140 : loss = 2.587679624557495\n",
      "Iteration 1200 : loss = 2.5180091857910156\n",
      "Iteration 1260 : loss = 2.6403300762176514\n",
      "Iteration 1320 : loss = 2.6879568099975586\n",
      "Iteration 1380 : loss = 2.6771833896636963\n",
      "Iteration 1440 : loss = 2.441986322402954\n",
      "Iteration 1500 : loss = 2.5962870121002197\n",
      "Iteration 1560 : loss = 2.3710572719573975\n",
      "Iteration 1620 : loss = 2.7093029022216797\n",
      "Iteration 1680 : loss = 2.6962618827819824\n",
      "Iteration 1740 : loss = 2.6090033054351807\n",
      "Iteration 1800 : loss = 2.484548807144165\n",
      "Iteration 1860 : loss = 2.382192850112915\n",
      "Iteration 1920 : loss = 2.4908177852630615\n",
      "Iteration 1980 : loss = 2.6206140518188477\n",
      "Iteration 2040 : loss = 2.575969696044922\n",
      "Iteration 2100 : loss = 2.732691526412964\n",
      "Iteration 2160 : loss = 2.638556957244873\n",
      "Iteration 2220 : loss = 2.5925583839416504\n",
      "Iteration 2280 : loss = 2.3803563117980957\n",
      "Iteration 2340 : loss = 2.5142159461975098\n",
      "Iteration 2400 : loss = 2.689307451248169\n",
      "Iteration 2460 : loss = 2.4331414699554443\n",
      "Iteration 2520 : loss = 2.5111465454101562\n",
      "Iteration 2580 : loss = 2.392578363418579\n",
      "Iteration 2640 : loss = 2.450300455093384\n",
      "Iteration 2700 : loss = 2.553727149963379\n",
      "Iteration 2760 : loss = 2.380704879760742\n",
      "Iteration 2820 : loss = 2.5778045654296875\n",
      "Iteration 2880 : loss = 2.6941752433776855\n",
      "Iteration 2940 : loss = 2.636882781982422\n",
      "Iteration 3000 : loss = 2.2580156326293945\n",
      "Iteration 3060 : loss = 2.475106716156006\n",
      "Iteration 3120 : loss = 2.625999927520752\n",
      "Iteration 3180 : loss = 2.6096880435943604\n",
      "Iteration 3240 : loss = 2.4998884201049805\n",
      "Iteration 3300 : loss = 2.385342836380005\n",
      "Iteration 3360 : loss = 2.60910701751709\n",
      "Iteration 3420 : loss = 2.557995080947876\n",
      "Iteration 3480 : loss = 2.5450050830841064\n",
      "Iteration 3540 : loss = 2.5685269832611084\n",
      "Iteration 3600 : loss = 2.5595011711120605\n",
      "Iteration 3660 : loss = 2.500178575515747\n",
      "Iteration 3720 : loss = 2.6437792778015137\n",
      "Iteration 3780 : loss = 2.597607374191284\n",
      "Iteration 3840 : loss = 2.6790056228637695\n",
      "Iteration 3900 : loss = 2.4846136569976807\n",
      "Iteration 3960 : loss = 2.322632312774658\n",
      "Iteration 4020 : loss = 2.4990999698638916\n",
      "Iteration 4080 : loss = 2.4673638343811035\n",
      "Iteration 4140 : loss = 2.453190326690674\n",
      "Iteration 4200 : loss = 2.6243906021118164\n",
      "Iteration 4260 : loss = 2.47576904296875\n",
      "Iteration 4320 : loss = 2.6927621364593506\n",
      "Iteration 4380 : loss = 2.440561056137085\n",
      "Iteration 4440 : loss = 2.540823459625244\n",
      "Iteration 4500 : loss = 2.553213357925415\n",
      "Iteration 4560 : loss = 2.5474932193756104\n",
      "Iteration 4620 : loss = 2.278658866882324\n",
      "Iteration 4680 : loss = 3.0786526203155518\n",
      "Iteration 4740 : loss = 2.6351518630981445\n",
      "Iteration 4800 : loss = 2.5922129154205322\n",
      "Iteration 4860 : loss = 2.438650131225586\n",
      "Iteration 4920 : loss = 2.5262694358825684\n",
      "Iteration 4980 : loss = 2.7229413986206055\n"
     ]
    }
   ],
   "source": [
    "# number of iterations\n",
    "num_itr = 5000\n",
    "\n",
    "# learning rate\n",
    "lr = 0.1\n",
    "\n",
    "for i in range(num_itr):\n",
    "    # minibatches\n",
    "    ix = torch.randint(0, len(Xtr), (batch_size,))\n",
    "    X = Xtr[ix]\n",
    "    Y = Ytr[ix]\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[X]\n",
    "    hidden = torch.tanh(emb.view(batch_size , block_size*emb_size)  @ W1 + b1)\n",
    "    logits = hidden @ W2 + b2\n",
    "    loss = F.cross_entropy(logits,Y)\n",
    "    if (i < 500 and i%10 ==0) or (i%60==0):\n",
    "        print(f'Iteration {i} : loss = {loss}')\n",
    "\n",
    "    # backward pass\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    for p in params:\n",
    "        p.data -= p.grad * lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d47d6a-20da-479c-8f42-787bb65761f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we get a way more accurate loss , but still , it isnt good enough and also we see that after 140 ish iterations\n",
    "# the loss becomes wavy and doesnt decline in the steady way we would want it to ...\n",
    "# this is because of the random choice of learning rate\n",
    "\n",
    "# SO LETS try to find a good learning rate... and the arrays of possible learning rates has to be spaced exponentially instead \n",
    "# of linearly because , there are just as many important lr in 10^-2 to 10^-1 as there are in 10^-1 to 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cad160-0161-4ad4-b1c6-3c0fc77e31ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.0000, -2.9970, -2.9940, -2.9910, -2.9880, -2.9850, -2.9820, -2.9790,\n",
      "        -2.9760, -2.9730, -2.9700, -2.9670, -2.9640, -2.9610, -2.9580, -2.9550,\n",
      "        -2.9520, -2.9489, -2.9459, -2.9429, -2.9399, -2.9369, -2.9339, -2.9309,\n",
      "        -2.9279, -2.9249, -2.9219, -2.9189, -2.9159, -2.9129, -2.9099, -2.9069,\n",
      "        -2.9039, -2.9009, -2.8979, -2.8949, -2.8919, -2.8889, -2.8859, -2.8829,\n",
      "        -2.8799, -2.8769, -2.8739, -2.8709, -2.8679, -2.8649, -2.8619, -2.8589,\n",
      "        -2.8559, -2.8529, -2.8498, -2.8468, -2.8438, -2.8408, -2.8378, -2.8348,\n",
      "        -2.8318, -2.8288, -2.8258, -2.8228, -2.8198, -2.8168, -2.8138, -2.8108,\n",
      "        -2.8078, -2.8048, -2.8018, -2.7988, -2.7958, -2.7928, -2.7898, -2.7868,\n",
      "        -2.7838, -2.7808, -2.7778, -2.7748, -2.7718, -2.7688, -2.7658, -2.7628,\n",
      "        -2.7598, -2.7568, -2.7538, -2.7508, -2.7477, -2.7447, -2.7417, -2.7387,\n",
      "        -2.7357, -2.7327, -2.7297, -2.7267, -2.7237, -2.7207, -2.7177, -2.7147,\n",
      "        -2.7117, -2.7087, -2.7057, -2.7027, -2.6997, -2.6967, -2.6937, -2.6907,\n",
      "        -2.6877, -2.6847, -2.6817, -2.6787, -2.6757, -2.6727, -2.6697, -2.6667,\n",
      "        -2.6637, -2.6607, -2.6577, -2.6547, -2.6517, -2.6486, -2.6456, -2.6426,\n",
      "        -2.6396, -2.6366, -2.6336, -2.6306, -2.6276, -2.6246, -2.6216, -2.6186,\n",
      "        -2.6156, -2.6126, -2.6096, -2.6066, -2.6036, -2.6006, -2.5976, -2.5946,\n",
      "        -2.5916, -2.5886, -2.5856, -2.5826, -2.5796, -2.5766, -2.5736, -2.5706,\n",
      "        -2.5676, -2.5646, -2.5616, -2.5586, -2.5556, -2.5526, -2.5495, -2.5465,\n",
      "        -2.5435, -2.5405, -2.5375, -2.5345, -2.5315, -2.5285, -2.5255, -2.5225,\n",
      "        -2.5195, -2.5165, -2.5135, -2.5105, -2.5075, -2.5045, -2.5015, -2.4985,\n",
      "        -2.4955, -2.4925, -2.4895, -2.4865, -2.4835, -2.4805, -2.4775, -2.4745,\n",
      "        -2.4715, -2.4685, -2.4655, -2.4625, -2.4595, -2.4565, -2.4535, -2.4505,\n",
      "        -2.4474, -2.4444, -2.4414, -2.4384, -2.4354, -2.4324, -2.4294, -2.4264,\n",
      "        -2.4234, -2.4204, -2.4174, -2.4144, -2.4114, -2.4084, -2.4054, -2.4024,\n",
      "        -2.3994, -2.3964, -2.3934, -2.3904, -2.3874, -2.3844, -2.3814, -2.3784,\n",
      "        -2.3754, -2.3724, -2.3694, -2.3664, -2.3634, -2.3604, -2.3574, -2.3544,\n",
      "        -2.3514, -2.3483, -2.3453, -2.3423, -2.3393, -2.3363, -2.3333, -2.3303,\n",
      "        -2.3273, -2.3243, -2.3213, -2.3183, -2.3153, -2.3123, -2.3093, -2.3063,\n",
      "        -2.3033, -2.3003, -2.2973, -2.2943, -2.2913, -2.2883, -2.2853, -2.2823,\n",
      "        -2.2793, -2.2763, -2.2733, -2.2703, -2.2673, -2.2643, -2.2613, -2.2583,\n",
      "        -2.2553, -2.2523, -2.2492, -2.2462, -2.2432, -2.2402, -2.2372, -2.2342,\n",
      "        -2.2312, -2.2282, -2.2252, -2.2222, -2.2192, -2.2162, -2.2132, -2.2102,\n",
      "        -2.2072, -2.2042, -2.2012, -2.1982, -2.1952, -2.1922, -2.1892, -2.1862,\n",
      "        -2.1832, -2.1802, -2.1772, -2.1742, -2.1712, -2.1682, -2.1652, -2.1622,\n",
      "        -2.1592, -2.1562, -2.1532, -2.1502, -2.1471, -2.1441, -2.1411, -2.1381,\n",
      "        -2.1351, -2.1321, -2.1291, -2.1261, -2.1231, -2.1201, -2.1171, -2.1141,\n",
      "        -2.1111, -2.1081, -2.1051, -2.1021, -2.0991, -2.0961, -2.0931, -2.0901,\n",
      "        -2.0871, -2.0841, -2.0811, -2.0781, -2.0751, -2.0721, -2.0691, -2.0661,\n",
      "        -2.0631, -2.0601, -2.0571, -2.0541, -2.0511, -2.0480, -2.0450, -2.0420,\n",
      "        -2.0390, -2.0360, -2.0330, -2.0300, -2.0270, -2.0240, -2.0210, -2.0180,\n",
      "        -2.0150, -2.0120, -2.0090, -2.0060, -2.0030, -2.0000, -1.9970, -1.9940,\n",
      "        -1.9910, -1.9880, -1.9850, -1.9820, -1.9790, -1.9760, -1.9730, -1.9700,\n",
      "        -1.9670, -1.9640, -1.9610, -1.9580, -1.9550, -1.9520, -1.9489, -1.9459,\n",
      "        -1.9429, -1.9399, -1.9369, -1.9339, -1.9309, -1.9279, -1.9249, -1.9219,\n",
      "        -1.9189, -1.9159, -1.9129, -1.9099, -1.9069, -1.9039, -1.9009, -1.8979,\n",
      "        -1.8949, -1.8919, -1.8889, -1.8859, -1.8829, -1.8799, -1.8769, -1.8739,\n",
      "        -1.8709, -1.8679, -1.8649, -1.8619, -1.8589, -1.8559, -1.8529, -1.8498,\n",
      "        -1.8468, -1.8438, -1.8408, -1.8378, -1.8348, -1.8318, -1.8288, -1.8258,\n",
      "        -1.8228, -1.8198, -1.8168, -1.8138, -1.8108, -1.8078, -1.8048, -1.8018,\n",
      "        -1.7988, -1.7958, -1.7928, -1.7898, -1.7868, -1.7838, -1.7808, -1.7778,\n",
      "        -1.7748, -1.7718, -1.7688, -1.7658, -1.7628, -1.7598, -1.7568, -1.7538,\n",
      "        -1.7508, -1.7477, -1.7447, -1.7417, -1.7387, -1.7357, -1.7327, -1.7297,\n",
      "        -1.7267, -1.7237, -1.7207, -1.7177, -1.7147, -1.7117, -1.7087, -1.7057,\n",
      "        -1.7027, -1.6997, -1.6967, -1.6937, -1.6907, -1.6877, -1.6847, -1.6817,\n",
      "        -1.6787, -1.6757, -1.6727, -1.6697, -1.6667, -1.6637, -1.6607, -1.6577,\n",
      "        -1.6547, -1.6517, -1.6486, -1.6456, -1.6426, -1.6396, -1.6366, -1.6336,\n",
      "        -1.6306, -1.6276, -1.6246, -1.6216, -1.6186, -1.6156, -1.6126, -1.6096,\n",
      "        -1.6066, -1.6036, -1.6006, -1.5976, -1.5946, -1.5916, -1.5886, -1.5856,\n",
      "        -1.5826, -1.5796, -1.5766, -1.5736, -1.5706, -1.5676, -1.5646, -1.5616,\n",
      "        -1.5586, -1.5556, -1.5526, -1.5495, -1.5465, -1.5435, -1.5405, -1.5375,\n",
      "        -1.5345, -1.5315, -1.5285, -1.5255, -1.5225, -1.5195, -1.5165, -1.5135,\n",
      "        -1.5105, -1.5075, -1.5045, -1.5015, -1.4985, -1.4955, -1.4925, -1.4895,\n",
      "        -1.4865, -1.4835, -1.4805, -1.4775, -1.4745, -1.4715, -1.4685, -1.4655,\n",
      "        -1.4625, -1.4595, -1.4565, -1.4535, -1.4505, -1.4474, -1.4444, -1.4414,\n",
      "        -1.4384, -1.4354, -1.4324, -1.4294, -1.4264, -1.4234, -1.4204, -1.4174,\n",
      "        -1.4144, -1.4114, -1.4084, -1.4054, -1.4024, -1.3994, -1.3964, -1.3934,\n",
      "        -1.3904, -1.3874, -1.3844, -1.3814, -1.3784, -1.3754, -1.3724, -1.3694,\n",
      "        -1.3664, -1.3634, -1.3604, -1.3574, -1.3544, -1.3514, -1.3483, -1.3453,\n",
      "        -1.3423, -1.3393, -1.3363, -1.3333, -1.3303, -1.3273, -1.3243, -1.3213,\n",
      "        -1.3183, -1.3153, -1.3123, -1.3093, -1.3063, -1.3033, -1.3003, -1.2973,\n",
      "        -1.2943, -1.2913, -1.2883, -1.2853, -1.2823, -1.2793, -1.2763, -1.2733,\n",
      "        -1.2703, -1.2673, -1.2643, -1.2613, -1.2583, -1.2553, -1.2523, -1.2492,\n",
      "        -1.2462, -1.2432, -1.2402, -1.2372, -1.2342, -1.2312, -1.2282, -1.2252,\n",
      "        -1.2222, -1.2192, -1.2162, -1.2132, -1.2102, -1.2072, -1.2042, -1.2012,\n",
      "        -1.1982, -1.1952, -1.1922, -1.1892, -1.1862, -1.1832, -1.1802, -1.1772,\n",
      "        -1.1742, -1.1712, -1.1682, -1.1652, -1.1622, -1.1592, -1.1562, -1.1532,\n",
      "        -1.1502, -1.1471, -1.1441, -1.1411, -1.1381, -1.1351, -1.1321, -1.1291,\n",
      "        -1.1261, -1.1231, -1.1201, -1.1171, -1.1141, -1.1111, -1.1081, -1.1051,\n",
      "        -1.1021, -1.0991, -1.0961, -1.0931, -1.0901, -1.0871, -1.0841, -1.0811,\n",
      "        -1.0781, -1.0751, -1.0721, -1.0691, -1.0661, -1.0631, -1.0601, -1.0571,\n",
      "        -1.0541, -1.0511, -1.0480, -1.0450, -1.0420, -1.0390, -1.0360, -1.0330,\n",
      "        -1.0300, -1.0270, -1.0240, -1.0210, -1.0180, -1.0150, -1.0120, -1.0090,\n",
      "        -1.0060, -1.0030, -1.0000, -0.9970, -0.9940, -0.9910, -0.9880, -0.9850,\n",
      "        -0.9820, -0.9790, -0.9760, -0.9730, -0.9700, -0.9670, -0.9640, -0.9610,\n",
      "        -0.9580, -0.9550, -0.9520, -0.9489, -0.9459, -0.9429, -0.9399, -0.9369,\n",
      "        -0.9339, -0.9309, -0.9279, -0.9249, -0.9219, -0.9189, -0.9159, -0.9129,\n",
      "        -0.9099, -0.9069, -0.9039, -0.9009, -0.8979, -0.8949, -0.8919, -0.8889,\n",
      "        -0.8859, -0.8829, -0.8799, -0.8769, -0.8739, -0.8709, -0.8679, -0.8649,\n",
      "        -0.8619, -0.8589, -0.8559, -0.8529, -0.8498, -0.8468, -0.8438, -0.8408,\n",
      "        -0.8378, -0.8348, -0.8318, -0.8288, -0.8258, -0.8228, -0.8198, -0.8168,\n",
      "        -0.8138, -0.8108, -0.8078, -0.8048, -0.8018, -0.7988, -0.7958, -0.7928,\n",
      "        -0.7898, -0.7868, -0.7838, -0.7808, -0.7778, -0.7748, -0.7718, -0.7688,\n",
      "        -0.7658, -0.7628, -0.7598, -0.7568, -0.7538, -0.7508, -0.7477, -0.7447,\n",
      "        -0.7417, -0.7387, -0.7357, -0.7327, -0.7297, -0.7267, -0.7237, -0.7207,\n",
      "        -0.7177, -0.7147, -0.7117, -0.7087, -0.7057, -0.7027, -0.6997, -0.6967,\n",
      "        -0.6937, -0.6907, -0.6877, -0.6847, -0.6817, -0.6787, -0.6757, -0.6727,\n",
      "        -0.6697, -0.6667, -0.6637, -0.6607, -0.6577, -0.6547, -0.6517, -0.6486,\n",
      "        -0.6456, -0.6426, -0.6396, -0.6366, -0.6336, -0.6306, -0.6276, -0.6246,\n",
      "        -0.6216, -0.6186, -0.6156, -0.6126, -0.6096, -0.6066, -0.6036, -0.6006,\n",
      "        -0.5976, -0.5946, -0.5916, -0.5886, -0.5856, -0.5826, -0.5796, -0.5766,\n",
      "        -0.5736, -0.5706, -0.5676, -0.5646, -0.5616, -0.5586, -0.5556, -0.5526,\n",
      "        -0.5495, -0.5465, -0.5435, -0.5405, -0.5375, -0.5345, -0.5315, -0.5285,\n",
      "        -0.5255, -0.5225, -0.5195, -0.5165, -0.5135, -0.5105, -0.5075, -0.5045,\n",
      "        -0.5015, -0.4985, -0.4955, -0.4925, -0.4895, -0.4865, -0.4835, -0.4805,\n",
      "        -0.4775, -0.4745, -0.4715, -0.4685, -0.4655, -0.4625, -0.4595, -0.4565,\n",
      "        -0.4535, -0.4505, -0.4474, -0.4444, -0.4414, -0.4384, -0.4354, -0.4324,\n",
      "        -0.4294, -0.4264, -0.4234, -0.4204, -0.4174, -0.4144, -0.4114, -0.4084,\n",
      "        -0.4054, -0.4024, -0.3994, -0.3964, -0.3934, -0.3904, -0.3874, -0.3844,\n",
      "        -0.3814, -0.3784, -0.3754, -0.3724, -0.3694, -0.3664, -0.3634, -0.3604,\n",
      "        -0.3574, -0.3544, -0.3514, -0.3483, -0.3453, -0.3423, -0.3393, -0.3363,\n",
      "        -0.3333, -0.3303, -0.3273, -0.3243, -0.3213, -0.3183, -0.3153, -0.3123,\n",
      "        -0.3093, -0.3063, -0.3033, -0.3003, -0.2973, -0.2943, -0.2913, -0.2883,\n",
      "        -0.2853, -0.2823, -0.2793, -0.2763, -0.2733, -0.2703, -0.2673, -0.2643,\n",
      "        -0.2613, -0.2583, -0.2553, -0.2523, -0.2492, -0.2462, -0.2432, -0.2402,\n",
      "        -0.2372, -0.2342, -0.2312, -0.2282, -0.2252, -0.2222, -0.2192, -0.2162,\n",
      "        -0.2132, -0.2102, -0.2072, -0.2042, -0.2012, -0.1982, -0.1952, -0.1922,\n",
      "        -0.1892, -0.1862, -0.1832, -0.1802, -0.1772, -0.1742, -0.1712, -0.1682,\n",
      "        -0.1652, -0.1622, -0.1592, -0.1562, -0.1532, -0.1502, -0.1471, -0.1441,\n",
      "        -0.1411, -0.1381, -0.1351, -0.1321, -0.1291, -0.1261, -0.1231, -0.1201,\n",
      "        -0.1171, -0.1141, -0.1111, -0.1081, -0.1051, -0.1021, -0.0991, -0.0961,\n",
      "        -0.0931, -0.0901, -0.0871, -0.0841, -0.0811, -0.0781, -0.0751, -0.0721,\n",
      "        -0.0691, -0.0661, -0.0631, -0.0601, -0.0571, -0.0541, -0.0511, -0.0480,\n",
      "        -0.0450, -0.0420, -0.0390, -0.0360, -0.0330, -0.0300, -0.0270, -0.0240,\n",
      "        -0.0210, -0.0180, -0.0150, -0.0120, -0.0090, -0.0060, -0.0030,  0.0000])\n",
      "tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011,\n",
      "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
      "        0.0011, 0.0011, 0.0011, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
      "        0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0013, 0.0013, 0.0013,\n",
      "        0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0014,\n",
      "        0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
      "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
      "        0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
      "        0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017,\n",
      "        0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0019,\n",
      "        0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0020, 0.0020,\n",
      "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0021, 0.0021, 0.0021, 0.0021,\n",
      "        0.0021, 0.0021, 0.0021, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
      "        0.0022, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0024, 0.0024,\n",
      "        0.0024, 0.0024, 0.0024, 0.0024, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
      "        0.0025, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0027, 0.0027, 0.0027,\n",
      "        0.0027, 0.0027, 0.0027, 0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0029,\n",
      "        0.0029, 0.0029, 0.0029, 0.0029, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n",
      "        0.0031, 0.0031, 0.0031, 0.0031, 0.0032, 0.0032, 0.0032, 0.0032, 0.0032,\n",
      "        0.0033, 0.0033, 0.0033, 0.0033, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
      "        0.0035, 0.0035, 0.0035, 0.0035, 0.0036, 0.0036, 0.0036, 0.0036, 0.0037,\n",
      "        0.0037, 0.0037, 0.0037, 0.0038, 0.0038, 0.0038, 0.0039, 0.0039, 0.0039,\n",
      "        0.0039, 0.0040, 0.0040, 0.0040, 0.0040, 0.0041, 0.0041, 0.0041, 0.0042,\n",
      "        0.0042, 0.0042, 0.0042, 0.0043, 0.0043, 0.0043, 0.0044, 0.0044, 0.0044,\n",
      "        0.0045, 0.0045, 0.0045, 0.0045, 0.0046, 0.0046, 0.0046, 0.0047, 0.0047,\n",
      "        0.0047, 0.0048, 0.0048, 0.0048, 0.0049, 0.0049, 0.0049, 0.0050, 0.0050,\n",
      "        0.0050, 0.0051, 0.0051, 0.0051, 0.0052, 0.0052, 0.0053, 0.0053, 0.0053,\n",
      "        0.0054, 0.0054, 0.0054, 0.0055, 0.0055, 0.0056, 0.0056, 0.0056, 0.0057,\n",
      "        0.0057, 0.0058, 0.0058, 0.0058, 0.0059, 0.0059, 0.0060, 0.0060, 0.0060,\n",
      "        0.0061, 0.0061, 0.0062, 0.0062, 0.0062, 0.0063, 0.0063, 0.0064, 0.0064,\n",
      "        0.0065, 0.0065, 0.0066, 0.0066, 0.0067, 0.0067, 0.0067, 0.0068, 0.0068,\n",
      "        0.0069, 0.0069, 0.0070, 0.0070, 0.0071, 0.0071, 0.0072, 0.0072, 0.0073,\n",
      "        0.0073, 0.0074, 0.0074, 0.0075, 0.0075, 0.0076, 0.0076, 0.0077, 0.0077,\n",
      "        0.0078, 0.0079, 0.0079, 0.0080, 0.0080, 0.0081, 0.0081, 0.0082, 0.0082,\n",
      "        0.0083, 0.0084, 0.0084, 0.0085, 0.0085, 0.0086, 0.0086, 0.0087, 0.0088,\n",
      "        0.0088, 0.0089, 0.0090, 0.0090, 0.0091, 0.0091, 0.0092, 0.0093, 0.0093,\n",
      "        0.0094, 0.0095, 0.0095, 0.0096, 0.0097, 0.0097, 0.0098, 0.0099, 0.0099,\n",
      "        0.0100, 0.0101, 0.0101, 0.0102, 0.0103, 0.0104, 0.0104, 0.0105, 0.0106,\n",
      "        0.0106, 0.0107, 0.0108, 0.0109, 0.0109, 0.0110, 0.0111, 0.0112, 0.0112,\n",
      "        0.0113, 0.0114, 0.0115, 0.0116, 0.0116, 0.0117, 0.0118, 0.0119, 0.0120,\n",
      "        0.0121, 0.0121, 0.0122, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0127,\n",
      "        0.0128, 0.0129, 0.0130, 0.0131, 0.0132, 0.0133, 0.0134, 0.0135, 0.0136,\n",
      "        0.0137, 0.0137, 0.0138, 0.0139, 0.0140, 0.0141, 0.0142, 0.0143, 0.0144,\n",
      "        0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.0150, 0.0151, 0.0152, 0.0154,\n",
      "        0.0155, 0.0156, 0.0157, 0.0158, 0.0159, 0.0160, 0.0161, 0.0162, 0.0163,\n",
      "        0.0165, 0.0166, 0.0167, 0.0168, 0.0169, 0.0170, 0.0171, 0.0173, 0.0174,\n",
      "        0.0175, 0.0176, 0.0178, 0.0179, 0.0180, 0.0181, 0.0182, 0.0184, 0.0185,\n",
      "        0.0186, 0.0188, 0.0189, 0.0190, 0.0192, 0.0193, 0.0194, 0.0196, 0.0197,\n",
      "        0.0198, 0.0200, 0.0201, 0.0202, 0.0204, 0.0205, 0.0207, 0.0208, 0.0210,\n",
      "        0.0211, 0.0212, 0.0214, 0.0215, 0.0217, 0.0218, 0.0220, 0.0221, 0.0223,\n",
      "        0.0225, 0.0226, 0.0228, 0.0229, 0.0231, 0.0232, 0.0234, 0.0236, 0.0237,\n",
      "        0.0239, 0.0241, 0.0242, 0.0244, 0.0246, 0.0247, 0.0249, 0.0251, 0.0253,\n",
      "        0.0254, 0.0256, 0.0258, 0.0260, 0.0261, 0.0263, 0.0265, 0.0267, 0.0269,\n",
      "        0.0271, 0.0273, 0.0274, 0.0276, 0.0278, 0.0280, 0.0282, 0.0284, 0.0286,\n",
      "        0.0288, 0.0290, 0.0292, 0.0294, 0.0296, 0.0298, 0.0300, 0.0302, 0.0304,\n",
      "        0.0307, 0.0309, 0.0311, 0.0313, 0.0315, 0.0317, 0.0320, 0.0322, 0.0324,\n",
      "        0.0326, 0.0328, 0.0331, 0.0333, 0.0335, 0.0338, 0.0340, 0.0342, 0.0345,\n",
      "        0.0347, 0.0350, 0.0352, 0.0354, 0.0357, 0.0359, 0.0362, 0.0364, 0.0367,\n",
      "        0.0369, 0.0372, 0.0375, 0.0377, 0.0380, 0.0382, 0.0385, 0.0388, 0.0390,\n",
      "        0.0393, 0.0396, 0.0399, 0.0401, 0.0404, 0.0407, 0.0410, 0.0413, 0.0416,\n",
      "        0.0418, 0.0421, 0.0424, 0.0427, 0.0430, 0.0433, 0.0436, 0.0439, 0.0442,\n",
      "        0.0445, 0.0448, 0.0451, 0.0455, 0.0458, 0.0461, 0.0464, 0.0467, 0.0471,\n",
      "        0.0474, 0.0477, 0.0480, 0.0484, 0.0487, 0.0491, 0.0494, 0.0497, 0.0501,\n",
      "        0.0504, 0.0508, 0.0511, 0.0515, 0.0518, 0.0522, 0.0526, 0.0529, 0.0533,\n",
      "        0.0537, 0.0540, 0.0544, 0.0548, 0.0552, 0.0556, 0.0559, 0.0563, 0.0567,\n",
      "        0.0571, 0.0575, 0.0579, 0.0583, 0.0587, 0.0591, 0.0595, 0.0599, 0.0604,\n",
      "        0.0608, 0.0612, 0.0616, 0.0621, 0.0625, 0.0629, 0.0634, 0.0638, 0.0642,\n",
      "        0.0647, 0.0651, 0.0656, 0.0660, 0.0665, 0.0670, 0.0674, 0.0679, 0.0684,\n",
      "        0.0688, 0.0693, 0.0698, 0.0703, 0.0708, 0.0713, 0.0718, 0.0723, 0.0728,\n",
      "        0.0733, 0.0738, 0.0743, 0.0748, 0.0753, 0.0758, 0.0764, 0.0769, 0.0774,\n",
      "        0.0780, 0.0785, 0.0790, 0.0796, 0.0802, 0.0807, 0.0813, 0.0818, 0.0824,\n",
      "        0.0830, 0.0835, 0.0841, 0.0847, 0.0853, 0.0859, 0.0865, 0.0871, 0.0877,\n",
      "        0.0883, 0.0889, 0.0895, 0.0901, 0.0908, 0.0914, 0.0920, 0.0927, 0.0933,\n",
      "        0.0940, 0.0946, 0.0953, 0.0959, 0.0966, 0.0973, 0.0979, 0.0986, 0.0993,\n",
      "        0.1000, 0.1007, 0.1014, 0.1021, 0.1028, 0.1035, 0.1042, 0.1050, 0.1057,\n",
      "        0.1064, 0.1072, 0.1079, 0.1087, 0.1094, 0.1102, 0.1109, 0.1117, 0.1125,\n",
      "        0.1133, 0.1140, 0.1148, 0.1156, 0.1164, 0.1172, 0.1181, 0.1189, 0.1197,\n",
      "        0.1205, 0.1214, 0.1222, 0.1231, 0.1239, 0.1248, 0.1256, 0.1265, 0.1274,\n",
      "        0.1283, 0.1292, 0.1301, 0.1310, 0.1319, 0.1328, 0.1337, 0.1346, 0.1356,\n",
      "        0.1365, 0.1374, 0.1384, 0.1394, 0.1403, 0.1413, 0.1423, 0.1433, 0.1443,\n",
      "        0.1453, 0.1463, 0.1473, 0.1483, 0.1493, 0.1504, 0.1514, 0.1525, 0.1535,\n",
      "        0.1546, 0.1557, 0.1567, 0.1578, 0.1589, 0.1600, 0.1611, 0.1623, 0.1634,\n",
      "        0.1645, 0.1657, 0.1668, 0.1680, 0.1691, 0.1703, 0.1715, 0.1727, 0.1739,\n",
      "        0.1751, 0.1763, 0.1775, 0.1788, 0.1800, 0.1812, 0.1825, 0.1838, 0.1850,\n",
      "        0.1863, 0.1876, 0.1889, 0.1902, 0.1916, 0.1929, 0.1942, 0.1956, 0.1969,\n",
      "        0.1983, 0.1997, 0.2010, 0.2024, 0.2038, 0.2053, 0.2067, 0.2081, 0.2096,\n",
      "        0.2110, 0.2125, 0.2140, 0.2154, 0.2169, 0.2184, 0.2200, 0.2215, 0.2230,\n",
      "        0.2246, 0.2261, 0.2277, 0.2293, 0.2309, 0.2325, 0.2341, 0.2357, 0.2373,\n",
      "        0.2390, 0.2406, 0.2423, 0.2440, 0.2457, 0.2474, 0.2491, 0.2508, 0.2526,\n",
      "        0.2543, 0.2561, 0.2579, 0.2597, 0.2615, 0.2633, 0.2651, 0.2669, 0.2688,\n",
      "        0.2707, 0.2725, 0.2744, 0.2763, 0.2783, 0.2802, 0.2821, 0.2841, 0.2861,\n",
      "        0.2880, 0.2900, 0.2921, 0.2941, 0.2961, 0.2982, 0.3002, 0.3023, 0.3044,\n",
      "        0.3065, 0.3087, 0.3108, 0.3130, 0.3151, 0.3173, 0.3195, 0.3217, 0.3240,\n",
      "        0.3262, 0.3285, 0.3308, 0.3331, 0.3354, 0.3377, 0.3400, 0.3424, 0.3448,\n",
      "        0.3472, 0.3496, 0.3520, 0.3544, 0.3569, 0.3594, 0.3619, 0.3644, 0.3669,\n",
      "        0.3695, 0.3720, 0.3746, 0.3772, 0.3798, 0.3825, 0.3851, 0.3878, 0.3905,\n",
      "        0.3932, 0.3959, 0.3987, 0.4014, 0.4042, 0.4070, 0.4098, 0.4127, 0.4155,\n",
      "        0.4184, 0.4213, 0.4243, 0.4272, 0.4302, 0.4331, 0.4362, 0.4392, 0.4422,\n",
      "        0.4453, 0.4484, 0.4515, 0.4546, 0.4578, 0.4610, 0.4642, 0.4674, 0.4706,\n",
      "        0.4739, 0.4772, 0.4805, 0.4838, 0.4872, 0.4906, 0.4940, 0.4974, 0.5008,\n",
      "        0.5043, 0.5078, 0.5113, 0.5149, 0.5185, 0.5221, 0.5257, 0.5293, 0.5330,\n",
      "        0.5367, 0.5404, 0.5442, 0.5479, 0.5517, 0.5556, 0.5594, 0.5633, 0.5672,\n",
      "        0.5712, 0.5751, 0.5791, 0.5831, 0.5872, 0.5913, 0.5954, 0.5995, 0.6036,\n",
      "        0.6078, 0.6120, 0.6163, 0.6206, 0.6249, 0.6292, 0.6336, 0.6380, 0.6424,\n",
      "        0.6469, 0.6513, 0.6559, 0.6604, 0.6650, 0.6696, 0.6743, 0.6789, 0.6837,\n",
      "        0.6884, 0.6932, 0.6980, 0.7028, 0.7077, 0.7126, 0.7176, 0.7225, 0.7275,\n",
      "        0.7326, 0.7377, 0.7428, 0.7480, 0.7531, 0.7584, 0.7636, 0.7689, 0.7743,\n",
      "        0.7796, 0.7850, 0.7905, 0.7960, 0.8015, 0.8071, 0.8127, 0.8183, 0.8240,\n",
      "        0.8297, 0.8355, 0.8412, 0.8471, 0.8530, 0.8589, 0.8648, 0.8708, 0.8769,\n",
      "        0.8830, 0.8891, 0.8953, 0.9015, 0.9077, 0.9140, 0.9204, 0.9268, 0.9332,\n",
      "        0.9397, 0.9462, 0.9528, 0.9594, 0.9660, 0.9727, 0.9795, 0.9863, 0.9931,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "lre = torch.linspace(-3,0,1000)\n",
    "print(lre)\n",
    "lrs = 10**lre\n",
    "print(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03592641-cf2b-49e9-8cb5-46eb2e6e9cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 : loss = 2.4577043056488037\n",
      "Iteration 10 : loss = 2.3981451988220215\n",
      "Iteration 20 : loss = 2.47373104095459\n",
      "Iteration 30 : loss = 2.6726291179656982\n",
      "Iteration 40 : loss = 2.1875550746917725\n",
      "Iteration 50 : loss = 2.570056200027466\n",
      "Iteration 60 : loss = 2.7222506999969482\n",
      "Iteration 70 : loss = 2.4639081954956055\n",
      "Iteration 80 : loss = 2.6085593700408936\n",
      "Iteration 90 : loss = 2.501979351043701\n",
      "Iteration 100 : loss = 2.577425718307495\n",
      "Iteration 110 : loss = 2.458922863006592\n",
      "Iteration 120 : loss = 2.5312509536743164\n",
      "Iteration 130 : loss = 2.3368232250213623\n",
      "Iteration 140 : loss = 2.356492042541504\n",
      "Iteration 150 : loss = 2.5203986167907715\n",
      "Iteration 160 : loss = 2.3732528686523438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 170 : loss = 2.4152791500091553\n",
      "Iteration 180 : loss = 2.505908250808716\n",
      "Iteration 190 : loss = 2.5759739875793457\n",
      "Iteration 200 : loss = 2.6471807956695557\n",
      "Iteration 210 : loss = 2.489762306213379\n",
      "Iteration 220 : loss = 2.3893871307373047\n",
      "Iteration 230 : loss = 2.675510883331299\n",
      "Iteration 240 : loss = 2.3984858989715576\n",
      "Iteration 250 : loss = 2.527090311050415\n",
      "Iteration 260 : loss = 2.714221954345703\n",
      "Iteration 270 : loss = 2.411743402481079\n",
      "Iteration 280 : loss = 2.54780650138855\n",
      "Iteration 290 : loss = 2.489705801010132\n",
      "Iteration 300 : loss = 2.5921552181243896\n",
      "Iteration 310 : loss = 2.6650609970092773\n",
      "Iteration 320 : loss = 2.4389779567718506\n",
      "Iteration 330 : loss = 2.4569060802459717\n",
      "Iteration 340 : loss = 2.341202735900879\n",
      "Iteration 350 : loss = 2.322201728820801\n",
      "Iteration 360 : loss = 2.078017234802246\n",
      "Iteration 370 : loss = 2.580625057220459\n",
      "Iteration 380 : loss = 2.4043352603912354\n",
      "Iteration 390 : loss = 2.3105602264404297\n",
      "Iteration 400 : loss = 2.3194284439086914\n",
      "Iteration 410 : loss = 2.392336845397949\n",
      "Iteration 420 : loss = 2.2900736331939697\n",
      "Iteration 430 : loss = 2.719822406768799\n",
      "Iteration 440 : loss = 2.366997480392456\n",
      "Iteration 450 : loss = 2.475102663040161\n",
      "Iteration 460 : loss = 2.6492724418640137\n",
      "Iteration 470 : loss = 2.2205910682678223\n",
      "Iteration 480 : loss = 2.5159859657287598\n",
      "Iteration 490 : loss = 2.340473175048828\n"
     ]
    }
   ],
   "source": [
    "# number of iterations\n",
    "num_itr = 1000\n",
    "\n",
    "# learning rate\n",
    "step_i = []\n",
    "\n",
    "\n",
    "for i in range(num_itr):\n",
    "    # minibatches\n",
    "    ix = torch.randint(0, len(Xtr), (batch_size,))\n",
    "    X = Xtr[ix]\n",
    "    Y = Ytr[ix]\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[X]\n",
    "    hidden = torch.tanh(emb.view(batch_size , block_size*emb_size)  @ W1 + b1)\n",
    "    logits = hidden @ W2 + b2\n",
    "    loss = F.cross_entropy(logits,Y)\n",
    "    step_i.append(loss.item())\n",
    "    if (i < 500 and i%10 ==0) or (i%1000==0):\n",
    "        print(f'Iteration {i} : loss = {loss}')\n",
    "\n",
    "    # backward pass\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    for p in params:\n",
    "        p.data -= p.grad * lrs[i]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0068730-102e-47c6-ab81-a6dc56221f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x752c7f97c680>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lre , step_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1687677-c11e-4c98-b6a0-52609dcfc7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# around the valley region is a good lr -> 10^-1 = 0.1 was p good anyways , we choose it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f980311e-6266-4672-a73c-a5738e698a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini batch size :  73\n",
      "Number of parameters are  3481\n",
      "Iteration 0 : loss = 5.295319080352783\n",
      "Iteration 50 : loss = 2.9174914360046387\n",
      "Iteration 100 : loss = 2.8784730434417725\n",
      "Iteration 150 : loss = 2.449430227279663\n",
      "Iteration 200 : loss = 2.5590627193450928\n",
      "Iteration 250 : loss = 2.7324352264404297\n",
      "Iteration 300 : loss = 2.7387192249298096\n",
      "Iteration 350 : loss = 2.673959493637085\n",
      "Iteration 400 : loss = 2.738643169403076\n",
      "Iteration 450 : loss = 2.585407257080078\n",
      "Iteration 500 : loss = 2.6656742095947266\n",
      "Iteration 1000 : loss = 2.687943696975708\n",
      "Iteration 2000 : loss = 2.4571285247802734\n",
      "Iteration 3000 : loss = 2.329030990600586\n",
      "Iteration 4000 : loss = 2.275143623352051\n",
      "Iteration 5000 : loss = 2.3750998973846436\n",
      "Iteration 6000 : loss = 2.245107650756836\n",
      "Iteration 0 : loss = 2.626063346862793\n",
      "Iteration 50 : loss = 2.4208028316497803\n",
      "Iteration 100 : loss = 2.6535096168518066\n",
      "Iteration 150 : loss = 2.571354389190674\n",
      "Iteration 200 : loss = 2.4837896823883057\n",
      "Iteration 250 : loss = 2.4386024475097656\n",
      "Iteration 300 : loss = 2.3314199447631836\n",
      "Iteration 350 : loss = 2.103468894958496\n",
      "Iteration 400 : loss = 2.4494502544403076\n",
      "Iteration 450 : loss = 2.49176287651062\n",
      "Iteration 500 : loss = 2.3618977069854736\n",
      "Iteration 1000 : loss = 2.119875907897949\n",
      "Iteration 2000 : loss = 2.3039917945861816\n",
      "Iteration 3000 : loss = 2.4007906913757324\n",
      "Iteration 0 : loss = 2.2358829975128174\n",
      "Iteration 50 : loss = 2.315749406814575\n",
      "Iteration 100 : loss = 2.472806453704834\n",
      "Iteration 150 : loss = 2.3381919860839844\n",
      "Iteration 200 : loss = 2.216193675994873\n",
      "Iteration 250 : loss = 2.3062891960144043\n",
      "Iteration 300 : loss = 2.330793857574463\n",
      "Iteration 350 : loss = 2.3997576236724854\n",
      "Iteration 400 : loss = 2.3537559509277344\n",
      "Iteration 450 : loss = 2.2320265769958496\n",
      "Iteration 500 : loss = 2.287381887435913\n",
      "Iteration 1000 : loss = 2.3838839530944824\n",
      "Iteration 2000 : loss = 2.214207172393799\n",
      "Iteration 3000 : loss = 2.5697197914123535\n",
      "Iteration 4000 : loss = 2.333376407623291\n",
      "Iteration 5000 : loss = 2.4477384090423584\n",
      "Iteration 6000 : loss = 2.3554446697235107\n"
     ]
    }
   ],
   "source": [
    "# re -run it\n",
    "\n",
    "# now we put it together and build the loop\n",
    "\n",
    "# embedding vector\n",
    "# hyper paramameters\n",
    "\n",
    "block_size = 3\n",
    "emb_size = 2\n",
    "vocab_size = len(chars)\n",
    "hidden_layer_neurons = 100\n",
    "batch_size = int(0.0004 * len(Xtr))\n",
    "print(\"Mini batch size : \",batch_size)\n",
    "\n",
    "hyper_params = [block_size , emb_size , vocab_size , hidden_layer_neurons ]\n",
    "\n",
    "# parameters\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "C = torch.rand((vocab_size , emb_size) , dtype = torch.float32 , requires_grad = True) # 27 x 2\n",
    "\n",
    "W1 = torch.rand((block_size * emb_size , hidden_layer_neurons  ) , dtype = torch.float32 , requires_grad = True) # 2*3 = 6 x 100\n",
    "b1 = torch.rand(hidden_layer_neurons , dtype = torch.float32 , requires_grad = True) # 100\n",
    "\n",
    "W2 = torch.rand(( hidden_layer_neurons , vocab_size ) , dtype = torch.float32 , requires_grad = True) # 100 x 27\n",
    "b2 = torch.rand(vocab_size , dtype = torch.float32 , requires_grad = True) # 27\n",
    "\n",
    "params = [C , W1, b1 , W2 , b2]\n",
    "print(\"Number of parameters are \" , sum (p.nelement() for p in params))\n",
    "\n",
    "# create mini_batch\n",
    "ix = torch.randint(0, len(Xtr), (batch_size,))\n",
    "X = Xtr[ix]\n",
    "Y = Ytr[ix]\n",
    "\n",
    "\n",
    "# number of iterations\n",
    "num_itr = 6500\n",
    "\n",
    "\n",
    "for i in range(num_itr):\n",
    "    # minibatches\n",
    "    ix = torch.randint(0, len(Xtr), (batch_size,))\n",
    "    X = Xtr[ix]\n",
    "    Y = Ytr[ix]\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[X]\n",
    "    hidden = torch.tanh(emb.view(batch_size , block_size*emb_size)  @ W1 + b1)\n",
    "    logits = hidden @ W2 + b2\n",
    "    loss = F.cross_entropy(logits,Y)\n",
    "    if (i <= 500 and i%50 ==0) or (i%1000==0):\n",
    "        print(f'Iteration {i} : loss = {loss}')\n",
    "\n",
    "    # backward pass\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    for p in params:\n",
    "        p.data -= p.grad * 0.1\n",
    "\n",
    "for i in range(num_itr//2):\n",
    "    # minibatches\n",
    "    ix = torch.randint(0, len(Xtr), (batch_size,))\n",
    "    X = Xtr[ix]\n",
    "    Y = Ytr[ix]\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[X]\n",
    "    hidden = torch.tanh(emb.view(batch_size , block_size*emb_size)  @ W1 + b1)\n",
    "    logits = hidden @ W2 + b2\n",
    "    loss = F.cross_entropy(logits,Y)\n",
    "    if (i <= 500 and i%50 ==0) or (i%1000==0):\n",
    "        print(f'Iteration {i} : loss = {loss}')\n",
    "\n",
    "    # backward pass\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    for p in params:\n",
    "        p.data -= p.grad * 0.1\n",
    "\n",
    "\n",
    "for i in range(num_itr):\n",
    "    # minibatches\n",
    "    ix = torch.randint(0, len(Xtr), (batch_size,))\n",
    "    X = Xtr[ix]\n",
    "    Y = Ytr[ix]\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[X]\n",
    "    hidden = torch.tanh(emb.view(batch_size , block_size*emb_size)  @ W1 + b1)\n",
    "    logits = hidden @ W2 + b2\n",
    "    loss = F.cross_entropy(logits,Y)\n",
    "    if (i <= 500 and i%50 ==0) or (i%1000==0):\n",
    "        print(f'Iteration {i} : loss = {loss}')\n",
    "\n",
    "    # backward pass\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    for p in params:\n",
    "        p.data -= p.grad * 0.004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933108c3-809c-45c1-b964-0ce59d9f119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we continue running it till we get stagnated then we do a learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be91238e-dfd4-418f-8805-20c71cb00dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2204, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#  now we test on dev split\n",
    "ix = torch.randint(0, len(Xdev), (batch_size,))\n",
    "X = Xdev[ix]\n",
    "Y = Ydev[ix]\n",
    "\n",
    "# forward pass\n",
    "emb = C[X]\n",
    "hidden = torch.tanh(emb.view(batch_size , block_size*emb_size)  @ W1 + b1)\n",
    "logits = hidden @ W2 + b2\n",
    "loss = F.cross_entropy(logits,Y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c7f9b3-609e-4081-a2a8-ac5fbda74128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev has the same as training => we are prolly underfitting , which is good , so we should increase params and imrpve loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d8eceb-bab6-4dd6-abb7-6965e06abb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini batch size :  63\n",
      "Number of parameters are  7595\n",
      "Iteration 0 : loss = 4.164789199829102\n",
      "Iteration 50 : loss = 2.8596112728118896\n",
      "Iteration 100 : loss = 2.772709846496582\n",
      "Iteration 150 : loss = 2.6900949478149414\n",
      "Iteration 200 : loss = 2.6396849155426025\n",
      "Iteration 250 : loss = 2.8479440212249756\n",
      "Iteration 300 : loss = 2.7749485969543457\n",
      "Iteration 350 : loss = 2.475573778152466\n",
      "Iteration 400 : loss = 2.4661405086517334\n",
      "Iteration 450 : loss = 2.5738773345947266\n",
      "Iteration 500 : loss = 2.52573823928833\n",
      "Iteration 1000 : loss = 2.6915576457977295\n",
      "Iteration 2000 : loss = 2.2942898273468018\n",
      "Iteration 3000 : loss = 2.49971866607666\n",
      "Iteration 4000 : loss = 2.3787455558776855\n",
      "Iteration 5000 : loss = 2.4979090690612793\n",
      "Iteration 6000 : loss = 2.4624364376068115\n",
      "Iteration 7000 : loss = 2.2813005447387695\n",
      "Iteration 8000 : loss = 2.760359764099121\n",
      "Iteration 9000 : loss = 2.14269757270813\n",
      "Iteration 10000 : loss = 2.310370683670044\n",
      "Iteration 11000 : loss = 2.1161129474639893\n",
      "Iteration 12000 : loss = 2.038196563720703\n",
      "Iteration 13000 : loss = 2.2103655338287354\n",
      "Iteration 14000 : loss = 1.9159425497055054\n",
      "Iteration 15000 : loss = 2.07291316986084\n",
      "Iteration 16000 : loss = 2.1978957653045654\n",
      "Iteration 17000 : loss = 2.257622241973877\n"
     ]
    }
   ],
   "source": [
    "# re -run it\n",
    "\n",
    "# now we put it together and build the loop\n",
    "\n",
    "# embedding vector\n",
    "# hyper paramameters\n",
    "\n",
    "block_size = 3\n",
    "emb_size = 2\n",
    "vocab_size = len(chars)\n",
    "hidden_layer_neurons = 221\n",
    "batch_size = int(0.00035 * len(Xtr))\n",
    "print(\"Mini batch size : \",batch_size)\n",
    "\n",
    "hyper_params = [block_size , emb_size , vocab_size , hidden_layer_neurons ]\n",
    "\n",
    "# parameters\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "C = torch.randn((vocab_size , emb_size) , dtype = torch.float32 , requires_grad = True) # 27 x 2\n",
    "\n",
    "W1 = torch.randn((block_size * emb_size , hidden_layer_neurons  ) , dtype = torch.float32 , requires_grad = True) # 2*3 = 6 x 100\n",
    "b1 = torch.randn(hidden_layer_neurons , dtype = torch.float32 , requires_grad = True) # 100\n",
    "\n",
    "W2 = torch.randn(( hidden_layer_neurons , vocab_size ) , dtype = torch.float32 , requires_grad = True)   # 100 x 27\n",
    "W2.data *= 0.1\n",
    "b2 = torch.zeros(vocab_size , dtype = torch.float32 , requires_grad = True) # 27\n",
    "\n",
    "params = [C , W1, b1 , W2 , b2]\n",
    "print(\"Number of parameters are \" , sum (p.nelement() for p in params))\n",
    "\n",
    "# create mini_batch\n",
    "ix = torch.randint(0, len(Xtr), (batch_size,))\n",
    "X = Xtr[ix]\n",
    "Y = Ytr[ix]\n",
    "\n",
    "\n",
    "# number of iterations\n",
    "num_itr = 17500\n",
    "\n",
    "\n",
    "for i in range(num_itr):\n",
    "    # minibatches\n",
    "    ix = torch.randint(0, len(Xtr), (batch_size,))\n",
    "    X = Xtr[ix]\n",
    "    Y = Ytr[ix]\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[X]\n",
    "    hidden = torch.tanh(emb.view(batch_size , block_size*emb_size)  @ W1 + b1)\n",
    "    logits = hidden @ W2 + b2\n",
    "    loss = F.cross_entropy(logits,Y)\n",
    "    if (i <= 500 and i%50 ==0) or (i%1000==0):\n",
    "        print(f'Iteration {i} : loss = {loss}')\n",
    "\n",
    "    # backward pass\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    if 0 <= i < 7000:\n",
    "        lr = 0.1\n",
    "    elif 7000 <= i <12000:\n",
    "        lr = 0.06\n",
    "    elif 12000<= i < 16000:\n",
    "        lr = 0.02\n",
    "    else:\n",
    "        lr = 0.009\n",
    "\n",
    "    for p in params:\n",
    "        p.data -= p.grad * lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37806e55-39ee-4edf-8df3-eb0546fc26f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0818, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#  now we test on dev split\n",
    "ix = torch.randint(0, len(Xdev), (batch_size,))\n",
    "X = Xdev[ix]\n",
    "Y = Ydev[ix]\n",
    "\n",
    "# forward pass\n",
    "emb = C[X]\n",
    "hidden = torch.tanh(emb.view(batch_size , block_size*emb_size)  @ W1 + b1)\n",
    "logits = hidden @ W2 + b2\n",
    "loss = F.cross_entropy(logits,Y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d26f3b8-0ae7-4c81-965a-f84b2434dc50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize dimensions 0 and 1 of the embedding matrix C for all characters\n",
    "# only for 2 dim  vectors \n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(C[:,0].data, C[:,1].data, s=200)\n",
    "for i in range(C.shape[0]):\n",
    "    plt.text(C[i,0].item(), C[i,1].item(), itos[i], ha=\"center\", va=\"center\", color='white')\n",
    "plt.grid('minor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f95747-e206-487e-8654-0e4f888210ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ffffff' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m X \u001b[38;5;241m=\u001b[39m Xtst[ix]\n\u001b[1;32m      4\u001b[0m Y \u001b[38;5;241m=\u001b[39m Ytst[ix]\n\u001b[0;32m----> 5\u001b[0m ffffff\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[1;32m      7\u001b[0m emb \u001b[38;5;241m=\u001b[39m C[X]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ffffff' is not defined"
     ]
    }
   ],
   "source": [
    "#  now we test on test split\n",
    "ix = torch.randint(0, len(Xtst), (batch_size,))\n",
    "X = Xtst[ix]\n",
    "Y = Ytst[ix]\n",
    "\n",
    "# forward pass\n",
    "emb = C[X]\n",
    "hidden = torch.tanh(emb.view(batch_size , block_size*emb_size)  @ W1 + b1)\n",
    "logits = hidden @ W2 + b2\n",
    "loss = F.cross_entropy(logits,Y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eef630-5a78-4f38-9503-52aa1a3607a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cavayzia.\n",
      "sajanzekdeteni.\n",
      "hemitia.\n",
      "adela.\n",
      "benn.\n",
      "itthespy.\n",
      "miri.\n",
      "stary.\n",
      "jaylyn.\n",
      "shana.\n"
     ]
    }
   ],
   "source": [
    "# let's sample\n",
    "\n",
    "for _ in range(10):\n",
    "    context = [0]*block_size\n",
    "    outs = []\n",
    "    while True:\n",
    "        emb = C[torch.tensor(context)]\n",
    "        hidden = torch.tanh(emb.view(1 , block_size*emb_size) @ W1 + b1)\n",
    "        logits = hidden @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        next = torch.multinomial(probs, num_samples=1).item()\n",
    "        context = context[1:] + [next]\n",
    "        outs += [itos[next]]\n",
    "        if next == 0:\n",
    "            break\n",
    "\n",
    "    print(\"\".join(outs))\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284f1c13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063d07c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

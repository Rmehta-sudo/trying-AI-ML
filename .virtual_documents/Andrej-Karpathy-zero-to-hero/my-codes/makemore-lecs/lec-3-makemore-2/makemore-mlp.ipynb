





# let's code 
print("Hello")





# plan of action
# words ,  split into block-sized-context and outputs 
# split into train , dev , testing data
# emdedding vector -> vocab     X embed_dimensions 
# context_size*embed_dimensions X num_neurons_lay_1
# 2nd layer: num_neurons_lay_1  X num_poss_outputs

import torch
import random
import torch.nn.functional as F
import matplotlib.pyplot as plt


# start
words = open("names.txt" , "r").read().splitlines()

chars = set(''.join(words))
chars.add('.')

# itos , stoi

stoi , itos = {} , {}
sorted_chars = sorted(chars)
for index , char in enumerate(sorted_chars):
    stoi[char] =  index
    itos[index] =  char

print("stoi : " , stoi)
print("itos : " , itos)
print()


# create the dataset

block_size = 3

random.seed(42)
random.shuffle(words)

xs , ys = [] , []
for word in words:
    # print(word)
    word = '.' + word + '.'
    context = [0]*block_size
    for i in range(len(word)-1):
        context = context[1:] + [stoi[word[i]]]
        # print(context)
        xs.append(context)
        ys.append(stoi[word[i+1]])

# for i in range(len(xs)):
#     print(xs[i] , ys[i])
xs = torch.tensor(xs)
ys = torch.tensor(ys)
print("input contexts: ",xs)
print()
print("outputs: " , ys)


# spitting the data

n1 = int(0.8*len(xs))
n2 = int(0.9*len(xs))

print(n1,n2)

Xtr = xs[:n1]
Ytr = ys[:n1]

Xdev = xs[n1+1:n2]
Ydev = ys[n1+1:n2]

Xtst = xs[n2+1:]
Ytst = ys[n2+1:]

print(Xtr,Ytr,Xdev,Ydev,Xtst,Ytst)


# embedding vector
# hyper paramameters

block_size = 3
emb_size = 2
vocab_size = len(chars)
hidden_layer_neurons = 100
batch_size = int(0.001 * len(Xtr))
print("Mini batch size : ",batch_size)

hyper_params = [block_size , emb_size , vocab_size , hidden_layer_neurons ]

# parameters

g = torch.Generator().manual_seed(2147483647) # for reproducibility

C = torch.rand((vocab_size , emb_size) , dtype = torch.float32 , requires_grad = True) # 27 x 2
print(C)

W1 = torch.rand((block_size * emb_size , hidden_layer_neurons  ) , dtype = torch.float32 , requires_grad = True) # 2*3 = 6 x 100
b1 = torch.rand(hidden_layer_neurons , dtype = torch.float32 , requires_grad = True) # 100

W2 = torch.rand(( hidden_layer_neurons , vocab_size ) , dtype = torch.float32 , requires_grad = True) # 100 x 27
b2 = torch.rand(vocab_size , dtype = torch.float32 , requires_grad = True) # 27

params = [C , W1, b1 , W2 , b2]


print(params)


# create mini_batch
ix = torch.randint(0, len(Xtr), (batch_size,))
X = Xtr[ix]
Y = Ytr[ix]
emb = C[X]

emb


emb.shape


hidden = torch.tanh(emb.view( batch_size , block_size * emb_size) @ W1 + b1)


hidden.shape


logits = (hidden @ W2 + b2)
logits


logits.shape


counts = logits.exp()
counts


probs = counts / counts.sum(1 , keepdim=True)
print(probs)

loss = - probs[torch.arange(batch_size) , Y].log().mean()
loss


# this manual way of calculating is annoying , makes for worse backprop and creates issues like storing inf and nan
# instead , we use the inbuild softmax function that allows for better backprop using analytically derived derivatives , and 
# it first subtracts the largest value from each row before raises exp hence no inf and nan



loss = F.cross_entropy(logits , Y)
loss


print("Number of parameters are " , sum (p.nelement() for p in params))


# now we put it together and build the loop

# embedding vector
# hyper paramameters

block_size = 3
emb_size = 2
vocab_size = len(chars)
hidden_layer_neurons = 100
batch_size = int(0.0004 * len(Xtr))
print("Mini batch size : ",batch_size)

hyper_params = [block_size , emb_size , vocab_size , hidden_layer_neurons ]

# parameters

g = torch.Generator().manual_seed(2147483647) # for reproducibility

C = torch.rand((vocab_size , emb_size) , dtype = torch.float32 , requires_grad = True) # 27 x 2

W1 = torch.rand((block_size * emb_size , hidden_layer_neurons  ) , dtype = torch.float32 , requires_grad = True) # 2*3 = 6 x 100
b1 = torch.rand(hidden_layer_neurons , dtype = torch.float32 , requires_grad = True) # 100

W2 = torch.rand(( hidden_layer_neurons , vocab_size ) , dtype = torch.float32 , requires_grad = True) # 100 x 27
b2 = torch.rand(vocab_size , dtype = torch.float32 , requires_grad = True) # 27

params = [C , W1, b1 , W2 , b2]
print("Number of parameters are " , sum (p.nelement() for p in params))

# create mini_batch
ix = torch.randint(0, len(Xtr), (batch_size,))
X = Xtr[ix]
Y = Ytr[ix]



# number of iterations
num_itr = 10000

# learning rate
lr = 0.1

for i in range(num_itr):
    # forward pass
    emb = C[X]
    hidden = torch.tanh(emb.view(batch_size , block_size*emb_size)  @ W1 + b1)
    logits = hidden @ W2 + b2
    loss = F.cross_entropy(logits,Y)
    if (i < 500 and i%10 ==0) or (i%100==0):
        print(f'Iteration {i} : loss = {loss}')

    # backward pass
    for p in params:
        p.grad = None
    loss.backward()

    for p in params:
        p.data -= p.grad * lr
    



# we achieve a super low loss here coz we are overfitting this batch pf 182 samples - and 3480 params !!
# and ya we cannot make loss zero , coz for most inputs , there are multple poss outputs so that wont ever happen
# ... is supposed to predict a , o , e , etc (depeinid on all the starting letters of diferent names) 
# hence the prob for those will always be less than 1 , and so on

# so for a more general loss , we use the whole data set , and take random minibatches of it in each iteration ,
# making the learning faster , 



# number of iterations
num_itr = 5000

# learning rate
lr = 0.1

for i in range(num_itr):
    # minibatches
    ix = torch.randint(0, len(Xtr), (batch_size,))
    X = Xtr[ix]
    Y = Ytr[ix]
    
    # forward pass
    emb = C[X]
    hidden = torch.tanh(emb.view(batch_size , block_size*emb_size)  @ W1 + b1)
    logits = hidden @ W2 + b2
    loss = F.cross_entropy(logits,Y)
    if (i < 500 and i%10 ==0) or (i%60==0):
        print(f'Iteration {i} : loss = {loss}')

    # backward pass
    for p in params:
        p.grad = None
    loss.backward()

    for p in params:
        p.data -= p.grad * lr


# now we get a way more accurate loss , but still , it isnt good enough and also we see that after 140 ish iterations
# the loss becomes wavy and doesnt decline in the steady way we would want it to ...
# this is because of the random choice of learning rate

# SO LETS try to find a good learning rate... and the arrays of possible learning rates has to be spaced exponentially instead 
# of linearly because , there are just as many important lr in 10^-2 to 10^-1 as there are in 10^-1 to 1 



lre = torch.linspace(-3,0,1000)
print(lre)
lrs = 10**lre
print(lrs)


# number of iterations
num_itr = 1000

# learning rate
step_i = []


for i in range(num_itr):
    # minibatches
    ix = torch.randint(0, len(Xtr), (batch_size,))
    X = Xtr[ix]
    Y = Ytr[ix]
    
    # forward pass
    emb = C[X]
    hidden = torch.tanh(emb.view(batch_size , block_size*emb_size)  @ W1 + b1)
    logits = hidden @ W2 + b2
    loss = F.cross_entropy(logits,Y)
    step_i.append(loss.item())
    if (i < 500 and i%10 ==0) or (i%1000==0):
        print(f'Iteration {i} : loss = {loss}')

    # backward pass
    for p in params:
        p.grad = None
    loss.backward()

    for p in params:
        p.data -= p.grad * lrs[i]
        


plt.plot(lre , step_i)


# around the valley region is a good lr -> 10^-1 = 0.1 was p good anyways , we choose it 


# re -run it

# now we put it together and build the loop

# embedding vector
# hyper paramameters

block_size = 3
emb_size = 2
vocab_size = len(chars)
hidden_layer_neurons = 100
batch_size = int(0.0004 * len(Xtr))
print("Mini batch size : ",batch_size)

hyper_params = [block_size , emb_size , vocab_size , hidden_layer_neurons ]

# parameters

g = torch.Generator().manual_seed(2147483647) # for reproducibility

C = torch.rand((vocab_size , emb_size) , dtype = torch.float32 , requires_grad = True) # 27 x 2

W1 = torch.rand((block_size * emb_size , hidden_layer_neurons  ) , dtype = torch.float32 , requires_grad = True) # 2*3 = 6 x 100
b1 = torch.rand(hidden_layer_neurons , dtype = torch.float32 , requires_grad = True) # 100

W2 = torch.rand(( hidden_layer_neurons , vocab_size ) , dtype = torch.float32 , requires_grad = True) # 100 x 27
b2 = torch.rand(vocab_size , dtype = torch.float32 , requires_grad = True) # 27

params = [C , W1, b1 , W2 , b2]
print("Number of parameters are " , sum (p.nelement() for p in params))

# create mini_batch
ix = torch.randint(0, len(Xtr), (batch_size,))
X = Xtr[ix]
Y = Ytr[ix]


# number of iterations
num_itr = 6500


for i in range(num_itr):
    # minibatches
    ix = torch.randint(0, len(Xtr), (batch_size,))
    X = Xtr[ix]
    Y = Ytr[ix]
    
    # forward pass
    emb = C[X]
    hidden = torch.tanh(emb.view(batch_size , block_size*emb_size)  @ W1 + b1)
    logits = hidden @ W2 + b2
    loss = F.cross_entropy(logits,Y)
    if (i <= 500 and i%50 ==0) or (i%1000==0):
        print(f'Iteration {i} : loss = {loss}')

    # backward pass
    for p in params:
        p.grad = None
    loss.backward()

    for p in params:
        p.data -= p.grad * 0.1

for i in range(num_itr//2):
    # minibatches
    ix = torch.randint(0, len(Xtr), (batch_size,))
    X = Xtr[ix]
    Y = Ytr[ix]
    
    # forward pass
    emb = C[X]
    hidden = torch.tanh(emb.view(batch_size , block_size*emb_size)  @ W1 + b1)
    logits = hidden @ W2 + b2
    loss = F.cross_entropy(logits,Y)
    if (i <= 500 and i%50 ==0) or (i%1000==0):
        print(f'Iteration {i} : loss = {loss}')

    # backward pass
    for p in params:
        p.grad = None
    loss.backward()

    for p in params:
        p.data -= p.grad * 0.1


for i in range(num_itr):
    # minibatches
    ix = torch.randint(0, len(Xtr), (batch_size,))
    X = Xtr[ix]
    Y = Ytr[ix]
    
    # forward pass
    emb = C[X]
    hidden = torch.tanh(emb.view(batch_size , block_size*emb_size)  @ W1 + b1)
    logits = hidden @ W2 + b2
    loss = F.cross_entropy(logits,Y)
    if (i <= 500 and i%50 ==0) or (i%1000==0):
        print(f'Iteration {i} : loss = {loss}')

    # backward pass
    for p in params:
        p.grad = None
    loss.backward()

    for p in params:
        p.data -= p.grad * 0.004


# we continue running it till we get stagnated then we do a learning rate decay


#  now we test on dev split
ix = torch.randint(0, len(Xdev), (batch_size,))
X = Xdev[ix]
Y = Ydev[ix]

# forward pass
emb = C[X]
hidden = torch.tanh(emb.view(batch_size , block_size*emb_size)  @ W1 + b1)
logits = hidden @ W2 + b2
loss = F.cross_entropy(logits,Y)
print(loss)


# dev has the same as training => we are prolly underfitting , which is good , so we should increase params and imrpve loss


# re -run it

# now we put it together and build the loop

# embedding vector
# hyper paramameters

block_size = 3
emb_size = 6
vocab_size = len(chars)
hidden_layer_neurons = 250
batch_size = int(0.0003 * len(Xtr))
print("Mini batch size : ",batch_size)

hyper_params = [block_size , emb_size , vocab_size , hidden_layer_neurons ]

# parameters

g = torch.Generator().manual_seed(2147483647) # for reproducibility

C = torch.rand((vocab_size , emb_size) , dtype = torch.float32 , requires_grad = True) # 27 x 2

W1 = torch.rand((block_size * emb_size , hidden_layer_neurons  ) , dtype = torch.float32 , requires_grad = True) # 2*3 = 6 x 100
b1 = torch.rand(hidden_layer_neurons , dtype = torch.float32 , requires_grad = True) # 100

W2 = torch.rand(( hidden_layer_neurons , vocab_size ) , dtype = torch.float32 , requires_grad = True) # 100 x 27
b2 = torch.rand(vocab_size , dtype = torch.float32 , requires_grad = True) # 27

params = [C , W1, b1 , W2 , b2]
print("Number of parameters are " , sum (p.nelement() for p in params))

# create mini_batch
ix = torch.randint(0, len(Xtr), (batch_size,))
X = Xtr[ix]
Y = Ytr[ix]


# number of iterations
num_itr = 6500


for i in range(num_itr):
    # minibatches
    ix = torch.randint(0, len(Xtr), (batch_size,))
    X = Xtr[ix]
    Y = Ytr[ix]
    
    # forward pass
    emb = C[X]
    hidden = torch.tanh(emb.view(batch_size , block_size*emb_size)  @ W1 + b1)
    logits = hidden @ W2 + b2
    loss = F.cross_entropy(logits,Y)
    if (i <= 500 and i%50 ==0) or (i%1000==0):
        print(f'Iteration {i} : loss = {loss}')

    # backward pass
    for p in params:
        p.grad = None
    loss.backward()

    for p in params:
        p.data -= p.grad * (0.1 if i <2500 else 0.07)

for i in range(num_itr//2):
    # minibatches
    ix = torch.randint(0, len(Xtr), (batch_size,))
    X = Xtr[ix]
    Y = Ytr[ix]
    
    # forward pass
    emb = C[X]
    hidden = torch.tanh(emb.view(batch_size , block_size*emb_size)  @ W1 + b1)
    logits = hidden @ W2 + b2
    loss = F.cross_entropy(logits,Y)
    if (i <= 500 and i%50 ==0) or (i%1000==0):
        print(f'Iteration {i} : loss = {loss}')

    # backward pass
    for p in params:
        p.grad = None
    loss.backward()

    for p in params:
        p.data -= p.grad * (0.07 if i <2000 else 0.04)


for i in range(num_itr):
    # minibatches
    ix = torch.randint(0, len(Xtr), (batch_size,))
    X = Xtr[ix]
    Y = Ytr[ix]
    
    # forward pass
    emb = C[X]
    hidden = torch.tanh(emb.view(batch_size , block_size*emb_size)  @ W1 + b1)
    logits = hidden @ W2 + b2
    loss = F.cross_entropy(logits,Y)
    if (i <= 500 and i%50 ==0) or (i%1000==0):
        print(f'Iteration {i} : loss = {loss}')

    # backward pass
    for p in params:
        p.grad = None
    loss.backward()

    for p in params:
        p.data -= p.grad * 0.01


#  now we test on dev split
ix = torch.randint(0, len(Xdev), (batch_size,))
X = Xdev[ix]
Y = Ydev[ix]

# forward pass
emb = C[X]
hidden = torch.tanh(emb.view(batch_size , block_size*emb_size)  @ W1 + b1)
logits = hidden @ W2 + b2
loss = F.cross_entropy(logits,Y)
print(loss)


# visualize dimensions 0 and 1 of the embedding matrix C for all characters
# only for 2 dim  vectors 
plt.figure(figsize=(8,8))
plt.scatter(C[:,0].data, C[:,1].data, s=200)
for i in range(C.shape[0]):
    plt.text(C[i,0].item(), C[i,1].item(), itos[i], ha="center", va="center", color='white')
plt.grid('minor')


#  now we test on test split
ix = torch.randint(0, len(Xtst), (batch_size,))
X = Xtst[ix]
Y = Ytst[ix]

# forward pass
emb = C[X]
hidden = torch.tanh(emb.view(batch_size , block_size*emb_size)  @ W1 + b1)
logits = hidden @ W2 + b2
loss = F.cross_entropy(logits,Y)
print(loss)


# let's sample

for _ in range(10):
    context = [0]*block_size
    outs = []
    while True:
        emb = C[torch.tensor(context)]
        hidden = torch.tanh(emb.view(1 , block_size*emb_size) @ W1 + b1)
        logits = hidden @ W2 + b2
        probs = F.softmax(logits, dim=1)
        next = torch.multinomial(probs, num_samples=1).item()
        context = context[1:] + [next]
        outs += [itos[next]]
        if next == 0:
            break

    print("".join(outs))

   










# let's code 
print("Hello")


words = open('names.txt' , 'r').read().splitlines()


# mess around with the data
print(words)


W = words[:5]
print(W) # use this for smaller sampling and intuitive understanding


# Create our data

#bigrams
bigrams = []
for word in words:
    word = '.' + word + '.'
    for ch1 , ch2 in zip (word , word[1:]):
        bigrams.append(ch1+ch2)

print("bigrams are : ",bigrams)


# character set
chars = set(''.join(words))
chars.add('.')
print("Our charset is : " , chars)


# mapping -> 27 chars in a 27-D space
stoi , itos = {} , {}
sorted_chars = sorted(chars)
for index , char in enumerate(sorted_chars):
    stoi[char] =  index
    itos[index] =  char

print("stoi : " , stoi)
print("itos : " , itos)



# creating the count and prob matrix
import torch
import matplotlib.pyplot as plt


N = torch.zeros(27,27 , dtype = torch.int32)
print(N)

for bigram in bigrams:
    ch1 = bigram[0]
    ch2 = bigram[1]
    N[stoi[ch1]][stoi[ch2]] += 1

print("With counts : " , N)


# plot it 
plt.imshow(N)


# better plot
get_ipython().run_line_magic('matplotlib', 'inline')

plt.figure(figsize=(16,16))
plt.imshow(N, cmap='Blues')
for i in range(27):
    for j in range(27):
        chstr = itos[i] + itos[j]
        plt.text(j, i, chstr, ha="center", va="bottom", color='gray')
        plt.text(j, i, N[i, j].item(), ha="center", va="top", color='gray')
plt.axis('off');



# now creating the prob 
# experiment first


print(N[0])
print(N[0]/N[0].sum())

# to do this on the whole matrix , we want to to sum along the rows , create a sum COLUMN VECTOR 
# and divide the main matrix by that ... thus we wanna sum along the rows (dim = 1)
# torch.sum(N , dim = 1 , keepdim = True) , this sums along the rows(dim 1) and preserves the 
# number of dimensions 
print(torch.sum(N , dim = 1).shape)
print(torch.sum(N , dim = 1,keepdim=True).shape)

eg = torch.tensor([[10,10,10] ,
                   [5,5,5],
                 [2,2,2]])
print(torch.sum(eg , dim = 1))
print(torch.sum(eg , dim = 1,keepdim=True))
print(eg / torch.sum(eg , dim = 1) , "\nhere it is  divided by \n[[30,15,6]\n,[30,15,6],\n[30,15,6]]\n\n")
print(eg / torch.sum(eg , dim = 1,keepdim = True) , "\nhere it is correctly divided by \n[[30,30,30],\n[15,15,15],\n[6,6,6]]")



# now creating the prob 

N = N.float()
P = N/torch.sum(N , dim=1 , keepdim=True)
print(P)
# we however want to smoothen it out , coz in our data , some probabilites are zero
# and they may cause a loss of inf later on 
N = N+1
P = N/torch.sum(N , dim=1 , keepdim = True)
print("Probs with smoothening:\n",P)





# calculating nll - negative log likelhood
sum_log = 0
num = 0
for bigram in bigrams :
    ch1 = bigram[0] 
    ch2 = bigram[1]
    sum_log += torch.log(P[stoi[ch1]][stoi[ch2]])
    num += 1
print(sum_log)
print("Average negative log likelihood = ",sum_log/num)


# Now we shall sample from this statistical model and also check loss of random words

def whatstheloss(word):
    nll = 0
    n = 0
    for ch1 , ch2 in zip(word , word[1:]):
        nll += -torch.log(P[stoi[ch1]][stoi[ch2]])
        n += 1
    return nll/n

names = ['andrej',"chris", "robert", "tony" , "samuel","captain" , "natasha", "rachit" , "parth" , "anushka" , "jyotshna" , "samir" , "sarthak" , "sedah" , "jenik" , "amay" , "ahmad" , "yashav", "dalol" , "divyanshu" , "yash","sian"]
# for name in names:
#     print(name , f'{whatstheloss(name).item():.3f}')

results = []
for name in names:
    loss_value = whatstheloss(name).item()
    results.append((loss_value, name)) # Store as (loss, name) tuple

# Sort the list. By default, it sorts by the first element, which is the loss.
results.sort()

# Print the sorted results
for loss, name in results:
    print(f"{name}: {loss:.3f}")


# now we shall sample to create names from thus model
g = torch.Generator().manual_seed(2147483647)
for i in range (10):
    name = ""
    char = '.'
    while True:
        idx = torch.multinomial(P[stoi[char]] , num_samples = 1 , replacement = True,generator=g).item()
        ch = itos[idx]
        if ch == '.':
            break
        name += ch
        char = ch
    print(name)








xs , ys = [],[]
for b in bigrams[:32]:
    xs.append(b[0])
    ys.append(b[1])
for i,j in zip(xs,ys):
    print(i,j)
    


xs = torch.tensor([stoi[i] for i in xs])
ys = torch.tensor([stoi[i] for i in ys])


print (xs , ys)


# encode each of these values (values from 0 to 26 into a 27 dim space)
import torch.nn.functional as F
one_hot_enc = F.one_hot(xs, num_classes=27).float() # float coz of the operation we will do on it
print(one_hot_enc)
print(one_hot_enc.shape)



W = torch.randn((27,27) , requires_grad = True,dtype = torch.float32)
print(W)
print(W.shape)





# log-counts also called logits by convention
logits = one_hot_enc @ W
print("Initial out: logits -> " , logits)
logits = logits.exp()
print("taking exp -> " , logits)
probs = logits / logits.sum(1,keepdim = True)
print("prob : ",probs)


# now all we gotta do is somehow find a W so that the probablitlites that come out in probs are pretty good
# loss function
output_idx = torch.arange(probs.shape[0])
print(output_idx)
y_probs = probs[output_idx , ys]
y_probs


# loss = nll/n
log_sum , n = 0,0
for p in y_probs:
    log_sum +=  -p.log()
    n += 1

print("nll = ",log_sum)
print("nll/n = ",log_sum/n)


-y_probs.log().mean()


# workig example 
# In[70]:


nlls = torch.zeros(5)
for i in range(5):
  # i-th bigram:
  x = xs[i].item() # input character index
  y = ys[i].item() # label character index
  print('--------')
  print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')
  print('input to the neural net:', x)
  print('output probabilities from the neural net:', probs[i])
  print('label (actual next character):', y)
  p = probs[i, y]
  print('probability assigned by the net to the the correct character:', p.item())
  logp = torch.log(p)
  print('log likelihood:', logp.item())
  nll = -logp
  print('negative log likelihood:', nll.item())
  nlls[i] = nll

print('=========')
print('average negative log likelihood, i.e. loss =', nlls.mean().item())



# training the net

# setup :

xs , ys = [],[]
for b in bigrams:
    xs.append(b[0])
    ys.append(b[1])

xs = torch.tensor([stoi[i] for i in xs])
ys = torch.tensor([stoi[i] for i in ys])

one_hot_enc = F.one_hot(xs , num_classes=27).float()
W = torch.randn((27,27) , dtype=torch.float32 , requires_grad=True)

step = 3
output_idx = torch.arange(len(ys))    


# main loop

step = 4
for i in range(1000):
    # forward pass :
    
    logits = one_hot_enc @ W
    counts = logits.exp()
    p      =  counts / counts.sum(1,keepdim=True)
    probs_y = p[output_idx , ys]

    loss = -probs_y.log().mean() + (W**2).mean() * 0.02
    print(f"iteration {i} : loss = {loss}")

    # backward pass
    W.grad = None
    loss.backward()

    # update weights
    W.data += -step * W.grad


# now we sample

g = torch.Generator().manual_seed(2147483647)

for i in range(10):
  
  out = []
  ix = 0
  while True:
    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float() # single character input encoded
    logits = xenc @ W # predict log-counts
    counts = logits.exp() # counts, equivalent to N
    p = counts / counts.sum(1, keepdims=True) # probabilities for next character
    # ----------
    
    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()
    if ix == 0:
      break
    out.append(itos[ix])
    
  print(''.join(out))












